{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3058e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from src.model import GPT,Config\n",
    "from src.trainer import Trainer\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b993b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = './log'\n",
    "DATASET_PATH = './data/gutenberg'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes, split='train'):\n",
    "        super().__init__()\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "        \n",
    "        # get the shard filenames\n",
    "        data_root = DATASET_PATH\n",
    "        shard_filenames = os.listdir(data_root)\n",
    "        shard_filenames = sorted([filename for filename in shard_filenames if split in filename])\n",
    "        self.shard_filepaths = [os.path.join(data_root, filename) for filename in shard_filenames]\n",
    "        assert len(self.shard_filepaths) > 0, f'no shards found for split {split}'\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f'found {len(self.shard_filepaths)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def load_tokens(self, filepath):\n",
    "        tokens = torch.tensor(np.load(filepath).astype(np.int32), dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard 0\n",
    "        self.curr_shard = 0\n",
    "        self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "        self.curr_pos = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        batch = self.tokens[self.curr_pos : self.curr_pos + B*T + 1]\n",
    "        x_batch = batch[:-1].view(B, T)\n",
    "        y_batch = batch[1:].view(B, T)\n",
    "        self.curr_pos += B * T * self.num_processes\n",
    "        if self.curr_pos + (B * T + 1) > len(self.tokens):\n",
    "            self.curr_shard = (self.curr_shard + 1) % len(self.shard_filepaths)\n",
    "            self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "            self.curr_pos = self.B * self.T * self.process_rank\n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a831a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add76cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 6\n",
    "CTX_LENGTH = 2048\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 10\n",
    "EMBED_DIM = 768\n",
    "WEIGHT_DECAY =0.1\n",
    "MAX_LR = 1e-3\n",
    "MIN_LR = 1e-4\n",
    "EVAL_FREQ = 1\n",
    "MAX_STEPS = 2400\n",
    "WARMUP_STEPS = 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbf5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf24d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='train')\n",
    "val_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7461c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 111,086,592\n",
      "num decay parameter tensors: 42 with 110,985,216 parameters\n",
      "num nodecay parameter tensors: 82 with 101,376 parameters\n",
      "using fused AdamW optimizer: True\n"
     ]
    }
   ],
   "source": [
    "gpt_config = Config(vocab_size=50304,  # number of tokens: 50000 BPE merges + 256 bytes tokens + 1 <endoftext> token = 50257, \n",
    "                    # 50304 (nice number, lots of power of 2s) used instead of 50257 (bad, odd number)\n",
    "                           context_length=CTX_LENGTH, \n",
    "                           num_layers=NUM_LAYERS, \n",
    "                           num_heads=NUM_HEADS, \n",
    "                           embedding_dim=EMBED_DIM\n",
    "                           )\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of trainable parameters: {total_params:,}')\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "optimizer = model.configure_optimizer(weight_decay=WEIGHT_DECAY,lr=MAX_LR,device_type=device_type,master_process=master_process)\n",
    "token_encoder = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f51484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | train loss: 10.98 | val loss: 10.90 | perplexity: 54435.09 | lr: 1.40e-06 | norm: 16.4292 | dt: 12362.8950ms | tok/sec: 31.8061\n",
      "step    1 | train loss: 10.91 | val loss: 10.74 | perplexity: 46157.28 | lr: 2.80e-06 | norm: 16.4414 | dt: 15741.0264ms | tok/sec: 24.9803\n",
      "step    2 | train loss: 10.75 | val loss: 10.53 | perplexity: 37282.40 | lr: 4.20e-06 | norm: 14.9576 | dt: 14234.7906ms | tok/sec: 27.6236\n",
      "step    3 | train loss: 10.55 | val loss: 10.32 | perplexity: 30198.92 | lr: 5.59e-06 | norm: 12.9142 | dt: 14551.4734ms | tok/sec: 27.0224\n",
      "step    4 | train loss: 10.35 | val loss: 10.13 | perplexity: 25173.52 | lr: 6.99e-06 | norm: 9.9737 | dt: 14153.6860ms | tok/sec: 27.7819\n",
      "step    5 | train loss: 10.16 | val loss: 9.98 | perplexity: 21543.51 | lr: 8.39e-06 | norm: 8.8327 | dt: 14194.8779ms | tok/sec: 27.7013\n",
      "step    6 | train loss: 9.98 | val loss: 9.85 | perplexity: 18890.57 | lr: 9.79e-06 | norm: 7.1139 | dt: 14202.7395ms | tok/sec: 27.6859\n",
      "step    7 | train loss: 9.86 | val loss: 9.74 | perplexity: 16934.02 | lr: 1.12e-05 | norm: 6.3725 | dt: 14200.7985ms | tok/sec: 27.6897\n",
      "step    8 | train loss: 9.75 | val loss: 9.65 | perplexity: 15480.32 | lr: 1.26e-05 | norm: 6.0349 | dt: 14194.5806ms | tok/sec: 27.7018\n",
      "step    9 | train loss: 9.66 | val loss: 9.57 | perplexity: 14292.08 | lr: 1.40e-05 | norm: 4.5178 | dt: 14183.3334ms | tok/sec: 27.7238\n",
      "step   10 | train loss: 9.50 | val loss: 9.50 | perplexity: 13367.50 | lr: 1.54e-05 | norm: 4.3221 | dt: 14160.6011ms | tok/sec: 27.7683\n",
      "step   11 | train loss: 9.76 | val loss: 9.45 | perplexity: 12764.78 | lr: 1.68e-05 | norm: 4.3523 | dt: 14174.5610ms | tok/sec: 27.7410\n",
      "step   12 | train loss: 9.49 | val loss: 9.41 | perplexity: 12220.91 | lr: 1.82e-05 | norm: 3.9135 | dt: 14131.6211ms | tok/sec: 27.8253\n",
      "step   13 | train loss: 9.42 | val loss: 9.36 | perplexity: 11639.93 | lr: 1.96e-05 | norm: 3.4410 | dt: 14183.2232ms | tok/sec: 27.7240\n",
      "step   14 | train loss: 9.36 | val loss: 9.32 | perplexity: 11131.04 | lr: 2.10e-05 | norm: 3.4336 | dt: 14155.0171ms | tok/sec: 27.7793\n",
      "step   15 | train loss: 9.34 | val loss: 9.28 | perplexity: 10695.02 | lr: 2.24e-05 | norm: 2.8047 | dt: 14144.4786ms | tok/sec: 27.8000\n",
      "step   16 | train loss: 9.24 | val loss: 9.24 | perplexity: 10256.11 | lr: 2.38e-05 | norm: 3.6002 | dt: 14016.4552ms | tok/sec: 28.0539\n",
      "step   17 | train loss: 9.32 | val loss: 9.19 | perplexity: 9838.45 | lr: 2.52e-05 | norm: 3.2982 | dt: 14029.3293ms | tok/sec: 28.0281\n",
      "step   18 | train loss: 9.40 | val loss: 9.15 | perplexity: 9456.61 | lr: 2.66e-05 | norm: 2.9634 | dt: 14317.9564ms | tok/sec: 27.4631\n",
      "step   19 | train loss: 9.32 | val loss: 9.11 | perplexity: 9036.60 | lr: 2.80e-05 | norm: 3.0592 | dt: 14124.6977ms | tok/sec: 27.8389\n",
      "step   20 | train loss: 9.11 | val loss: 9.05 | perplexity: 8519.55 | lr: 2.94e-05 | norm: 3.5160 | dt: 14125.4177ms | tok/sec: 27.8375\n",
      "step   21 | train loss: 9.04 | val loss: 8.98 | perplexity: 7907.52 | lr: 3.08e-05 | norm: 3.2189 | dt: 14076.2348ms | tok/sec: 27.9347\n",
      "step   22 | train loss: 8.85 | val loss: 8.90 | perplexity: 7327.25 | lr: 3.22e-05 | norm: 3.7057 | dt: 14044.0693ms | tok/sec: 27.9987\n",
      "step   23 | train loss: 8.75 | val loss: 8.86 | perplexity: 7025.94 | lr: 3.36e-05 | norm: 4.3465 | dt: 14109.7529ms | tok/sec: 27.8684\n",
      "step   24 | train loss: 9.07 | val loss: 8.81 | perplexity: 6696.24 | lr: 3.50e-05 | norm: 3.1815 | dt: 14147.6798ms | tok/sec: 27.7937\n",
      "step   25 | train loss: 9.19 | val loss: 8.76 | perplexity: 6373.80 | lr: 3.64e-05 | norm: 3.1400 | dt: 14066.8361ms | tok/sec: 27.9534\n",
      "step   26 | train loss: 8.81 | val loss: 8.70 | perplexity: 6030.30 | lr: 3.78e-05 | norm: 3.3640 | dt: 14049.4399ms | tok/sec: 27.9880\n",
      "step   27 | train loss: 8.61 | val loss: 8.65 | perplexity: 5707.18 | lr: 3.92e-05 | norm: 2.9231 | dt: 13983.8161ms | tok/sec: 28.1194\n",
      "step   28 | train loss: 8.87 | val loss: 8.60 | perplexity: 5416.79 | lr: 4.06e-05 | norm: 2.5634 | dt: 13981.4286ms | tok/sec: 28.1242\n",
      "step   29 | train loss: 8.83 | val loss: 8.55 | perplexity: 5174.55 | lr: 4.20e-05 | norm: 3.1583 | dt: 13967.9925ms | tok/sec: 28.1512\n",
      "step   30 | train loss: 8.31 | val loss: 8.50 | perplexity: 4928.74 | lr: 4.34e-05 | norm: 3.5416 | dt: 13960.6810ms | tok/sec: 28.1660\n",
      "step   31 | train loss: 8.48 | val loss: 8.45 | perplexity: 4665.52 | lr: 4.48e-05 | norm: 2.5871 | dt: 13950.3734ms | tok/sec: 28.1868\n",
      "step   32 | train loss: 8.32 | val loss: 8.40 | perplexity: 4453.11 | lr: 4.62e-05 | norm: 3.4167 | dt: 13957.2561ms | tok/sec: 28.1729\n",
      "step   33 | train loss: 8.27 | val loss: 8.36 | perplexity: 4268.47 | lr: 4.76e-05 | norm: 2.5894 | dt: 13988.5204ms | tok/sec: 28.1099\n",
      "step   34 | train loss: 8.26 | val loss: 8.31 | perplexity: 4068.81 | lr: 4.90e-05 | norm: 2.5186 | dt: 14073.3330ms | tok/sec: 27.9405\n",
      "step   35 | train loss: 8.19 | val loss: 8.28 | perplexity: 3947.32 | lr: 5.03e-05 | norm: 2.3889 | dt: 14236.3541ms | tok/sec: 27.6206\n",
      "step   36 | train loss: 8.15 | val loss: 8.24 | perplexity: 3790.53 | lr: 5.17e-05 | norm: 2.1754 | dt: 14107.5974ms | tok/sec: 27.8726\n",
      "step   37 | train loss: 7.91 | val loss: 8.21 | perplexity: 3659.74 | lr: 5.31e-05 | norm: 2.5381 | dt: 14016.0584ms | tok/sec: 28.0547\n",
      "step   38 | train loss: 8.34 | val loss: 8.15 | perplexity: 3468.94 | lr: 5.45e-05 | norm: 2.2704 | dt: 13997.5049ms | tok/sec: 28.0919\n",
      "step   39 | train loss: 8.21 | val loss: 8.09 | perplexity: 3259.26 | lr: 5.59e-05 | norm: 2.5747 | dt: 13986.9180ms | tok/sec: 28.1131\n",
      "step   40 | train loss: 8.11 | val loss: 8.02 | perplexity: 3055.81 | lr: 5.73e-05 | norm: 2.2119 | dt: 13961.9203ms | tok/sec: 28.1635\n",
      "step   41 | train loss: 8.36 | val loss: 7.96 | perplexity: 2852.10 | lr: 5.87e-05 | norm: 2.4151 | dt: 14024.1275ms | tok/sec: 28.0385\n",
      "step   42 | train loss: 7.93 | val loss: 7.89 | perplexity: 2670.37 | lr: 6.01e-05 | norm: 2.2492 | dt: 13951.5607ms | tok/sec: 28.1844\n",
      "step   43 | train loss: 8.23 | val loss: 7.83 | perplexity: 2518.84 | lr: 6.15e-05 | norm: 1.9628 | dt: 13991.9379ms | tok/sec: 28.1030\n",
      "step   44 | train loss: 7.63 | val loss: 7.78 | perplexity: 2391.82 | lr: 6.29e-05 | norm: 2.1145 | dt: 13989.0573ms | tok/sec: 28.1088\n",
      "step   45 | train loss: 7.96 | val loss: 7.72 | perplexity: 2255.64 | lr: 6.43e-05 | norm: 2.5807 | dt: 14092.7675ms | tok/sec: 27.9020\n",
      "step   46 | train loss: 7.65 | val loss: 7.68 | perplexity: 2165.98 | lr: 6.57e-05 | norm: 2.0212 | dt: 13993.7086ms | tok/sec: 28.0995\n",
      "step   47 | train loss: 7.91 | val loss: 7.62 | perplexity: 2044.96 | lr: 6.71e-05 | norm: 2.0748 | dt: 13998.9600ms | tok/sec: 28.0889\n",
      "step   48 | train loss: 7.76 | val loss: 7.58 | perplexity: 1956.01 | lr: 6.85e-05 | norm: 1.7416 | dt: 14061.1873ms | tok/sec: 27.9646\n",
      "step   49 | train loss: 7.50 | val loss: 7.52 | perplexity: 1848.99 | lr: 6.99e-05 | norm: 2.5543 | dt: 13996.8333ms | tok/sec: 28.0932\n",
      "step   50 | train loss: 7.54 | val loss: 7.47 | perplexity: 1750.06 | lr: 7.13e-05 | norm: 1.9798 | dt: 14070.7572ms | tok/sec: 27.9456\n",
      "step   51 | train loss: 7.44 | val loss: 7.41 | perplexity: 1653.01 | lr: 7.27e-05 | norm: 2.1102 | dt: 14095.5391ms | tok/sec: 27.8965\n",
      "step   52 | train loss: 7.61 | val loss: 7.35 | perplexity: 1562.91 | lr: 7.41e-05 | norm: 1.8218 | dt: 13974.9067ms | tok/sec: 28.1373\n",
      "step   53 | train loss: 7.52 | val loss: 7.30 | perplexity: 1486.89 | lr: 7.55e-05 | norm: 1.5577 | dt: 13977.4296ms | tok/sec: 28.1322\n",
      "step   54 | train loss: 7.31 | val loss: 7.25 | perplexity: 1415.07 | lr: 7.69e-05 | norm: 1.6583 | dt: 13998.5158ms | tok/sec: 28.0898\n",
      "step   55 | train loss: 7.78 | val loss: 7.22 | perplexity: 1360.67 | lr: 7.83e-05 | norm: 2.0632 | dt: 14045.1798ms | tok/sec: 27.9965\n",
      "step   56 | train loss: 7.48 | val loss: 7.18 | perplexity: 1306.36 | lr: 7.97e-05 | norm: 1.4772 | dt: 13938.1177ms | tok/sec: 28.2116\n",
      "step   57 | train loss: 7.66 | val loss: 7.13 | perplexity: 1243.67 | lr: 8.11e-05 | norm: 1.9658 | dt: 13980.9337ms | tok/sec: 28.1252\n",
      "step   58 | train loss: 7.33 | val loss: 7.07 | perplexity: 1180.08 | lr: 8.25e-05 | norm: 1.4940 | dt: 14037.2841ms | tok/sec: 28.0123\n",
      "step   59 | train loss: 6.94 | val loss: 7.03 | perplexity: 1125.22 | lr: 8.39e-05 | norm: 1.7245 | dt: 14162.0023ms | tok/sec: 27.7656\n",
      "step   60 | train loss: 7.15 | val loss: 6.98 | perplexity: 1070.08 | lr: 8.53e-05 | norm: 1.6477 | dt: 13986.6180ms | tok/sec: 28.1137\n",
      "step   61 | train loss: 6.91 | val loss: 6.93 | perplexity: 1022.29 | lr: 8.67e-05 | norm: 1.5266 | dt: 14023.4647ms | tok/sec: 28.0399\n",
      "step   62 | train loss: 7.00 | val loss: 6.88 | perplexity: 976.65 | lr: 8.81e-05 | norm: 1.6458 | dt: 13993.2992ms | tok/sec: 28.1003\n",
      "step   63 | train loss: 7.04 | val loss: 6.85 | perplexity: 943.16 | lr: 8.95e-05 | norm: 1.4779 | dt: 14064.0397ms | tok/sec: 27.9590\n",
      "step   64 | train loss: 6.85 | val loss: 6.82 | perplexity: 915.68 | lr: 9.09e-05 | norm: 1.4255 | dt: 13978.3981ms | tok/sec: 28.1303\n",
      "step   65 | train loss: 6.88 | val loss: 6.78 | perplexity: 878.28 | lr: 9.23e-05 | norm: 1.4175 | dt: 14039.2523ms | tok/sec: 28.0083\n",
      "step   66 | train loss: 7.09 | val loss: 6.74 | perplexity: 843.41 | lr: 9.37e-05 | norm: 1.3485 | dt: 13964.7210ms | tok/sec: 28.1578\n",
      "step   67 | train loss: 7.04 | val loss: 6.70 | perplexity: 813.37 | lr: 9.51e-05 | norm: 1.3528 | dt: 13981.9946ms | tok/sec: 28.1230\n",
      "step   68 | train loss: 6.60 | val loss: 6.67 | perplexity: 785.08 | lr: 9.65e-05 | norm: 1.4782 | dt: 13998.9712ms | tok/sec: 28.0889\n",
      "step   69 | train loss: 6.76 | val loss: 6.63 | perplexity: 760.16 | lr: 9.79e-05 | norm: 1.7178 | dt: 13988.4317ms | tok/sec: 28.1101\n",
      "step   70 | train loss: 6.66 | val loss: 6.60 | perplexity: 734.58 | lr: 9.93e-05 | norm: 1.1509 | dt: 14058.6786ms | tok/sec: 27.9696\n",
      "step   71 | train loss: 6.61 | val loss: 6.57 | perplexity: 709.89 | lr: 1.01e-04 | norm: 1.3403 | dt: 13990.8872ms | tok/sec: 28.1052\n",
      "step   72 | train loss: 6.28 | val loss: 6.54 | perplexity: 690.64 | lr: 1.02e-04 | norm: 1.1666 | dt: 13988.0664ms | tok/sec: 28.1108\n",
      "step   73 | train loss: 6.63 | val loss: 6.51 | perplexity: 669.93 | lr: 1.03e-04 | norm: 1.0685 | dt: 13989.7025ms | tok/sec: 28.1075\n",
      "step   74 | train loss: 6.52 | val loss: 6.47 | perplexity: 648.05 | lr: 1.05e-04 | norm: 1.2623 | dt: 14004.4820ms | tok/sec: 28.0779\n",
      "step   75 | train loss: 6.52 | val loss: 6.44 | perplexity: 629.06 | lr: 1.06e-04 | norm: 0.8911 | dt: 13993.5634ms | tok/sec: 28.0998\n",
      "step   76 | train loss: 6.45 | val loss: 6.42 | perplexity: 612.26 | lr: 1.08e-04 | norm: 1.1439 | dt: 14000.1991ms | tok/sec: 28.0865\n",
      "step   77 | train loss: 6.72 | val loss: 6.39 | perplexity: 597.11 | lr: 1.09e-04 | norm: 1.0013 | dt: 13984.9396ms | tok/sec: 28.1171\n",
      "step   78 | train loss: 6.56 | val loss: 6.37 | perplexity: 582.09 | lr: 1.10e-04 | norm: 1.0041 | dt: 13976.9139ms | tok/sec: 28.1332\n",
      "step   79 | train loss: 6.07 | val loss: 6.34 | perplexity: 569.14 | lr: 1.12e-04 | norm: 1.0019 | dt: 13993.8824ms | tok/sec: 28.0991\n",
      "step   80 | train loss: 6.02 | val loss: 6.33 | perplexity: 563.45 | lr: 1.13e-04 | norm: 1.2860 | dt: 13957.7801ms | tok/sec: 28.1718\n",
      "step   81 | train loss: 6.38 | val loss: 6.31 | perplexity: 550.16 | lr: 1.15e-04 | norm: 1.7762 | dt: 14099.7419ms | tok/sec: 27.8882\n",
      "step   82 | train loss: 6.05 | val loss: 6.29 | perplexity: 540.12 | lr: 1.16e-04 | norm: 0.9161 | dt: 14007.5822ms | tok/sec: 28.0717\n",
      "step   83 | train loss: 6.40 | val loss: 6.28 | perplexity: 531.58 | lr: 1.17e-04 | norm: 1.0944 | dt: 14006.5761ms | tok/sec: 28.0737\n",
      "step   84 | train loss: 6.61 | val loss: 6.25 | perplexity: 520.09 | lr: 1.19e-04 | norm: 1.0539 | dt: 13990.9840ms | tok/sec: 28.1050\n",
      "step   85 | train loss: 6.53 | val loss: 6.25 | perplexity: 517.68 | lr: 1.20e-04 | norm: 0.7934 | dt: 13970.2933ms | tok/sec: 28.1466\n",
      "step   86 | train loss: 6.42 | val loss: 6.23 | perplexity: 509.71 | lr: 1.22e-04 | norm: 1.1576 | dt: 13966.8391ms | tok/sec: 28.1535\n",
      "step   87 | train loss: 6.35 | val loss: 6.21 | perplexity: 498.67 | lr: 1.23e-04 | norm: 0.9004 | dt: 13977.6654ms | tok/sec: 28.1317\n",
      "step   88 | train loss: 6.38 | val loss: 6.21 | perplexity: 496.65 | lr: 1.24e-04 | norm: 0.7935 | dt: 13982.7101ms | tok/sec: 28.1216\n",
      "step   89 | train loss: 6.18 | val loss: 6.19 | perplexity: 485.90 | lr: 1.26e-04 | norm: 1.4948 | dt: 14014.7123ms | tok/sec: 28.0574\n",
      "step   90 | train loss: 5.91 | val loss: 6.18 | perplexity: 480.80 | lr: 1.27e-04 | norm: 1.2135 | dt: 13991.0145ms | tok/sec: 28.1049\n",
      "step   91 | train loss: 6.43 | val loss: 6.16 | perplexity: 474.75 | lr: 1.29e-04 | norm: 1.3006 | dt: 14012.9795ms | tok/sec: 28.0608\n",
      "step   92 | train loss: 6.44 | val loss: 6.15 | perplexity: 469.41 | lr: 1.30e-04 | norm: 1.2612 | dt: 14034.6019ms | tok/sec: 28.0176\n",
      "step   93 | train loss: 5.92 | val loss: 6.13 | perplexity: 458.15 | lr: 1.31e-04 | norm: 1.7121 | dt: 14003.8993ms | tok/sec: 28.0790\n",
      "step   94 | train loss: 6.45 | val loss: 6.12 | perplexity: 453.96 | lr: 1.33e-04 | norm: 0.8864 | dt: 13991.9643ms | tok/sec: 28.1030\n",
      "step   95 | train loss: 6.24 | val loss: 6.11 | perplexity: 450.48 | lr: 1.34e-04 | norm: 1.0832 | dt: 13991.7605ms | tok/sec: 28.1034\n",
      "step   96 | train loss: 6.30 | val loss: 6.09 | perplexity: 443.51 | lr: 1.36e-04 | norm: 1.5017 | dt: 14043.8139ms | tok/sec: 27.9992\n",
      "step   97 | train loss: 5.88 | val loss: 6.08 | perplexity: 437.71 | lr: 1.37e-04 | norm: 1.6661 | dt: 13980.7982ms | tok/sec: 28.1254\n",
      "step   98 | train loss: 5.63 | val loss: 6.06 | perplexity: 429.74 | lr: 1.38e-04 | norm: 1.3725 | dt: 14115.9384ms | tok/sec: 27.8562\n",
      "step   99 | train loss: 5.95 | val loss: 6.04 | perplexity: 420.34 | lr: 1.40e-04 | norm: 1.0886 | dt: 14034.1277ms | tok/sec: 28.0186\n",
      "step  100 | train loss: 6.00 | val loss: 6.03 | perplexity: 416.43 | lr: 1.41e-04 | norm: 0.8032 | dt: 14125.6378ms | tok/sec: 27.8370\n",
      "step  101 | train loss: 5.99 | val loss: 6.03 | perplexity: 415.18 | lr: 1.43e-04 | norm: 0.7387 | dt: 14027.6823ms | tok/sec: 28.0314\n",
      "step  102 | train loss: 6.20 | val loss: 6.02 | perplexity: 410.31 | lr: 1.44e-04 | norm: 0.6821 | dt: 14023.3183ms | tok/sec: 28.0402\n",
      "step  103 | train loss: 5.79 | val loss: 6.01 | perplexity: 408.02 | lr: 1.45e-04 | norm: 1.0383 | dt: 14026.0813ms | tok/sec: 28.0346\n",
      "step  104 | train loss: 6.15 | val loss: 6.00 | perplexity: 402.75 | lr: 1.47e-04 | norm: 0.8443 | dt: 14104.5728ms | tok/sec: 27.8786\n",
      "step  105 | train loss: 6.13 | val loss: 5.98 | perplexity: 396.37 | lr: 1.48e-04 | norm: 0.8212 | dt: 13987.7801ms | tok/sec: 28.1114\n",
      "step  106 | train loss: 5.79 | val loss: 5.98 | perplexity: 396.80 | lr: 1.50e-04 | norm: 0.8280 | dt: 13984.9737ms | tok/sec: 28.1170\n",
      "step  107 | train loss: 5.99 | val loss: 5.96 | perplexity: 388.09 | lr: 1.51e-04 | norm: 1.0679 | dt: 14029.9838ms | tok/sec: 28.0268\n",
      "step  108 | train loss: 6.04 | val loss: 5.94 | perplexity: 381.03 | lr: 1.52e-04 | norm: 0.9792 | dt: 14133.6329ms | tok/sec: 27.8213\n",
      "step  109 | train loss: 5.97 | val loss: 5.93 | perplexity: 376.37 | lr: 1.54e-04 | norm: 0.9889 | dt: 14034.9734ms | tok/sec: 28.0169\n",
      "step  110 | train loss: 5.66 | val loss: 5.93 | perplexity: 375.68 | lr: 1.55e-04 | norm: 0.7715 | dt: 14024.5032ms | tok/sec: 28.0378\n",
      "step  111 | train loss: 5.95 | val loss: 5.91 | perplexity: 368.27 | lr: 1.57e-04 | norm: 1.0871 | dt: 14012.3894ms | tok/sec: 28.0620\n",
      "step  112 | train loss: 5.90 | val loss: 5.90 | perplexity: 366.35 | lr: 1.58e-04 | norm: 0.8622 | dt: 14051.4059ms | tok/sec: 27.9841\n",
      "step  113 | train loss: 5.94 | val loss: 5.89 | perplexity: 362.70 | lr: 1.59e-04 | norm: 0.9449 | dt: 14008.7838ms | tok/sec: 28.0692\n",
      "step  114 | train loss: 5.78 | val loss: 5.89 | perplexity: 360.30 | lr: 1.61e-04 | norm: 1.0293 | dt: 14026.6435ms | tok/sec: 28.0335\n",
      "step  115 | train loss: 5.99 | val loss: 5.87 | perplexity: 352.96 | lr: 1.62e-04 | norm: 1.2599 | dt: 14098.3567ms | tok/sec: 27.8909\n",
      "step  116 | train loss: 5.84 | val loss: 5.85 | perplexity: 348.48 | lr: 1.64e-04 | norm: 0.7825 | dt: 14119.6353ms | tok/sec: 27.8489\n",
      "step  117 | train loss: 5.98 | val loss: 5.84 | perplexity: 344.12 | lr: 1.65e-04 | norm: 0.8278 | dt: 14045.3551ms | tok/sec: 27.9962\n",
      "step  118 | train loss: 5.33 | val loss: 5.84 | perplexity: 345.40 | lr: 1.66e-04 | norm: 1.2662 | dt: 14019.9726ms | tok/sec: 28.0468\n",
      "step  119 | train loss: 6.25 | val loss: 5.84 | perplexity: 344.80 | lr: 1.68e-04 | norm: 0.9626 | dt: 13991.3511ms | tok/sec: 28.1042\n",
      "step  120 | train loss: 5.85 | val loss: 5.83 | perplexity: 339.48 | lr: 1.69e-04 | norm: 0.8479 | dt: 13985.1871ms | tok/sec: 28.1166\n",
      "step  121 | train loss: 6.16 | val loss: 5.83 | perplexity: 340.58 | lr: 1.71e-04 | norm: 0.7292 | dt: 13991.5903ms | tok/sec: 28.1037\n",
      "step  122 | train loss: 5.65 | val loss: 5.81 | perplexity: 333.96 | lr: 1.72e-04 | norm: 0.9316 | dt: 14075.6447ms | tok/sec: 27.9359\n",
      "step  123 | train loss: 5.67 | val loss: 5.79 | perplexity: 328.60 | lr: 1.73e-04 | norm: 0.9709 | dt: 14009.1200ms | tok/sec: 28.0686\n",
      "step  124 | train loss: 5.64 | val loss: 5.79 | perplexity: 326.92 | lr: 1.75e-04 | norm: 0.9269 | dt: 14011.2927ms | tok/sec: 28.0642\n",
      "step  125 | train loss: 5.88 | val loss: 5.78 | perplexity: 322.95 | lr: 1.76e-04 | norm: 0.9351 | dt: 14020.6358ms | tok/sec: 28.0455\n",
      "step  126 | train loss: 5.58 | val loss: 5.80 | perplexity: 329.81 | lr: 1.78e-04 | norm: 1.1218 | dt: 14026.8703ms | tok/sec: 28.0331\n",
      "step  127 | train loss: 5.54 | val loss: 5.79 | perplexity: 325.52 | lr: 1.79e-04 | norm: 1.5481 | dt: 14193.3711ms | tok/sec: 27.7042\n",
      "step  128 | train loss: 5.93 | val loss: 5.77 | perplexity: 321.26 | lr: 1.80e-04 | norm: 1.4686 | dt: 14014.8489ms | tok/sec: 28.0571\n",
      "step  129 | train loss: 5.94 | val loss: 5.79 | perplexity: 326.11 | lr: 1.82e-04 | norm: 0.9502 | dt: 14091.5451ms | tok/sec: 27.9044\n",
      "step  130 | train loss: 6.39 | val loss: 5.80 | perplexity: 330.40 | lr: 1.83e-04 | norm: 1.0611 | dt: 13994.8273ms | tok/sec: 28.0972\n",
      "step  131 | train loss: 5.69 | val loss: 5.77 | perplexity: 319.32 | lr: 1.85e-04 | norm: 1.3182 | dt: 13994.7913ms | tok/sec: 28.0973\n",
      "step  132 | train loss: 5.75 | val loss: 5.76 | perplexity: 315.93 | lr: 1.86e-04 | norm: 0.8172 | dt: 13983.2768ms | tok/sec: 28.1204\n",
      "step  133 | train loss: 5.89 | val loss: 5.74 | perplexity: 311.62 | lr: 1.87e-04 | norm: 1.0796 | dt: 13967.7348ms | tok/sec: 28.1517\n",
      "step  134 | train loss: 5.80 | val loss: 5.75 | perplexity: 314.43 | lr: 1.89e-04 | norm: 0.7852 | dt: 14051.6860ms | tok/sec: 27.9835\n",
      "step  135 | train loss: 5.73 | val loss: 5.74 | perplexity: 310.15 | lr: 1.90e-04 | norm: 0.9929 | dt: 13988.5879ms | tok/sec: 28.1098\n",
      "step  136 | train loss: 5.80 | val loss: 5.74 | perplexity: 311.06 | lr: 1.92e-04 | norm: 0.9207 | dt: 14042.2482ms | tok/sec: 28.0024\n",
      "step  137 | train loss: 5.98 | val loss: 5.74 | perplexity: 311.96 | lr: 1.93e-04 | norm: 0.9435 | dt: 14019.4826ms | tok/sec: 28.0478\n",
      "step  138 | train loss: 5.79 | val loss: 5.74 | perplexity: 311.28 | lr: 1.94e-04 | norm: 1.1057 | dt: 14003.8898ms | tok/sec: 28.0791\n",
      "step  139 | train loss: 5.68 | val loss: 5.72 | perplexity: 304.78 | lr: 1.96e-04 | norm: 0.8932 | dt: 14023.0327ms | tok/sec: 28.0407\n",
      "step  140 | train loss: 5.69 | val loss: 5.69 | perplexity: 295.57 | lr: 1.97e-04 | norm: 0.9716 | dt: 13998.6498ms | tok/sec: 28.0896\n",
      "step  141 | train loss: 5.91 | val loss: 5.69 | perplexity: 296.28 | lr: 1.99e-04 | norm: 0.6919 | dt: 14010.6580ms | tok/sec: 28.0655\n",
      "step  142 | train loss: 5.58 | val loss: 5.69 | perplexity: 295.12 | lr: 2.00e-04 | norm: 1.0534 | dt: 13994.9572ms | tok/sec: 28.0970\n",
      "step  143 | train loss: 5.77 | val loss: 5.69 | perplexity: 294.43 | lr: 2.01e-04 | norm: 0.9626 | dt: 13992.4731ms | tok/sec: 28.1020\n",
      "step  144 | train loss: 5.87 | val loss: 5.67 | perplexity: 291.29 | lr: 2.03e-04 | norm: 0.9204 | dt: 13996.1519ms | tok/sec: 28.0946\n",
      "step  145 | train loss: 5.97 | val loss: 5.67 | perplexity: 288.87 | lr: 2.04e-04 | norm: 0.8455 | dt: 14012.1424ms | tok/sec: 28.0625\n",
      "step  146 | train loss: 5.98 | val loss: 5.67 | perplexity: 290.83 | lr: 2.06e-04 | norm: 0.7946 | dt: 14034.1046ms | tok/sec: 28.0186\n",
      "step  147 | train loss: 5.91 | val loss: 5.66 | perplexity: 286.70 | lr: 2.07e-04 | norm: 1.1096 | dt: 14123.1923ms | tok/sec: 27.8419\n",
      "step  148 | train loss: 5.61 | val loss: 5.65 | perplexity: 283.47 | lr: 2.08e-04 | norm: 0.7324 | dt: 14001.9255ms | tok/sec: 28.0830\n",
      "step  149 | train loss: 5.72 | val loss: 5.63 | perplexity: 277.97 | lr: 2.10e-04 | norm: 0.8585 | dt: 14027.2367ms | tok/sec: 28.0323\n",
      "step  150 | train loss: 5.88 | val loss: 5.62 | perplexity: 275.85 | lr: 2.11e-04 | norm: 0.8855 | dt: 14018.7538ms | tok/sec: 28.0493\n",
      "step  151 | train loss: 5.66 | val loss: 5.61 | perplexity: 274.19 | lr: 2.13e-04 | norm: 1.1244 | dt: 13997.0114ms | tok/sec: 28.0929\n",
      "step  152 | train loss: 5.89 | val loss: 5.62 | perplexity: 275.36 | lr: 2.14e-04 | norm: 0.7474 | dt: 13980.9084ms | tok/sec: 28.1252\n",
      "step  153 | train loss: 5.63 | val loss: 5.62 | perplexity: 275.99 | lr: 2.15e-04 | norm: 1.0308 | dt: 13961.4928ms | tok/sec: 28.1643\n",
      "step  154 | train loss: 5.95 | val loss: 5.61 | perplexity: 273.23 | lr: 2.17e-04 | norm: 1.2151 | dt: 13971.1394ms | tok/sec: 28.1449\n",
      "step  155 | train loss: 5.80 | val loss: 5.60 | perplexity: 270.68 | lr: 2.18e-04 | norm: 0.8782 | dt: 13978.1954ms | tok/sec: 28.1307\n",
      "step  156 | train loss: 5.67 | val loss: 5.59 | perplexity: 268.21 | lr: 2.20e-04 | norm: 1.0829 | dt: 13999.0466ms | tok/sec: 28.0888\n",
      "step  157 | train loss: 6.04 | val loss: 5.60 | perplexity: 271.27 | lr: 2.21e-04 | norm: 0.9614 | dt: 14010.1933ms | tok/sec: 28.0664\n",
      "step  158 | train loss: 5.26 | val loss: 5.60 | perplexity: 271.60 | lr: 2.22e-04 | norm: 1.2900 | dt: 14003.5946ms | tok/sec: 28.0796\n",
      "step  159 | train loss: 5.79 | val loss: 5.59 | perplexity: 266.92 | lr: 2.24e-04 | norm: 1.1802 | dt: 14020.0765ms | tok/sec: 28.0466\n",
      "step  160 | train loss: 5.79 | val loss: 5.58 | perplexity: 264.92 | lr: 2.25e-04 | norm: 1.0695 | dt: 14016.3431ms | tok/sec: 28.0541\n",
      "step  161 | train loss: 5.97 | val loss: 5.59 | perplexity: 267.30 | lr: 2.27e-04 | norm: 1.2935 | dt: 14088.5494ms | tok/sec: 27.9103\n",
      "step  162 | train loss: 5.90 | val loss: 5.58 | perplexity: 265.20 | lr: 2.28e-04 | norm: 1.1526 | dt: 14081.1729ms | tok/sec: 27.9249\n",
      "step  163 | train loss: 5.68 | val loss: 5.59 | perplexity: 266.71 | lr: 2.29e-04 | norm: 0.8099 | dt: 13983.9475ms | tok/sec: 28.1191\n",
      "step  164 | train loss: 6.11 | val loss: 5.57 | perplexity: 261.96 | lr: 2.31e-04 | norm: 1.1445 | dt: 13995.4445ms | tok/sec: 28.0960\n",
      "step  165 | train loss: 5.82 | val loss: 5.57 | perplexity: 262.66 | lr: 2.32e-04 | norm: 1.0654 | dt: 13997.2990ms | tok/sec: 28.0923\n",
      "step  166 | train loss: 5.59 | val loss: 5.57 | perplexity: 261.31 | lr: 2.34e-04 | norm: 0.8337 | dt: 14076.5522ms | tok/sec: 27.9341\n",
      "step  167 | train loss: 5.92 | val loss: 5.56 | perplexity: 259.04 | lr: 2.35e-04 | norm: 0.7675 | dt: 14006.0444ms | tok/sec: 28.0747\n",
      "step  168 | train loss: 5.64 | val loss: 5.55 | perplexity: 256.42 | lr: 2.36e-04 | norm: 0.9028 | dt: 14026.0549ms | tok/sec: 28.0347\n",
      "step  169 | train loss: 5.66 | val loss: 5.54 | perplexity: 255.56 | lr: 2.38e-04 | norm: 0.9828 | dt: 14023.0329ms | tok/sec: 28.0407\n",
      "step  170 | train loss: 5.51 | val loss: 5.55 | perplexity: 256.77 | lr: 2.39e-04 | norm: 0.9536 | dt: 14000.6058ms | tok/sec: 28.0856\n",
      "step  171 | train loss: 5.85 | val loss: 5.53 | perplexity: 251.70 | lr: 2.41e-04 | norm: 0.9544 | dt: 13984.0393ms | tok/sec: 28.1189\n",
      "step  172 | train loss: 6.55 | val loss: 5.56 | perplexity: 259.76 | lr: 2.42e-04 | norm: 1.0582 | dt: 13977.4284ms | tok/sec: 28.1322\n",
      "step  173 | train loss: 5.49 | val loss: 5.54 | perplexity: 255.05 | lr: 2.43e-04 | norm: 2.0565 | dt: 14009.1252ms | tok/sec: 28.0686\n",
      "step  174 | train loss: 5.67 | val loss: 5.53 | perplexity: 252.68 | lr: 2.45e-04 | norm: 1.1549 | dt: 13980.6652ms | tok/sec: 28.1257\n",
      "step  175 | train loss: 5.86 | val loss: 5.55 | perplexity: 257.30 | lr: 2.46e-04 | norm: 1.0694 | dt: 14011.6003ms | tok/sec: 28.0636\n",
      "step  176 | train loss: 5.58 | val loss: 5.54 | perplexity: 254.96 | lr: 2.48e-04 | norm: 1.6306 | dt: 14118.9139ms | tok/sec: 27.8503\n",
      "step  177 | train loss: 5.73 | val loss: 5.52 | perplexity: 249.44 | lr: 2.49e-04 | norm: 0.9126 | dt: 14015.2140ms | tok/sec: 28.0564\n",
      "step  178 | train loss: 5.57 | val loss: 5.51 | perplexity: 246.52 | lr: 2.50e-04 | norm: 0.8088 | dt: 14009.6259ms | tok/sec: 28.0676\n",
      "step  179 | train loss: 5.41 | val loss: 5.50 | perplexity: 244.97 | lr: 2.52e-04 | norm: 0.9396 | dt: 13985.4517ms | tok/sec: 28.1161\n",
      "step  180 | train loss: 5.36 | val loss: 5.50 | perplexity: 244.28 | lr: 2.53e-04 | norm: 0.6273 | dt: 13987.3390ms | tok/sec: 28.1123\n",
      "step  181 | train loss: 5.53 | val loss: 5.50 | perplexity: 244.49 | lr: 2.55e-04 | norm: 0.6762 | dt: 13983.7787ms | tok/sec: 28.1194\n",
      "step  182 | train loss: 5.46 | val loss: 5.50 | perplexity: 244.47 | lr: 2.56e-04 | norm: 0.5609 | dt: 13993.7544ms | tok/sec: 28.0994\n",
      "step  183 | train loss: 5.81 | val loss: 5.50 | perplexity: 244.07 | lr: 2.57e-04 | norm: 1.1052 | dt: 14193.3997ms | tok/sec: 27.7041\n",
      "step  184 | train loss: 5.74 | val loss: 5.48 | perplexity: 241.01 | lr: 2.59e-04 | norm: 0.9878 | dt: 14005.3110ms | tok/sec: 28.0762\n",
      "step  185 | train loss: 5.66 | val loss: 5.48 | perplexity: 239.33 | lr: 2.60e-04 | norm: 0.8079 | dt: 14007.7755ms | tok/sec: 28.0713\n",
      "step  186 | train loss: 5.40 | val loss: 5.49 | perplexity: 241.50 | lr: 2.62e-04 | norm: 0.7012 | dt: 13981.9877ms | tok/sec: 28.1230\n",
      "step  187 | train loss: 5.93 | val loss: 5.48 | perplexity: 238.95 | lr: 2.63e-04 | norm: 1.3085 | dt: 13980.4256ms | tok/sec: 28.1262\n",
      "step  188 | train loss: 5.48 | val loss: 5.49 | perplexity: 241.31 | lr: 2.64e-04 | norm: 0.9004 | dt: 13969.9726ms | tok/sec: 28.1472\n",
      "step  189 | train loss: 5.69 | val loss: 5.50 | perplexity: 243.62 | lr: 2.66e-04 | norm: 0.8924 | dt: 13978.1029ms | tok/sec: 28.1309\n",
      "step  190 | train loss: 5.70 | val loss: 5.49 | perplexity: 241.19 | lr: 2.67e-04 | norm: 1.0955 | dt: 13977.1178ms | tok/sec: 28.1328\n",
      "step  191 | train loss: 6.04 | val loss: 5.48 | perplexity: 239.33 | lr: 2.69e-04 | norm: 0.9711 | dt: 14103.9200ms | tok/sec: 27.8799\n",
      "step  192 | train loss: 5.36 | val loss: 5.47 | perplexity: 237.17 | lr: 2.70e-04 | norm: 0.9835 | dt: 14016.3004ms | tok/sec: 28.0542\n",
      "step  193 | train loss: 5.64 | val loss: 5.48 | perplexity: 239.06 | lr: 2.71e-04 | norm: 1.0632 | dt: 14033.1140ms | tok/sec: 28.0206\n",
      "step  194 | train loss: 5.58 | val loss: 5.49 | perplexity: 241.32 | lr: 2.73e-04 | norm: 1.0159 | dt: 14009.5487ms | tok/sec: 28.0677\n",
      "step  195 | train loss: 5.59 | val loss: 5.48 | perplexity: 239.11 | lr: 2.74e-04 | norm: 0.8866 | dt: 14004.0255ms | tok/sec: 28.0788\n",
      "step  196 | train loss: 5.55 | val loss: 5.47 | perplexity: 236.93 | lr: 2.76e-04 | norm: 0.7789 | dt: 14171.7799ms | tok/sec: 27.7464\n",
      "step  197 | train loss: 5.34 | val loss: 5.47 | perplexity: 238.23 | lr: 2.77e-04 | norm: 0.9189 | dt: 13995.6744ms | tok/sec: 28.0955\n",
      "step  198 | train loss: 5.79 | val loss: 5.46 | perplexity: 235.82 | lr: 2.78e-04 | norm: 0.9114 | dt: 14006.8650ms | tok/sec: 28.0731\n",
      "step  199 | train loss: 5.74 | val loss: 5.44 | perplexity: 231.18 | lr: 2.80e-04 | norm: 0.9299 | dt: 14016.4902ms | tok/sec: 28.0538\n",
      "step  200 | train loss: 5.64 | val loss: 5.44 | perplexity: 230.10 | lr: 2.81e-04 | norm: 0.6305 | dt: 14052.4955ms | tok/sec: 27.9819\n",
      "step  201 | train loss: 5.60 | val loss: 5.41 | perplexity: 224.52 | lr: 2.83e-04 | norm: 0.9204 | dt: 14024.3299ms | tok/sec: 28.0381\n",
      "step  202 | train loss: 5.38 | val loss: 5.39 | perplexity: 218.57 | lr: 2.84e-04 | norm: 0.8577 | dt: 14027.5660ms | tok/sec: 28.0317\n",
      "step  203 | train loss: 5.64 | val loss: 5.39 | perplexity: 219.55 | lr: 2.85e-04 | norm: 0.7690 | dt: 14019.3119ms | tok/sec: 28.0482\n",
      "step  204 | train loss: 5.41 | val loss: 5.39 | perplexity: 219.42 | lr: 2.87e-04 | norm: 1.2453 | dt: 14107.0571ms | tok/sec: 27.8737\n",
      "step  205 | train loss: 5.57 | val loss: 5.39 | perplexity: 218.38 | lr: 2.88e-04 | norm: 0.8154 | dt: 14088.9530ms | tok/sec: 27.9095\n",
      "step  206 | train loss: 5.56 | val loss: 5.40 | perplexity: 221.05 | lr: 2.90e-04 | norm: 0.6645 | dt: 13977.0453ms | tok/sec: 28.1330\n",
      "step  207 | train loss: 5.90 | val loss: 5.42 | perplexity: 226.98 | lr: 2.91e-04 | norm: 0.6216 | dt: 13964.3545ms | tok/sec: 28.1586\n",
      "step  208 | train loss: 5.60 | val loss: 5.43 | perplexity: 228.64 | lr: 2.92e-04 | norm: 0.7132 | dt: 13998.3854ms | tok/sec: 28.0901\n",
      "step  209 | train loss: 5.56 | val loss: 5.42 | perplexity: 226.04 | lr: 2.94e-04 | norm: 0.8149 | dt: 14081.7471ms | tok/sec: 27.9238\n",
      "step  210 | train loss: 5.39 | val loss: 5.41 | perplexity: 223.41 | lr: 2.95e-04 | norm: 0.8570 | dt: 14043.0615ms | tok/sec: 28.0007\n",
      "step  211 | train loss: 5.56 | val loss: 5.41 | perplexity: 222.67 | lr: 2.97e-04 | norm: 0.8527 | dt: 14030.9179ms | tok/sec: 28.0250\n",
      "step  212 | train loss: 5.49 | val loss: 5.40 | perplexity: 221.47 | lr: 2.98e-04 | norm: 0.7309 | dt: 14025.0576ms | tok/sec: 28.0367\n",
      "step  213 | train loss: 5.71 | val loss: 5.39 | perplexity: 219.38 | lr: 2.99e-04 | norm: 0.5921 | dt: 14132.3924ms | tok/sec: 27.8237\n",
      "step  214 | train loss: 5.48 | val loss: 5.38 | perplexity: 217.09 | lr: 3.01e-04 | norm: 0.6713 | dt: 14034.2224ms | tok/sec: 28.0184\n",
      "step  215 | train loss: 5.49 | val loss: 5.37 | perplexity: 215.45 | lr: 3.02e-04 | norm: 0.6511 | dt: 14015.8486ms | tok/sec: 28.0551\n",
      "step  216 | train loss: 5.80 | val loss: 5.37 | perplexity: 215.17 | lr: 3.03e-04 | norm: 0.6723 | dt: 13992.3234ms | tok/sec: 28.1023\n",
      "step  217 | train loss: 5.67 | val loss: 5.36 | perplexity: 212.88 | lr: 3.05e-04 | norm: 0.6757 | dt: 14006.3539ms | tok/sec: 28.0741\n",
      "step  218 | train loss: 5.42 | val loss: 5.37 | perplexity: 214.74 | lr: 3.06e-04 | norm: 0.7060 | dt: 13976.0120ms | tok/sec: 28.1351\n",
      "step  219 | train loss: 5.43 | val loss: 5.37 | perplexity: 213.80 | lr: 3.08e-04 | norm: 0.8576 | dt: 13978.2000ms | tok/sec: 28.1307\n",
      "step  220 | train loss: 6.29 | val loss: 5.38 | perplexity: 217.54 | lr: 3.09e-04 | norm: 1.8065 | dt: 14096.1895ms | tok/sec: 27.8952\n",
      "step  221 | train loss: 5.64 | val loss: 5.40 | perplexity: 221.75 | lr: 3.10e-04 | norm: 1.0189 | dt: 14010.2577ms | tok/sec: 28.0663\n",
      "step  222 | train loss: 5.50 | val loss: 5.39 | perplexity: 219.61 | lr: 3.12e-04 | norm: 0.8596 | dt: 14114.1264ms | tok/sec: 27.8597\n",
      "step  223 | train loss: 5.65 | val loss: 5.38 | perplexity: 217.50 | lr: 3.13e-04 | norm: 1.0566 | dt: 14009.4273ms | tok/sec: 28.0680\n",
      "step  224 | train loss: 5.70 | val loss: 5.37 | perplexity: 214.92 | lr: 3.15e-04 | norm: 1.9773 | dt: 14101.3806ms | tok/sec: 27.8849\n",
      "step  225 | train loss: 5.50 | val loss: 5.38 | perplexity: 215.97 | lr: 3.16e-04 | norm: 0.7936 | dt: 13564.2564ms | tok/sec: 28.9891\n",
      "step  226 | train loss: 5.86 | val loss: 5.38 | perplexity: 217.76 | lr: 3.17e-04 | norm: 0.8426 | dt: 13736.3653ms | tok/sec: 28.6259\n",
      "step  227 | train loss: 5.75 | val loss: 5.38 | perplexity: 216.30 | lr: 3.19e-04 | norm: 0.9032 | dt: 13591.3424ms | tok/sec: 28.9314\n",
      "step  228 | train loss: 5.52 | val loss: 5.37 | perplexity: 215.05 | lr: 3.20e-04 | norm: 0.6742 | dt: 13645.2570ms | tok/sec: 28.8170\n",
      "step  229 | train loss: 5.66 | val loss: 5.36 | perplexity: 212.03 | lr: 3.22e-04 | norm: 0.9734 | dt: 13660.2213ms | tok/sec: 28.7855\n",
      "step  230 | train loss: 5.80 | val loss: 5.36 | perplexity: 213.27 | lr: 3.23e-04 | norm: 0.6328 | dt: 13655.4992ms | tok/sec: 28.7954\n",
      "step  231 | train loss: 5.52 | val loss: 5.36 | perplexity: 213.55 | lr: 3.24e-04 | norm: 0.9497 | dt: 13694.7622ms | tok/sec: 28.7129\n",
      "step  232 | train loss: 5.88 | val loss: 5.38 | perplexity: 216.36 | lr: 3.26e-04 | norm: 1.0506 | dt: 13647.7594ms | tok/sec: 28.8118\n",
      "step  233 | train loss: 5.74 | val loss: 5.38 | perplexity: 217.11 | lr: 3.27e-04 | norm: 0.9032 | dt: 13655.3850ms | tok/sec: 28.7957\n",
      "step  234 | train loss: 5.51 | val loss: 5.38 | perplexity: 216.32 | lr: 3.29e-04 | norm: 0.9591 | dt: 13636.2777ms | tok/sec: 28.8360\n",
      "step  235 | train loss: 5.70 | val loss: 5.39 | perplexity: 219.78 | lr: 3.30e-04 | norm: 0.8127 | dt: 13751.6563ms | tok/sec: 28.5941\n",
      "step  236 | train loss: 5.32 | val loss: 5.38 | perplexity: 216.54 | lr: 3.31e-04 | norm: 0.8984 | dt: 13666.1861ms | tok/sec: 28.7729\n",
      "step  237 | train loss: 5.71 | val loss: 5.37 | perplexity: 215.15 | lr: 3.33e-04 | norm: 0.9861 | dt: 13621.4507ms | tok/sec: 28.8674\n",
      "step  238 | train loss: 5.89 | val loss: 5.38 | perplexity: 216.82 | lr: 3.34e-04 | norm: 0.6791 | dt: 13663.7092ms | tok/sec: 28.7781\n",
      "step  239 | train loss: 5.79 | val loss: 5.37 | perplexity: 214.36 | lr: 3.36e-04 | norm: 1.2037 | dt: 13652.8292ms | tok/sec: 28.8011\n",
      "step  240 | train loss: 5.56 | val loss: 5.37 | perplexity: 215.10 | lr: 3.37e-04 | norm: 0.6952 | dt: 13651.8595ms | tok/sec: 28.8031\n",
      "step  241 | train loss: 5.79 | val loss: 5.36 | perplexity: 212.56 | lr: 3.38e-04 | norm: 1.1533 | dt: 13772.8975ms | tok/sec: 28.5500\n",
      "step  242 | train loss: 5.57 | val loss: 5.35 | perplexity: 211.59 | lr: 3.40e-04 | norm: 0.7764 | dt: 13672.0905ms | tok/sec: 28.7605\n",
      "step  243 | train loss: 5.70 | val loss: 5.34 | perplexity: 208.38 | lr: 3.41e-04 | norm: 0.7962 | dt: 13652.4081ms | tok/sec: 28.8020\n",
      "step  244 | train loss: 5.48 | val loss: 5.33 | perplexity: 206.08 | lr: 3.43e-04 | norm: 0.6372 | dt: 13647.2161ms | tok/sec: 28.8129\n",
      "step  245 | train loss: 5.69 | val loss: 5.31 | perplexity: 203.33 | lr: 3.44e-04 | norm: 0.6279 | dt: 13627.9790ms | tok/sec: 28.8536\n",
      "step  246 | train loss: 5.41 | val loss: 5.32 | perplexity: 204.39 | lr: 3.45e-04 | norm: 0.5690 | dt: 13731.7185ms | tok/sec: 28.6356\n",
      "step  247 | train loss: 5.62 | val loss: 5.32 | perplexity: 203.74 | lr: 3.47e-04 | norm: 0.6897 | dt: 13666.5285ms | tok/sec: 28.7722\n",
      "step  248 | train loss: 5.63 | val loss: 5.30 | perplexity: 200.41 | lr: 3.48e-04 | norm: 0.6595 | dt: 13679.9419ms | tok/sec: 28.7440\n",
      "step  249 | train loss: 5.56 | val loss: 5.30 | perplexity: 200.50 | lr: 3.50e-04 | norm: 0.7306 | dt: 13646.9579ms | tok/sec: 28.8135\n",
      "step  250 | train loss: 5.06 | val loss: 5.30 | perplexity: 200.01 | lr: 3.51e-04 | norm: 0.7070 | dt: 13668.9873ms | tok/sec: 28.7670\n",
      "step  251 | train loss: 5.55 | val loss: 5.30 | perplexity: 200.32 | lr: 3.52e-04 | norm: 0.6073 | dt: 13712.9061ms | tok/sec: 28.6749\n",
      "step  252 | train loss: 5.52 | val loss: 5.30 | perplexity: 199.98 | lr: 3.54e-04 | norm: 0.7287 | dt: 13647.6793ms | tok/sec: 28.8119\n",
      "step  253 | train loss: 5.66 | val loss: 5.30 | perplexity: 200.19 | lr: 3.55e-04 | norm: 0.4983 | dt: 13681.8991ms | tok/sec: 28.7399\n",
      "step  254 | train loss: 5.65 | val loss: 5.29 | perplexity: 198.04 | lr: 3.57e-04 | norm: 0.6201 | dt: 13641.9611ms | tok/sec: 28.8240\n",
      "step  255 | train loss: 5.59 | val loss: 5.28 | perplexity: 196.60 | lr: 3.58e-04 | norm: 1.0014 | dt: 13683.0952ms | tok/sec: 28.7374\n",
      "step  256 | train loss: 5.53 | val loss: 5.28 | perplexity: 196.80 | lr: 3.59e-04 | norm: 0.6421 | dt: 13626.7366ms | tok/sec: 28.8562\n",
      "step  257 | train loss: 5.39 | val loss: 5.30 | perplexity: 199.41 | lr: 3.61e-04 | norm: 0.7583 | dt: 13644.4309ms | tok/sec: 28.8188\n",
      "step  258 | train loss: 5.30 | val loss: 5.29 | perplexity: 198.22 | lr: 3.62e-04 | norm: 0.9414 | dt: 13686.2838ms | tok/sec: 28.7307\n",
      "step  259 | train loss: 5.36 | val loss: 5.28 | perplexity: 196.34 | lr: 3.64e-04 | norm: 0.7029 | dt: 13614.5484ms | tok/sec: 28.8820\n",
      "step  260 | train loss: 5.55 | val loss: 5.28 | perplexity: 196.50 | lr: 3.65e-04 | norm: 0.6231 | dt: 13651.1905ms | tok/sec: 28.8045\n",
      "step  261 | train loss: 5.52 | val loss: 5.28 | perplexity: 196.65 | lr: 3.66e-04 | norm: 0.7930 | dt: 13570.9293ms | tok/sec: 28.9749\n",
      "step  262 | train loss: 5.34 | val loss: 5.27 | perplexity: 195.09 | lr: 3.68e-04 | norm: 0.8958 | dt: 13670.2292ms | tok/sec: 28.7644\n",
      "step  263 | train loss: 5.46 | val loss: 5.28 | perplexity: 195.54 | lr: 3.69e-04 | norm: 0.5485 | dt: 13624.6309ms | tok/sec: 28.8607\n",
      "step  264 | train loss: 5.52 | val loss: 5.28 | perplexity: 195.58 | lr: 3.71e-04 | norm: 0.7337 | dt: 13642.3819ms | tok/sec: 28.8231\n",
      "step  265 | train loss: 5.40 | val loss: 5.27 | perplexity: 193.89 | lr: 3.72e-04 | norm: 0.5845 | dt: 13784.2863ms | tok/sec: 28.5264\n",
      "step  266 | train loss: 5.45 | val loss: 5.28 | perplexity: 195.44 | lr: 3.73e-04 | norm: 0.7977 | dt: 13660.9690ms | tok/sec: 28.7839\n",
      "step  267 | train loss: 5.27 | val loss: 5.27 | perplexity: 194.51 | lr: 3.75e-04 | norm: 0.7622 | dt: 13644.9373ms | tok/sec: 28.8177\n",
      "step  268 | train loss: 5.54 | val loss: 5.29 | perplexity: 198.15 | lr: 3.76e-04 | norm: 0.6444 | dt: 13680.8255ms | tok/sec: 28.7421\n",
      "step  269 | train loss: 5.82 | val loss: 5.29 | perplexity: 198.85 | lr: 3.78e-04 | norm: 0.7965 | dt: 13666.6002ms | tok/sec: 28.7720\n",
      "step  270 | train loss: 5.36 | val loss: 5.29 | perplexity: 198.29 | lr: 3.79e-04 | norm: 0.9369 | dt: 13664.9437ms | tok/sec: 28.7755\n",
      "step  271 | train loss: 5.16 | val loss: 5.29 | perplexity: 198.79 | lr: 3.80e-04 | norm: 0.7273 | dt: 13652.9257ms | tok/sec: 28.8009\n",
      "step  272 | train loss: 5.29 | val loss: 5.29 | perplexity: 198.79 | lr: 3.82e-04 | norm: 0.7338 | dt: 13646.6420ms | tok/sec: 28.8141\n",
      "step  273 | train loss: 5.34 | val loss: 5.30 | perplexity: 199.61 | lr: 3.83e-04 | norm: 0.6715 | dt: 13645.8125ms | tok/sec: 28.8159\n",
      "step  274 | train loss: 5.33 | val loss: 5.29 | perplexity: 197.85 | lr: 3.85e-04 | norm: 0.7868 | dt: 13672.8423ms | tok/sec: 28.7589\n",
      "step  275 | train loss: 5.16 | val loss: 5.29 | perplexity: 198.44 | lr: 3.86e-04 | norm: 0.6516 | dt: 13633.3354ms | tok/sec: 28.8422\n",
      "step  276 | train loss: 5.80 | val loss: 5.30 | perplexity: 199.78 | lr: 3.87e-04 | norm: 0.8199 | dt: 13630.5828ms | tok/sec: 28.8481\n",
      "step  277 | train loss: 5.57 | val loss: 5.30 | perplexity: 201.10 | lr: 3.89e-04 | norm: 0.7414 | dt: 13660.0266ms | tok/sec: 28.7859\n",
      "step  278 | train loss: 5.34 | val loss: 5.30 | perplexity: 200.84 | lr: 3.90e-04 | norm: 0.7401 | dt: 13821.8942ms | tok/sec: 28.4488\n",
      "step  279 | train loss: 5.45 | val loss: 5.30 | perplexity: 200.45 | lr: 3.92e-04 | norm: 0.6486 | dt: 13638.0620ms | tok/sec: 28.8322\n",
      "step  280 | train loss: 5.72 | val loss: 5.30 | perplexity: 200.19 | lr: 3.93e-04 | norm: 0.9722 | dt: 13624.3114ms | tok/sec: 28.8613\n",
      "step  281 | train loss: 5.54 | val loss: 5.29 | perplexity: 199.20 | lr: 3.94e-04 | norm: 0.7311 | dt: 13646.6434ms | tok/sec: 28.8141\n",
      "step  282 | train loss: 5.07 | val loss: 5.29 | perplexity: 197.80 | lr: 3.96e-04 | norm: 0.7631 | dt: 13819.9849ms | tok/sec: 28.4527\n",
      "step  283 | train loss: 5.45 | val loss: 5.29 | perplexity: 198.12 | lr: 3.97e-04 | norm: 0.7812 | dt: 13648.0842ms | tok/sec: 28.8111\n",
      "step  284 | train loss: 5.45 | val loss: 5.27 | perplexity: 194.65 | lr: 3.99e-04 | norm: 0.7202 | dt: 13641.2132ms | tok/sec: 28.8256\n",
      "step  285 | train loss: 5.53 | val loss: 5.28 | perplexity: 195.86 | lr: 4.00e-04 | norm: 0.6480 | dt: 13656.4722ms | tok/sec: 28.7934\n",
      "step  286 | train loss: 5.31 | val loss: 5.27 | perplexity: 194.21 | lr: 4.01e-04 | norm: 0.8259 | dt: 13636.8499ms | tok/sec: 28.8348\n",
      "step  287 | train loss: 5.27 | val loss: 5.26 | perplexity: 192.62 | lr: 4.03e-04 | norm: 0.7549 | dt: 13662.4644ms | tok/sec: 28.7808\n",
      "step  288 | train loss: 5.22 | val loss: 5.27 | perplexity: 194.04 | lr: 4.04e-04 | norm: 0.5955 | dt: 13621.1934ms | tok/sec: 28.8680\n",
      "step  289 | train loss: 5.52 | val loss: 5.27 | perplexity: 194.15 | lr: 4.06e-04 | norm: 0.6631 | dt: 13677.6104ms | tok/sec: 28.7489\n",
      "step  290 | train loss: 5.10 | val loss: 5.27 | perplexity: 194.88 | lr: 4.07e-04 | norm: 0.8234 | dt: 13663.4552ms | tok/sec: 28.7787\n",
      "step  291 | train loss: 5.51 | val loss: 5.27 | perplexity: 194.63 | lr: 4.08e-04 | norm: 0.6433 | dt: 13640.3592ms | tok/sec: 28.8274\n",
      "step  292 | train loss: 5.67 | val loss: 5.27 | perplexity: 193.55 | lr: 4.10e-04 | norm: 0.6706 | dt: 13635.7512ms | tok/sec: 28.8371\n",
      "step  293 | train loss: 5.32 | val loss: 5.24 | perplexity: 188.88 | lr: 4.11e-04 | norm: 0.6387 | dt: 13787.3929ms | tok/sec: 28.5200\n",
      "step  294 | train loss: 5.25 | val loss: 5.24 | perplexity: 188.49 | lr: 4.13e-04 | norm: 0.5658 | dt: 13648.5276ms | tok/sec: 28.8101\n",
      "step  295 | train loss: 5.25 | val loss: 5.23 | perplexity: 187.15 | lr: 4.14e-04 | norm: 0.5992 | dt: 13730.3054ms | tok/sec: 28.6385\n",
      "step  296 | train loss: 5.28 | val loss: 5.22 | perplexity: 184.60 | lr: 4.15e-04 | norm: 0.5424 | dt: 13679.6994ms | tok/sec: 28.7445\n",
      "step  297 | train loss: 6.00 | val loss: 5.24 | perplexity: 187.79 | lr: 4.17e-04 | norm: 0.7945 | dt: 13646.2691ms | tok/sec: 28.8149\n",
      "step  298 | train loss: 5.61 | val loss: 5.23 | perplexity: 187.51 | lr: 4.18e-04 | norm: 0.7699 | dt: 13643.8758ms | tok/sec: 28.8200\n",
      "step  299 | train loss: 5.39 | val loss: 5.23 | perplexity: 186.26 | lr: 4.20e-04 | norm: 0.5383 | dt: 13624.1031ms | tok/sec: 28.8618\n",
      "step  300 | train loss: 5.43 | val loss: 5.21 | perplexity: 183.35 | lr: 4.21e-04 | norm: 0.7624 | dt: 13700.0213ms | tok/sec: 28.7019\n",
      "step  301 | train loss: 5.16 | val loss: 5.22 | perplexity: 184.88 | lr: 4.22e-04 | norm: 0.5049 | dt: 13628.4127ms | tok/sec: 28.8527\n",
      "step  302 | train loss: 5.54 | val loss: 5.21 | perplexity: 182.40 | lr: 4.24e-04 | norm: 0.6771 | dt: 13645.5755ms | tok/sec: 28.8164\n",
      "step  303 | train loss: 5.27 | val loss: 5.21 | perplexity: 183.92 | lr: 4.25e-04 | norm: 0.8801 | dt: 13636.5757ms | tok/sec: 28.8354\n",
      "step  304 | train loss: 5.24 | val loss: 5.22 | perplexity: 184.99 | lr: 4.27e-04 | norm: 0.7179 | dt: 13595.2141ms | tok/sec: 28.9231\n",
      "step  305 | train loss: 5.30 | val loss: 5.22 | perplexity: 184.37 | lr: 4.28e-04 | norm: 0.7013 | dt: 13647.0580ms | tok/sec: 28.8132\n",
      "step  306 | train loss: 5.31 | val loss: 5.22 | perplexity: 184.48 | lr: 4.29e-04 | norm: 0.6264 | dt: 13637.8274ms | tok/sec: 28.8327\n",
      "step  307 | train loss: 5.13 | val loss: 5.21 | perplexity: 182.96 | lr: 4.31e-04 | norm: 0.7146 | dt: 13754.4641ms | tok/sec: 28.5882\n",
      "step  308 | train loss: 5.31 | val loss: 5.20 | perplexity: 180.48 | lr: 4.32e-04 | norm: 0.6382 | dt: 13631.2385ms | tok/sec: 28.8467\n",
      "step  309 | train loss: 5.51 | val loss: 5.21 | perplexity: 182.62 | lr: 4.34e-04 | norm: 0.5136 | dt: 13676.8348ms | tok/sec: 28.7505\n",
      "step  310 | train loss: 5.07 | val loss: 5.19 | perplexity: 179.43 | lr: 4.35e-04 | norm: 1.0188 | dt: 13650.6093ms | tok/sec: 28.8057\n",
      "step  311 | train loss: 5.25 | val loss: 5.20 | perplexity: 180.60 | lr: 4.36e-04 | norm: 0.5834 | dt: 13626.8904ms | tok/sec: 28.8559\n",
      "step  312 | train loss: 5.46 | val loss: 5.21 | perplexity: 182.60 | lr: 4.38e-04 | norm: 0.6375 | dt: 13751.0526ms | tok/sec: 28.5953\n",
      "step  313 | train loss: 5.29 | val loss: 5.21 | perplexity: 182.38 | lr: 4.39e-04 | norm: 0.5889 | dt: 13611.7916ms | tok/sec: 28.8879\n",
      "step  314 | train loss: 5.62 | val loss: 5.22 | perplexity: 184.95 | lr: 4.41e-04 | norm: 0.6922 | dt: 13618.9494ms | tok/sec: 28.8727\n",
      "step  315 | train loss: 5.34 | val loss: 5.21 | perplexity: 183.47 | lr: 4.42e-04 | norm: 0.6229 | dt: 13746.9926ms | tok/sec: 28.6038\n",
      "step  316 | train loss: 5.28 | val loss: 5.21 | perplexity: 182.20 | lr: 4.43e-04 | norm: 0.5157 | dt: 13615.9101ms | tok/sec: 28.8792\n",
      "step  317 | train loss: 5.36 | val loss: 5.21 | perplexity: 182.24 | lr: 4.45e-04 | norm: 0.6653 | dt: 13562.0458ms | tok/sec: 28.9939\n",
      "step  318 | train loss: 5.50 | val loss: 5.20 | perplexity: 180.95 | lr: 4.46e-04 | norm: 0.6100 | dt: 13603.5070ms | tok/sec: 28.9055\n",
      "step  319 | train loss: 5.68 | val loss: 5.23 | perplexity: 185.99 | lr: 4.48e-04 | norm: 0.9086 | dt: 13611.6700ms | tok/sec: 28.8882\n",
      "step  320 | train loss: 5.29 | val loss: 5.22 | perplexity: 184.23 | lr: 4.49e-04 | norm: 0.8169 | dt: 13646.5993ms | tok/sec: 28.8142\n",
      "step  321 | train loss: 5.20 | val loss: 5.21 | perplexity: 183.31 | lr: 4.50e-04 | norm: 0.6846 | dt: 13626.7161ms | tok/sec: 28.8563\n",
      "step  322 | train loss: 5.17 | val loss: 5.22 | perplexity: 184.50 | lr: 4.52e-04 | norm: 0.7063 | dt: 13648.8612ms | tok/sec: 28.8094\n",
      "step  323 | train loss: 5.36 | val loss: 5.20 | perplexity: 181.27 | lr: 4.53e-04 | norm: 0.6840 | dt: 13615.2565ms | tok/sec: 28.8805\n",
      "step  324 | train loss: 5.35 | val loss: 5.19 | perplexity: 179.96 | lr: 4.55e-04 | norm: 0.5603 | dt: 13645.4892ms | tok/sec: 28.8166\n",
      "step  325 | train loss: 5.51 | val loss: 5.19 | perplexity: 178.75 | lr: 4.56e-04 | norm: 0.6758 | dt: 13612.8612ms | tok/sec: 28.8856\n",
      "step  326 | train loss: 5.41 | val loss: 5.19 | perplexity: 180.28 | lr: 4.57e-04 | norm: 0.6006 | dt: 13619.0975ms | tok/sec: 28.8724\n",
      "step  327 | train loss: 5.61 | val loss: 5.19 | perplexity: 178.59 | lr: 4.59e-04 | norm: 0.7748 | dt: 13667.5365ms | tok/sec: 28.7701\n",
      "step  328 | train loss: 5.73 | val loss: 5.19 | perplexity: 179.00 | lr: 4.60e-04 | norm: 0.5476 | dt: 13619.7348ms | tok/sec: 28.8710\n",
      "step  329 | train loss: 5.25 | val loss: 5.17 | perplexity: 176.33 | lr: 4.62e-04 | norm: 0.6173 | dt: 13607.3048ms | tok/sec: 28.8974\n",
      "step  330 | train loss: 5.01 | val loss: 5.17 | perplexity: 175.86 | lr: 4.63e-04 | norm: 0.5107 | dt: 13603.4260ms | tok/sec: 28.9057\n",
      "step  331 | train loss: 5.05 | val loss: 5.16 | perplexity: 173.51 | lr: 4.64e-04 | norm: 0.6019 | dt: 13639.4999ms | tok/sec: 28.8292\n",
      "step  332 | train loss: 5.46 | val loss: 5.16 | perplexity: 174.00 | lr: 4.66e-04 | norm: 0.6407 | dt: 13637.3250ms | tok/sec: 28.8338\n",
      "step  333 | train loss: 5.35 | val loss: 5.16 | perplexity: 173.31 | lr: 4.67e-04 | norm: 0.4750 | dt: 13653.4743ms | tok/sec: 28.7997\n",
      "step  334 | train loss: 5.14 | val loss: 5.16 | perplexity: 173.49 | lr: 4.69e-04 | norm: 0.5236 | dt: 13630.0948ms | tok/sec: 28.8491\n",
      "step  335 | train loss: 5.30 | val loss: 5.16 | perplexity: 174.25 | lr: 4.70e-04 | norm: 0.5892 | dt: 13788.0585ms | tok/sec: 28.5186\n",
      "step  336 | train loss: 5.31 | val loss: 5.17 | perplexity: 175.25 | lr: 4.71e-04 | norm: 0.6267 | dt: 13794.8132ms | tok/sec: 28.5046\n",
      "step  337 | train loss: 5.16 | val loss: 5.18 | perplexity: 177.72 | lr: 4.73e-04 | norm: 0.5230 | dt: 13716.8667ms | tok/sec: 28.6666\n",
      "step  338 | train loss: 5.20 | val loss: 5.16 | perplexity: 174.73 | lr: 4.74e-04 | norm: 0.8201 | dt: 13622.0975ms | tok/sec: 28.8660\n",
      "step  339 | train loss: 5.32 | val loss: 5.17 | perplexity: 176.15 | lr: 4.76e-04 | norm: 0.4362 | dt: 13697.5968ms | tok/sec: 28.7069\n",
      "step  340 | train loss: 5.45 | val loss: 5.16 | perplexity: 174.35 | lr: 4.77e-04 | norm: 0.6169 | dt: 13635.5667ms | tok/sec: 28.8375\n",
      "step  341 | train loss: 5.35 | val loss: 5.17 | perplexity: 175.42 | lr: 4.78e-04 | norm: 0.4387 | dt: 13685.8139ms | tok/sec: 28.7316\n",
      "step  342 | train loss: 5.30 | val loss: 5.16 | perplexity: 173.83 | lr: 4.80e-04 | norm: 0.8205 | dt: 13657.9847ms | tok/sec: 28.7902\n",
      "step  343 | train loss: 5.89 | val loss: 5.19 | perplexity: 179.53 | lr: 4.81e-04 | norm: 1.3091 | dt: 13626.7834ms | tok/sec: 28.8561\n",
      "step  344 | train loss: 5.22 | val loss: 5.22 | perplexity: 185.22 | lr: 4.83e-04 | norm: 1.2571 | dt: 13624.9385ms | tok/sec: 28.8600\n",
      "step  345 | train loss: 5.54 | val loss: 5.16 | perplexity: 174.59 | lr: 4.84e-04 | norm: 0.9752 | dt: 13669.7292ms | tok/sec: 28.7655\n",
      "step  346 | train loss: 5.51 | val loss: 5.18 | perplexity: 178.45 | lr: 4.85e-04 | norm: 0.7467 | dt: 13674.3352ms | tok/sec: 28.7558\n",
      "step  347 | train loss: 5.59 | val loss: 5.18 | perplexity: 178.00 | lr: 4.87e-04 | norm: 0.5445 | dt: 13607.8517ms | tok/sec: 28.8963\n",
      "step  348 | train loss: 5.35 | val loss: 5.19 | perplexity: 178.93 | lr: 4.88e-04 | norm: 1.0241 | dt: 13819.8106ms | tok/sec: 28.4531\n",
      "step  349 | train loss: 5.32 | val loss: 5.19 | perplexity: 179.96 | lr: 4.90e-04 | norm: 0.5161 | dt: 13651.9942ms | tok/sec: 28.8028\n",
      "step  350 | train loss: 5.19 | val loss: 5.18 | perplexity: 176.91 | lr: 4.91e-04 | norm: 0.6161 | dt: 13641.4924ms | tok/sec: 28.8250\n",
      "step  351 | train loss: 5.31 | val loss: 5.17 | perplexity: 176.60 | lr: 4.92e-04 | norm: 0.5128 | dt: 13652.4854ms | tok/sec: 28.8018\n",
      "step  352 | train loss: 5.33 | val loss: 5.17 | perplexity: 175.48 | lr: 4.94e-04 | norm: 0.5758 | dt: 13699.1420ms | tok/sec: 28.7037\n",
      "step  353 | train loss: 5.52 | val loss: 5.15 | perplexity: 173.08 | lr: 4.95e-04 | norm: 0.5331 | dt: 13669.8487ms | tok/sec: 28.7652\n",
      "step  354 | train loss: 5.59 | val loss: 5.16 | perplexity: 173.80 | lr: 4.97e-04 | norm: 0.5326 | dt: 13621.5270ms | tok/sec: 28.8672\n",
      "step  355 | train loss: 5.35 | val loss: 5.16 | perplexity: 174.99 | lr: 4.98e-04 | norm: 0.5454 | dt: 13782.0747ms | tok/sec: 28.5310\n",
      "step  356 | train loss: 5.51 | val loss: 5.16 | perplexity: 174.85 | lr: 4.99e-04 | norm: 0.4806 | dt: 13640.5582ms | tok/sec: 28.8270\n",
      "step  357 | train loss: 5.13 | val loss: 5.15 | perplexity: 172.13 | lr: 5.01e-04 | norm: 0.5792 | dt: 13636.4174ms | tok/sec: 28.8357\n",
      "step  358 | train loss: 5.49 | val loss: 5.15 | perplexity: 171.90 | lr: 5.02e-04 | norm: 0.6424 | dt: 13662.1721ms | tok/sec: 28.7814\n",
      "step  359 | train loss: 5.56 | val loss: 5.15 | perplexity: 172.32 | lr: 5.03e-04 | norm: 0.4895 | dt: 13644.2776ms | tok/sec: 28.8191\n",
      "step  360 | train loss: 5.27 | val loss: 5.14 | perplexity: 170.20 | lr: 5.05e-04 | norm: 0.6098 | dt: 13659.7157ms | tok/sec: 28.7865\n",
      "step  361 | train loss: 5.68 | val loss: 5.15 | perplexity: 171.63 | lr: 5.06e-04 | norm: 0.4890 | dt: 13633.2791ms | tok/sec: 28.8424\n",
      "step  362 | train loss: 5.59 | val loss: 5.15 | perplexity: 172.30 | lr: 5.08e-04 | norm: 0.4586 | dt: 13630.7065ms | tok/sec: 28.8478\n",
      "step  363 | train loss: 5.26 | val loss: 5.14 | perplexity: 170.57 | lr: 5.09e-04 | norm: 0.4323 | dt: 13617.2245ms | tok/sec: 28.8764\n",
      "step  364 | train loss: 5.28 | val loss: 5.12 | perplexity: 167.37 | lr: 5.10e-04 | norm: 0.3983 | dt: 13663.5337ms | tok/sec: 28.7785\n",
      "step  365 | train loss: 5.29 | val loss: 5.11 | perplexity: 165.64 | lr: 5.12e-04 | norm: 0.5515 | dt: 13617.4393ms | tok/sec: 28.8759\n",
      "step  366 | train loss: 5.24 | val loss: 5.11 | perplexity: 165.49 | lr: 5.13e-04 | norm: 0.5513 | dt: 13698.6892ms | tok/sec: 28.7046\n",
      "step  367 | train loss: 5.36 | val loss: 5.13 | perplexity: 169.39 | lr: 5.15e-04 | norm: 0.6017 | dt: 13690.0883ms | tok/sec: 28.7227\n",
      "step  368 | train loss: 5.44 | val loss: 5.13 | perplexity: 169.35 | lr: 5.16e-04 | norm: 0.7434 | dt: 13659.8499ms | tok/sec: 28.7863\n",
      "step  369 | train loss: 5.79 | val loss: 5.17 | perplexity: 175.45 | lr: 5.17e-04 | norm: 0.6156 | dt: 13640.3830ms | tok/sec: 28.8273\n",
      "step  370 | train loss: 5.27 | val loss: 5.14 | perplexity: 170.83 | lr: 5.19e-04 | norm: 0.7800 | dt: 13667.0144ms | tok/sec: 28.7712\n",
      "step  371 | train loss: 5.56 | val loss: 5.13 | perplexity: 169.19 | lr: 5.20e-04 | norm: 0.5281 | dt: 13680.2926ms | tok/sec: 28.7432\n",
      "step  372 | train loss: 5.44 | val loss: 5.13 | perplexity: 168.29 | lr: 5.22e-04 | norm: 0.5049 | dt: 13669.6815ms | tok/sec: 28.7656\n",
      "step  373 | train loss: 5.23 | val loss: 5.12 | perplexity: 166.75 | lr: 5.23e-04 | norm: 0.6031 | dt: 13796.6723ms | tok/sec: 28.5008\n",
      "step  374 | train loss: 5.31 | val loss: 5.10 | perplexity: 164.79 | lr: 5.24e-04 | norm: 0.7746 | dt: 13634.3708ms | tok/sec: 28.8401\n",
      "step  375 | train loss: 5.14 | val loss: 5.11 | perplexity: 165.57 | lr: 5.26e-04 | norm: 0.6849 | dt: 13814.1649ms | tok/sec: 28.4647\n",
      "step  376 | train loss: 5.15 | val loss: 5.14 | perplexity: 170.79 | lr: 5.27e-04 | norm: 0.6069 | dt: 13592.6299ms | tok/sec: 28.9286\n",
      "step  377 | train loss: 5.44 | val loss: 5.12 | perplexity: 166.50 | lr: 5.29e-04 | norm: 0.9527 | dt: 13613.8988ms | tok/sec: 28.8834\n",
      "step  378 | train loss: 5.15 | val loss: 5.11 | perplexity: 165.60 | lr: 5.30e-04 | norm: 0.6261 | dt: 13679.6026ms | tok/sec: 28.7447\n",
      "step  379 | train loss: 5.20 | val loss: 5.12 | perplexity: 166.67 | lr: 5.31e-04 | norm: 0.6881 | dt: 13633.3020ms | tok/sec: 28.8423\n",
      "step  380 | train loss: 5.19 | val loss: 5.11 | perplexity: 165.89 | lr: 5.33e-04 | norm: 0.5225 | dt: 13718.7731ms | tok/sec: 28.6626\n",
      "step  381 | train loss: 5.30 | val loss: 5.10 | perplexity: 164.47 | lr: 5.34e-04 | norm: 0.4774 | dt: 13617.8572ms | tok/sec: 28.8750\n",
      "step  382 | train loss: 5.22 | val loss: 5.09 | perplexity: 162.73 | lr: 5.36e-04 | norm: 0.5362 | dt: 13780.0498ms | tok/sec: 28.5352\n",
      "step  383 | train loss: 5.28 | val loss: 5.09 | perplexity: 161.95 | lr: 5.37e-04 | norm: 0.7610 | dt: 13601.1181ms | tok/sec: 28.9106\n",
      "step  384 | train loss: 5.09 | val loss: 5.10 | perplexity: 164.77 | lr: 5.38e-04 | norm: 0.5475 | dt: 13815.7690ms | tok/sec: 28.4614\n",
      "step  385 | train loss: 5.44 | val loss: 5.12 | perplexity: 167.14 | lr: 5.40e-04 | norm: 0.5931 | dt: 13602.4380ms | tok/sec: 28.9078\n",
      "step  386 | train loss: 5.18 | val loss: 5.11 | perplexity: 165.16 | lr: 5.41e-04 | norm: 0.7147 | dt: 13623.4031ms | tok/sec: 28.8633\n",
      "step  387 | train loss: 5.29 | val loss: 5.09 | perplexity: 161.89 | lr: 5.43e-04 | norm: 0.7547 | dt: 13613.5170ms | tok/sec: 28.8842\n",
      "step  388 | train loss: 5.48 | val loss: 5.09 | perplexity: 163.16 | lr: 5.44e-04 | norm: 0.6903 | dt: 13626.0211ms | tok/sec: 28.8577\n",
      "step  389 | train loss: 5.20 | val loss: 5.11 | perplexity: 165.80 | lr: 5.45e-04 | norm: 0.7490 | dt: 13671.6912ms | tok/sec: 28.7613\n",
      "step  390 | train loss: 5.38 | val loss: 5.11 | perplexity: 165.90 | lr: 5.47e-04 | norm: 0.5955 | dt: 13582.4101ms | tok/sec: 28.9504\n",
      "step  391 | train loss: 5.32 | val loss: 5.10 | perplexity: 163.37 | lr: 5.48e-04 | norm: 0.5524 | dt: 13797.1742ms | tok/sec: 28.4997\n",
      "step  392 | train loss: 5.66 | val loss: 5.11 | perplexity: 165.83 | lr: 5.50e-04 | norm: 1.1356 | dt: 13605.0606ms | tok/sec: 28.9022\n",
      "step  393 | train loss: 5.13 | val loss: 5.10 | perplexity: 164.50 | lr: 5.51e-04 | norm: 0.6575 | dt: 13631.1224ms | tok/sec: 28.8469\n",
      "step  394 | train loss: 5.75 | val loss: 5.12 | perplexity: 167.88 | lr: 5.52e-04 | norm: 0.5986 | dt: 13619.7219ms | tok/sec: 28.8711\n",
      "step  395 | train loss: 5.46 | val loss: 5.13 | perplexity: 168.74 | lr: 5.54e-04 | norm: 0.7173 | dt: 13639.3800ms | tok/sec: 28.8295\n",
      "step  396 | train loss: 5.21 | val loss: 5.13 | perplexity: 168.76 | lr: 5.55e-04 | norm: 0.7895 | dt: 13648.5434ms | tok/sec: 28.8101\n",
      "step  397 | train loss: 4.98 | val loss: 5.12 | perplexity: 167.73 | lr: 5.57e-04 | norm: 0.7079 | dt: 13674.7680ms | tok/sec: 28.7549\n",
      "step  398 | train loss: 5.40 | val loss: 5.10 | perplexity: 164.05 | lr: 5.58e-04 | norm: 0.5974 | dt: 13646.8306ms | tok/sec: 28.8137\n",
      "step  399 | train loss: 5.14 | val loss: 5.09 | perplexity: 162.48 | lr: 5.59e-04 | norm: 0.6301 | dt: 13622.2372ms | tok/sec: 28.8657\n",
      "step  400 | train loss: 5.31 | val loss: 5.08 | perplexity: 160.46 | lr: 5.61e-04 | norm: 0.6290 | dt: 13672.7290ms | tok/sec: 28.7591\n",
      "step  401 | train loss: 5.06 | val loss: 5.08 | perplexity: 160.81 | lr: 5.62e-04 | norm: 0.5257 | dt: 13634.4838ms | tok/sec: 28.8398\n",
      "step  402 | train loss: 5.23 | val loss: 5.09 | perplexity: 162.02 | lr: 5.64e-04 | norm: 0.5322 | dt: 13788.8057ms | tok/sec: 28.5170\n",
      "step  403 | train loss: 5.22 | val loss: 5.09 | perplexity: 162.23 | lr: 5.65e-04 | norm: 0.5749 | dt: 13633.9128ms | tok/sec: 28.8410\n",
      "step  404 | train loss: 5.21 | val loss: 5.08 | perplexity: 161.31 | lr: 5.66e-04 | norm: 0.6836 | dt: 13605.1478ms | tok/sec: 28.9020\n",
      "step  405 | train loss: 5.27 | val loss: 5.09 | perplexity: 162.42 | lr: 5.68e-04 | norm: 0.5401 | dt: 13635.3445ms | tok/sec: 28.8380\n",
      "step  406 | train loss: 5.42 | val loss: 5.08 | perplexity: 160.62 | lr: 5.69e-04 | norm: 0.6338 | dt: 13630.8157ms | tok/sec: 28.8476\n",
      "step  407 | train loss: 5.36 | val loss: 5.08 | perplexity: 161.19 | lr: 5.71e-04 | norm: 0.5136 | dt: 13622.1299ms | tok/sec: 28.8660\n",
      "step  408 | train loss: 5.15 | val loss: 5.07 | perplexity: 159.97 | lr: 5.72e-04 | norm: 0.6741 | dt: 13629.3378ms | tok/sec: 28.8507\n",
      "step  409 | train loss: 5.31 | val loss: 5.07 | perplexity: 159.74 | lr: 5.73e-04 | norm: 0.4816 | dt: 13615.7997ms | tok/sec: 28.8794\n",
      "step  410 | train loss: 4.99 | val loss: 5.07 | perplexity: 158.90 | lr: 5.75e-04 | norm: 0.5234 | dt: 13625.5915ms | tok/sec: 28.8586\n",
      "step  411 | train loss: 5.19 | val loss: 5.06 | perplexity: 158.07 | lr: 5.76e-04 | norm: 0.4929 | dt: 13586.8604ms | tok/sec: 28.9409\n",
      "step  412 | train loss: 5.45 | val loss: 5.07 | perplexity: 159.27 | lr: 5.78e-04 | norm: 0.7024 | dt: 13590.3268ms | tok/sec: 28.9335\n",
      "step  413 | train loss: 5.42 | val loss: 5.09 | perplexity: 162.06 | lr: 5.79e-04 | norm: 0.5879 | dt: 13621.9630ms | tok/sec: 28.8663\n",
      "step  414 | train loss: 5.08 | val loss: 5.07 | perplexity: 158.89 | lr: 5.80e-04 | norm: 0.6277 | dt: 13743.6199ms | tok/sec: 28.6108\n",
      "step  415 | train loss: 5.22 | val loss: 5.05 | perplexity: 156.78 | lr: 5.82e-04 | norm: 0.5234 | dt: 13643.0795ms | tok/sec: 28.8216\n",
      "step  416 | train loss: 5.34 | val loss: 5.06 | perplexity: 157.21 | lr: 5.83e-04 | norm: 0.5777 | dt: 13632.7653ms | tok/sec: 28.8435\n",
      "step  417 | train loss: 5.27 | val loss: 5.06 | perplexity: 157.79 | lr: 5.85e-04 | norm: 0.5538 | dt: 13628.4144ms | tok/sec: 28.8527\n",
      "step  418 | train loss: 5.49 | val loss: 5.08 | perplexity: 160.35 | lr: 5.86e-04 | norm: 0.8251 | dt: 13664.5491ms | tok/sec: 28.7764\n",
      "step  419 | train loss: 5.13 | val loss: 5.08 | perplexity: 161.32 | lr: 5.87e-04 | norm: 0.5978 | dt: 13663.4459ms | tok/sec: 28.7787\n",
      "step  420 | train loss: 5.31 | val loss: 5.09 | perplexity: 162.26 | lr: 5.89e-04 | norm: 0.7324 | dt: 13600.4989ms | tok/sec: 28.9119\n",
      "step  421 | train loss: 5.22 | val loss: 5.11 | perplexity: 165.79 | lr: 5.90e-04 | norm: 0.4992 | dt: 13606.9329ms | tok/sec: 28.8982\n",
      "step  422 | train loss: 5.50 | val loss: 5.11 | perplexity: 166.25 | lr: 5.92e-04 | norm: 0.5060 | dt: 13636.7629ms | tok/sec: 28.8350\n",
      "step  423 | train loss: 5.32 | val loss: 5.10 | perplexity: 163.75 | lr: 5.93e-04 | norm: 0.5388 | dt: 13625.0181ms | tok/sec: 28.8599\n",
      "step  424 | train loss: 5.31 | val loss: 5.07 | perplexity: 159.44 | lr: 5.94e-04 | norm: 0.5610 | dt: 13799.0260ms | tok/sec: 28.4959\n",
      "step  425 | train loss: 5.18 | val loss: 5.07 | perplexity: 158.63 | lr: 5.96e-04 | norm: 0.6132 | dt: 13655.9880ms | tok/sec: 28.7944\n",
      "step  426 | train loss: 5.58 | val loss: 5.08 | perplexity: 161.10 | lr: 5.97e-04 | norm: 0.6122 | dt: 13617.1405ms | tok/sec: 28.8765\n",
      "step  427 | train loss: 5.23 | val loss: 5.06 | perplexity: 158.36 | lr: 5.99e-04 | norm: 0.7107 | dt: 13746.3694ms | tok/sec: 28.6051\n",
      "step  428 | train loss: 5.29 | val loss: 5.07 | perplexity: 159.37 | lr: 6.00e-04 | norm: 0.5338 | dt: 13625.3481ms | tok/sec: 28.8592\n",
      "step  429 | train loss: 5.11 | val loss: 5.07 | perplexity: 158.65 | lr: 6.01e-04 | norm: 0.5678 | dt: 13612.2618ms | tok/sec: 28.8869\n",
      "step  430 | train loss: 5.41 | val loss: 5.08 | perplexity: 160.19 | lr: 6.03e-04 | norm: 0.5111 | dt: 13655.2646ms | tok/sec: 28.7959\n",
      "step  431 | train loss: 4.88 | val loss: 5.06 | perplexity: 157.81 | lr: 6.04e-04 | norm: 0.7052 | dt: 13623.1623ms | tok/sec: 28.8638\n",
      "step  432 | train loss: 5.15 | val loss: 5.05 | perplexity: 156.72 | lr: 6.06e-04 | norm: 0.4470 | dt: 13610.5578ms | tok/sec: 28.8905\n",
      "step  433 | train loss: 5.35 | val loss: 5.05 | perplexity: 156.28 | lr: 6.07e-04 | norm: 0.5704 | dt: 13617.7273ms | tok/sec: 28.8753\n",
      "step  434 | train loss: 5.27 | val loss: 5.06 | perplexity: 157.52 | lr: 6.08e-04 | norm: 0.5533 | dt: 13596.6315ms | tok/sec: 28.9201\n",
      "step  435 | train loss: 4.96 | val loss: 5.06 | perplexity: 157.64 | lr: 6.10e-04 | norm: 0.5695 | dt: 13586.3171ms | tok/sec: 28.9421\n",
      "step  436 | train loss: 5.37 | val loss: 5.05 | perplexity: 156.66 | lr: 6.11e-04 | norm: 0.5676 | dt: 13600.1637ms | tok/sec: 28.9126\n",
      "step  437 | train loss: 5.56 | val loss: 5.07 | perplexity: 158.59 | lr: 6.13e-04 | norm: 0.5684 | dt: 13612.5827ms | tok/sec: 28.8862\n",
      "step  438 | train loss: 5.61 | val loss: 5.06 | perplexity: 157.69 | lr: 6.14e-04 | norm: 0.5957 | dt: 13729.8837ms | tok/sec: 28.6394\n",
      "step  439 | train loss: 5.33 | val loss: 5.05 | perplexity: 156.80 | lr: 6.15e-04 | norm: 0.5266 | dt: 13603.3657ms | tok/sec: 28.9058\n",
      "step  440 | train loss: 5.53 | val loss: 5.07 | perplexity: 159.94 | lr: 6.17e-04 | norm: 0.7708 | dt: 13642.7286ms | tok/sec: 28.8224\n",
      "step  441 | train loss: 5.13 | val loss: 5.06 | perplexity: 157.50 | lr: 6.18e-04 | norm: 0.7378 | dt: 13590.3375ms | tok/sec: 28.9335\n",
      "step  442 | train loss: 5.09 | val loss: 5.06 | perplexity: 157.48 | lr: 6.20e-04 | norm: 0.5301 | dt: 13666.2426ms | tok/sec: 28.7728\n",
      "step  443 | train loss: 5.44 | val loss: 5.07 | perplexity: 159.37 | lr: 6.21e-04 | norm: 0.6356 | dt: 13639.7309ms | tok/sec: 28.8287\n",
      "step  444 | train loss: 5.28 | val loss: 5.07 | perplexity: 158.94 | lr: 6.22e-04 | norm: 0.4158 | dt: 13760.6511ms | tok/sec: 28.5754\n",
      "step  445 | train loss: 5.49 | val loss: 5.06 | perplexity: 157.13 | lr: 6.24e-04 | norm: 0.4967 | dt: 13627.5067ms | tok/sec: 28.8546\n",
      "step  446 | train loss: 4.91 | val loss: 5.06 | perplexity: 157.08 | lr: 6.25e-04 | norm: 0.5238 | dt: 13583.8723ms | tok/sec: 28.9473\n",
      "step  447 | train loss: 5.53 | val loss: 5.04 | perplexity: 155.18 | lr: 6.27e-04 | norm: 0.7115 | dt: 13621.3803ms | tok/sec: 28.8676\n",
      "step  448 | train loss: 5.08 | val loss: 5.07 | perplexity: 158.96 | lr: 6.28e-04 | norm: 1.0513 | dt: 13652.6504ms | tok/sec: 28.8014\n",
      "step  449 | train loss: 5.49 | val loss: 5.08 | perplexity: 161.24 | lr: 6.29e-04 | norm: 0.7825 | dt: 13658.4475ms | tok/sec: 28.7892\n",
      "step  450 | train loss: 5.22 | val loss: 5.09 | perplexity: 162.60 | lr: 6.31e-04 | norm: 0.9680 | dt: 13642.1704ms | tok/sec: 28.8236\n",
      "step  451 | train loss: 5.12 | val loss: 5.09 | perplexity: 162.32 | lr: 6.32e-04 | norm: 0.6819 | dt: 13672.9288ms | tok/sec: 28.7587\n",
      "step  452 | train loss: 5.25 | val loss: 5.08 | perplexity: 160.30 | lr: 6.34e-04 | norm: 0.5976 | dt: 13684.0756ms | tok/sec: 28.7353\n",
      "step  453 | train loss: 5.07 | val loss: 5.07 | perplexity: 159.60 | lr: 6.35e-04 | norm: 0.6166 | dt: 13654.6602ms | tok/sec: 28.7972\n",
      "step  454 | train loss: 5.31 | val loss: 5.06 | perplexity: 157.67 | lr: 6.36e-04 | norm: 0.5933 | dt: 13642.6125ms | tok/sec: 28.8226\n",
      "step  455 | train loss: 5.30 | val loss: 5.06 | perplexity: 157.79 | lr: 6.38e-04 | norm: 0.6017 | dt: 13639.4100ms | tok/sec: 28.8294\n",
      "step  456 | train loss: 5.36 | val loss: 5.08 | perplexity: 160.28 | lr: 6.39e-04 | norm: 0.7971 | dt: 13690.0680ms | tok/sec: 28.7227\n",
      "step  457 | train loss: 5.34 | val loss: 5.07 | perplexity: 159.08 | lr: 6.41e-04 | norm: 0.5396 | dt: 13917.4609ms | tok/sec: 28.2534\n",
      "step  458 | train loss: 5.32 | val loss: 5.08 | perplexity: 161.43 | lr: 6.42e-04 | norm: 0.5274 | dt: 13619.8013ms | tok/sec: 28.8709\n",
      "step  459 | train loss: 5.35 | val loss: 5.08 | perplexity: 161.24 | lr: 6.43e-04 | norm: 0.9678 | dt: 13691.6821ms | tok/sec: 28.7193\n",
      "step  460 | train loss: 5.35 | val loss: 5.08 | perplexity: 160.17 | lr: 6.45e-04 | norm: 0.7857 | dt: 13758.8067ms | tok/sec: 28.5792\n",
      "step  461 | train loss: 5.36 | val loss: 5.07 | perplexity: 158.77 | lr: 6.46e-04 | norm: 0.5221 | dt: 13707.0363ms | tok/sec: 28.6872\n",
      "step  462 | train loss: 5.48 | val loss: 5.07 | perplexity: 159.93 | lr: 6.48e-04 | norm: 0.7362 | dt: 13835.8696ms | tok/sec: 28.4200\n",
      "step  463 | train loss: 5.36 | val loss: 5.10 | perplexity: 164.71 | lr: 6.49e-04 | norm: 0.7384 | dt: 13665.9224ms | tok/sec: 28.7735\n",
      "step  464 | train loss: 5.01 | val loss: 5.08 | perplexity: 161.44 | lr: 6.50e-04 | norm: 0.7836 | dt: 13685.0977ms | tok/sec: 28.7332\n",
      "step  465 | train loss: 5.10 | val loss: 5.06 | perplexity: 157.22 | lr: 6.52e-04 | norm: 0.6706 | dt: 13655.6065ms | tok/sec: 28.7952\n",
      "step  466 | train loss: 5.15 | val loss: 5.05 | perplexity: 156.14 | lr: 6.53e-04 | norm: 0.5935 | dt: 13668.5276ms | tok/sec: 28.7680\n",
      "step  467 | train loss: 5.47 | val loss: 5.04 | perplexity: 155.17 | lr: 6.55e-04 | norm: 0.7283 | dt: 13723.0361ms | tok/sec: 28.6537\n",
      "step  468 | train loss: 5.12 | val loss: 5.04 | perplexity: 154.88 | lr: 6.56e-04 | norm: 0.7348 | dt: 13623.2119ms | tok/sec: 28.8637\n",
      "step  469 | train loss: 5.34 | val loss: 5.05 | perplexity: 155.98 | lr: 6.57e-04 | norm: 0.5098 | dt: 13631.6278ms | tok/sec: 28.8459\n",
      "step  470 | train loss: 5.29 | val loss: 5.04 | perplexity: 154.80 | lr: 6.59e-04 | norm: 0.5552 | dt: 13636.6017ms | tok/sec: 28.8353\n",
      "step  471 | train loss: 5.36 | val loss: 5.03 | perplexity: 152.51 | lr: 6.60e-04 | norm: 0.5618 | dt: 13618.5789ms | tok/sec: 28.8735\n",
      "step  472 | train loss: 5.08 | val loss: 5.03 | perplexity: 153.47 | lr: 6.62e-04 | norm: 0.5712 | dt: 13641.2270ms | tok/sec: 28.8256\n",
      "step  473 | train loss: 5.18 | val loss: 5.03 | perplexity: 153.15 | lr: 6.63e-04 | norm: 0.5249 | dt: 13691.0067ms | tok/sec: 28.7208\n",
      "step  474 | train loss: 5.09 | val loss: 5.04 | perplexity: 154.05 | lr: 6.64e-04 | norm: 0.5783 | dt: 13628.6225ms | tok/sec: 28.8522\n",
      "step  475 | train loss: 5.39 | val loss: 5.04 | perplexity: 153.93 | lr: 6.66e-04 | norm: 0.5071 | dt: 13706.8577ms | tok/sec: 28.6875\n",
      "step  476 | train loss: 5.39 | val loss: 5.03 | perplexity: 153.36 | lr: 6.67e-04 | norm: 0.5062 | dt: 13651.5858ms | tok/sec: 28.8037\n",
      "step  477 | train loss: 5.19 | val loss: 5.03 | perplexity: 153.27 | lr: 6.69e-04 | norm: 0.4581 | dt: 13671.0930ms | tok/sec: 28.7626\n",
      "step  478 | train loss: 5.48 | val loss: 5.04 | perplexity: 153.81 | lr: 6.70e-04 | norm: 0.4810 | dt: 13663.7566ms | tok/sec: 28.7780\n",
      "step  479 | train loss: 5.22 | val loss: 5.04 | perplexity: 154.77 | lr: 6.71e-04 | norm: 0.4133 | dt: 13672.9193ms | tok/sec: 28.7587\n",
      "step  480 | train loss: 5.33 | val loss: 5.03 | perplexity: 152.45 | lr: 6.73e-04 | norm: 0.4850 | dt: 13860.3151ms | tok/sec: 28.3699\n",
      "step  481 | train loss: 5.43 | val loss: 5.03 | perplexity: 152.33 | lr: 6.74e-04 | norm: 0.6653 | dt: 13615.4873ms | tok/sec: 28.8801\n",
      "step  482 | train loss: 4.89 | val loss: 5.03 | perplexity: 152.44 | lr: 6.76e-04 | norm: 0.5376 | dt: 13649.4043ms | tok/sec: 28.8083\n",
      "step  483 | train loss: 5.39 | val loss: 5.03 | perplexity: 152.59 | lr: 6.77e-04 | norm: 0.7730 | dt: 13641.1431ms | tok/sec: 28.8257\n",
      "step  484 | train loss: 5.13 | val loss: 5.03 | perplexity: 152.23 | lr: 6.78e-04 | norm: 0.5866 | dt: 13680.7785ms | tok/sec: 28.7422\n",
      "step  485 | train loss: 5.22 | val loss: 5.04 | perplexity: 153.71 | lr: 6.80e-04 | norm: 0.4682 | dt: 13599.2649ms | tok/sec: 28.9145\n",
      "step  486 | train loss: 5.16 | val loss: 5.05 | perplexity: 155.28 | lr: 6.81e-04 | norm: 0.5720 | dt: 13634.2695ms | tok/sec: 28.8403\n",
      "step  487 | train loss: 5.10 | val loss: 5.04 | perplexity: 154.03 | lr: 6.83e-04 | norm: 0.4840 | dt: 13644.5415ms | tok/sec: 28.8186\n",
      "step  488 | train loss: 5.25 | val loss: 5.03 | perplexity: 152.91 | lr: 6.84e-04 | norm: 0.4416 | dt: 13864.3692ms | tok/sec: 28.3616\n",
      "step  489 | train loss: 5.48 | val loss: 5.04 | perplexity: 154.56 | lr: 6.85e-04 | norm: 0.4473 | dt: 13674.6528ms | tok/sec: 28.7551\n",
      "step  490 | train loss: 5.25 | val loss: 5.04 | perplexity: 154.60 | lr: 6.87e-04 | norm: 0.5502 | dt: 13690.3915ms | tok/sec: 28.7220\n",
      "step  491 | train loss: 5.37 | val loss: 5.05 | perplexity: 156.28 | lr: 6.88e-04 | norm: 0.6207 | dt: 13634.2127ms | tok/sec: 28.8404\n",
      "step  492 | train loss: 5.31 | val loss: 5.05 | perplexity: 155.44 | lr: 6.90e-04 | norm: 0.4981 | dt: 13609.2677ms | tok/sec: 28.8933\n",
      "step  493 | train loss: 5.00 | val loss: 5.04 | perplexity: 153.73 | lr: 6.91e-04 | norm: 0.5830 | dt: 13578.8910ms | tok/sec: 28.9579\n",
      "step  494 | train loss: 5.29 | val loss: 5.04 | perplexity: 154.84 | lr: 6.92e-04 | norm: 0.5083 | dt: 13630.4917ms | tok/sec: 28.8483\n",
      "step  495 | train loss: 5.10 | val loss: 5.03 | perplexity: 152.84 | lr: 6.94e-04 | norm: 0.6244 | dt: 13665.9985ms | tok/sec: 28.7733\n",
      "step  496 | train loss: 5.33 | val loss: 5.02 | perplexity: 151.36 | lr: 6.95e-04 | norm: 0.4463 | dt: 13625.0231ms | tok/sec: 28.8598\n",
      "step  497 | train loss: 5.35 | val loss: 5.01 | perplexity: 150.02 | lr: 6.97e-04 | norm: 0.5050 | dt: 13666.7492ms | tok/sec: 28.7717\n",
      "step  498 | train loss: 5.33 | val loss: 5.02 | perplexity: 151.65 | lr: 6.98e-04 | norm: 0.9786 | dt: 13698.3244ms | tok/sec: 28.7054\n",
      "step  499 | train loss: 5.52 | val loss: 5.06 | perplexity: 157.80 | lr: 6.99e-04 | norm: 0.5391 | dt: 13636.8017ms | tok/sec: 28.8349\n",
      "step  500 | train loss: 5.26 | val loss: 5.05 | perplexity: 156.70 | lr: 7.01e-04 | norm: 0.6494 | dt: 13620.2056ms | tok/sec: 28.8700\n",
      "step  501 | train loss: 5.40 | val loss: 5.04 | perplexity: 154.96 | lr: 7.02e-04 | norm: 0.5648 | dt: 13635.2806ms | tok/sec: 28.8381\n",
      "step  502 | train loss: 5.19 | val loss: 5.03 | perplexity: 152.86 | lr: 7.03e-04 | norm: 0.6008 | dt: 13628.6161ms | tok/sec: 28.8522\n",
      "step  503 | train loss: 5.11 | val loss: 5.02 | perplexity: 151.94 | lr: 7.05e-04 | norm: 0.5302 | dt: 13665.5490ms | tok/sec: 28.7743\n",
      "step  504 | train loss: 5.58 | val loss: 5.01 | perplexity: 150.48 | lr: 7.06e-04 | norm: 0.5639 | dt: 13657.2337ms | tok/sec: 28.7918\n",
      "step  505 | train loss: 5.33 | val loss: 5.02 | perplexity: 150.75 | lr: 7.08e-04 | norm: 0.4623 | dt: 13666.5690ms | tok/sec: 28.7721\n",
      "step  506 | train loss: 5.11 | val loss: 5.02 | perplexity: 150.81 | lr: 7.09e-04 | norm: 0.5709 | dt: 13646.3051ms | tok/sec: 28.8148\n",
      "step  507 | train loss: 5.36 | val loss: 5.02 | perplexity: 150.92 | lr: 7.10e-04 | norm: 0.5110 | dt: 13815.4171ms | tok/sec: 28.4621\n",
      "step  508 | train loss: 5.13 | val loss: 5.01 | perplexity: 149.25 | lr: 7.12e-04 | norm: 0.6638 | dt: 13614.7180ms | tok/sec: 28.8817\n",
      "step  509 | train loss: 5.24 | val loss: 5.02 | perplexity: 151.42 | lr: 7.13e-04 | norm: 0.4745 | dt: 13618.1858ms | tok/sec: 28.8743\n",
      "step  510 | train loss: 5.16 | val loss: 5.02 | perplexity: 151.87 | lr: 7.15e-04 | norm: 0.5669 | dt: 13632.5371ms | tok/sec: 28.8439\n",
      "step  511 | train loss: 5.32 | val loss: 5.01 | perplexity: 149.97 | lr: 7.16e-04 | norm: 0.5000 | dt: 13679.9104ms | tok/sec: 28.7440\n",
      "step  512 | train loss: 5.26 | val loss: 5.01 | perplexity: 149.50 | lr: 7.17e-04 | norm: 0.4077 | dt: 13631.2222ms | tok/sec: 28.8467\n",
      "step  513 | train loss: 5.39 | val loss: 5.02 | perplexity: 150.75 | lr: 7.19e-04 | norm: 0.4524 | dt: 13631.1598ms | tok/sec: 28.8468\n",
      "step  514 | train loss: 5.16 | val loss: 5.01 | perplexity: 149.82 | lr: 7.20e-04 | norm: 0.5927 | dt: 13617.9178ms | tok/sec: 28.8749\n",
      "step  515 | train loss: 5.22 | val loss: 5.03 | perplexity: 152.60 | lr: 7.22e-04 | norm: 0.5386 | dt: 13608.7322ms | tok/sec: 28.8944\n",
      "step  516 | train loss: 5.09 | val loss: 5.01 | perplexity: 149.67 | lr: 7.23e-04 | norm: 0.5102 | dt: 13649.6413ms | tok/sec: 28.8078\n",
      "step  517 | train loss: 5.16 | val loss: 5.00 | perplexity: 148.37 | lr: 7.24e-04 | norm: 0.5284 | dt: 13662.9984ms | tok/sec: 28.7796\n",
      "step  518 | train loss: 5.18 | val loss: 4.99 | perplexity: 147.19 | lr: 7.26e-04 | norm: 0.6151 | dt: 13619.7033ms | tok/sec: 28.8711\n",
      "step  519 | train loss: 5.05 | val loss: 5.00 | perplexity: 147.74 | lr: 7.27e-04 | norm: 0.5062 | dt: 13665.4763ms | tok/sec: 28.7744\n",
      "step  520 | train loss: 5.20 | val loss: 5.01 | perplexity: 149.16 | lr: 7.29e-04 | norm: 0.7696 | dt: 13649.8401ms | tok/sec: 28.8074\n",
      "step  521 | train loss: 5.36 | val loss: 5.01 | perplexity: 149.59 | lr: 7.30e-04 | norm: 0.6510 | dt: 13678.9243ms | tok/sec: 28.7461\n",
      "step  522 | train loss: 4.94 | val loss: 5.01 | perplexity: 149.37 | lr: 7.31e-04 | norm: 0.7491 | dt: 13628.4747ms | tok/sec: 28.8525\n",
      "step  523 | train loss: 5.23 | val loss: 5.02 | perplexity: 151.17 | lr: 7.33e-04 | norm: 0.3959 | dt: 13624.0842ms | tok/sec: 28.8618\n",
      "step  524 | train loss: 5.32 | val loss: 5.00 | perplexity: 148.96 | lr: 7.34e-04 | norm: 0.5272 | dt: 13632.4933ms | tok/sec: 28.8440\n",
      "step  525 | train loss: 5.18 | val loss: 4.98 | perplexity: 146.16 | lr: 7.36e-04 | norm: 0.5662 | dt: 13772.2118ms | tok/sec: 28.5514\n",
      "step  526 | train loss: 5.24 | val loss: 4.98 | perplexity: 146.10 | lr: 7.37e-04 | norm: 0.4901 | dt: 13686.8496ms | tok/sec: 28.7295\n",
      "step  527 | train loss: 5.10 | val loss: 4.98 | perplexity: 145.28 | lr: 7.38e-04 | norm: 0.5092 | dt: 13707.7649ms | tok/sec: 28.6856\n",
      "step  528 | train loss: 5.22 | val loss: 4.98 | perplexity: 145.13 | lr: 7.40e-04 | norm: 0.5139 | dt: 13622.7093ms | tok/sec: 28.8647\n",
      "step  529 | train loss: 5.34 | val loss: 4.98 | perplexity: 145.55 | lr: 7.41e-04 | norm: 0.4068 | dt: 13594.2557ms | tok/sec: 28.9252\n",
      "step  530 | train loss: 5.22 | val loss: 4.97 | perplexity: 144.11 | lr: 7.43e-04 | norm: 0.4287 | dt: 13649.4873ms | tok/sec: 28.8081\n",
      "step  531 | train loss: 5.07 | val loss: 4.96 | perplexity: 143.24 | lr: 7.44e-04 | norm: 0.4896 | dt: 13907.1810ms | tok/sec: 28.2743\n",
      "step  532 | train loss: 5.14 | val loss: 4.96 | perplexity: 143.24 | lr: 7.45e-04 | norm: 0.5586 | dt: 13640.0020ms | tok/sec: 28.8281\n",
      "step  533 | train loss: 5.17 | val loss: 4.96 | perplexity: 143.07 | lr: 7.47e-04 | norm: 0.4343 | dt: 13634.6951ms | tok/sec: 28.8394\n",
      "step  534 | train loss: 5.11 | val loss: 4.96 | perplexity: 142.68 | lr: 7.48e-04 | norm: 0.4531 | dt: 13621.8283ms | tok/sec: 28.8666\n",
      "step  535 | train loss: 5.15 | val loss: 4.96 | perplexity: 143.12 | lr: 7.50e-04 | norm: 0.3687 | dt: 13595.8805ms | tok/sec: 28.9217\n",
      "step  536 | train loss: 5.07 | val loss: 4.97 | perplexity: 144.08 | lr: 7.51e-04 | norm: 0.5555 | dt: 13648.7305ms | tok/sec: 28.8097\n",
      "step  537 | train loss: 5.10 | val loss: 4.98 | perplexity: 144.98 | lr: 7.52e-04 | norm: 0.7144 | dt: 13611.8360ms | tok/sec: 28.8878\n",
      "step  538 | train loss: 5.05 | val loss: 4.98 | perplexity: 146.15 | lr: 7.54e-04 | norm: 0.4675 | dt: 13624.1980ms | tok/sec: 28.8616\n",
      "step  539 | train loss: 5.13 | val loss: 4.98 | perplexity: 145.11 | lr: 7.55e-04 | norm: 0.5982 | dt: 13631.1359ms | tok/sec: 28.8469\n",
      "step  540 | train loss: 5.19 | val loss: 4.97 | perplexity: 143.65 | lr: 7.57e-04 | norm: 0.4710 | dt: 13681.0527ms | tok/sec: 28.7416\n",
      "step  541 | train loss: 5.36 | val loss: 4.96 | perplexity: 142.87 | lr: 7.58e-04 | norm: 0.4215 | dt: 13595.1650ms | tok/sec: 28.9232\n",
      "step  542 | train loss: 5.12 | val loss: 4.96 | perplexity: 143.13 | lr: 7.59e-04 | norm: 0.5254 | dt: 13594.1482ms | tok/sec: 28.9254\n",
      "step  543 | train loss: 5.38 | val loss: 4.96 | perplexity: 142.32 | lr: 7.61e-04 | norm: 0.6163 | dt: 13595.9327ms | tok/sec: 28.9216\n",
      "step  544 | train loss: 5.15 | val loss: 4.98 | perplexity: 145.10 | lr: 7.62e-04 | norm: 0.6705 | dt: 13607.6632ms | tok/sec: 28.8967\n",
      "step  545 | train loss: 5.34 | val loss: 4.97 | perplexity: 143.73 | lr: 7.64e-04 | norm: 0.4651 | dt: 13635.0880ms | tok/sec: 28.8385\n",
      "step  546 | train loss: 4.79 | val loss: 4.96 | perplexity: 142.00 | lr: 7.65e-04 | norm: 0.5051 | dt: 13758.6102ms | tok/sec: 28.5796\n",
      "step  547 | train loss: 5.38 | val loss: 4.98 | perplexity: 144.92 | lr: 7.66e-04 | norm: 0.6118 | dt: 13639.6198ms | tok/sec: 28.8290\n",
      "step  548 | train loss: 5.17 | val loss: 4.97 | perplexity: 143.33 | lr: 7.68e-04 | norm: 0.5388 | dt: 13646.0845ms | tok/sec: 28.8153\n",
      "step  549 | train loss: 5.25 | val loss: 4.96 | perplexity: 143.11 | lr: 7.69e-04 | norm: 0.4503 | dt: 13617.9125ms | tok/sec: 28.8749\n",
      "step  550 | train loss: 5.20 | val loss: 4.97 | perplexity: 144.70 | lr: 7.71e-04 | norm: 0.5873 | dt: 13804.4956ms | tok/sec: 28.4846\n",
      "step  551 | train loss: 5.22 | val loss: 4.97 | perplexity: 143.82 | lr: 7.72e-04 | norm: 0.6043 | dt: 13691.1912ms | tok/sec: 28.7204\n",
      "step  552 | train loss: 5.08 | val loss: 4.96 | perplexity: 143.28 | lr: 7.73e-04 | norm: 0.4170 | dt: 13605.9744ms | tok/sec: 28.9002\n",
      "step  553 | train loss: 5.31 | val loss: 4.96 | perplexity: 143.23 | lr: 7.75e-04 | norm: 0.5281 | dt: 13668.2951ms | tok/sec: 28.7685\n",
      "step  554 | train loss: 5.21 | val loss: 4.96 | perplexity: 142.68 | lr: 7.76e-04 | norm: 0.4417 | dt: 13588.2134ms | tok/sec: 28.9380\n",
      "step  555 | train loss: 5.09 | val loss: 4.95 | perplexity: 141.04 | lr: 7.78e-04 | norm: 0.4716 | dt: 13846.9033ms | tok/sec: 28.3974\n",
      "step  556 | train loss: 5.21 | val loss: 4.97 | perplexity: 144.01 | lr: 7.79e-04 | norm: 0.5844 | dt: 13606.6008ms | tok/sec: 28.8989\n",
      "step  557 | train loss: 5.30 | val loss: 4.98 | perplexity: 145.27 | lr: 7.80e-04 | norm: 0.5353 | dt: 13634.8467ms | tok/sec: 28.8390\n",
      "step  558 | train loss: 5.06 | val loss: 4.98 | perplexity: 144.90 | lr: 7.82e-04 | norm: 0.4283 | dt: 13601.0315ms | tok/sec: 28.9107\n",
      "step  559 | train loss: 4.99 | val loss: 4.94 | perplexity: 139.12 | lr: 7.83e-04 | norm: 0.6973 | dt: 13637.6040ms | tok/sec: 28.8332\n",
      "step  560 | train loss: 5.29 | val loss: 4.94 | perplexity: 140.26 | lr: 7.85e-04 | norm: 0.5848 | dt: 13621.0201ms | tok/sec: 28.8683\n",
      "step  561 | train loss: 4.97 | val loss: 4.93 | perplexity: 137.77 | lr: 7.86e-04 | norm: 0.8172 | dt: 13588.8896ms | tok/sec: 28.9366\n",
      "step  562 | train loss: 5.11 | val loss: 4.94 | perplexity: 139.58 | lr: 7.87e-04 | norm: 0.5437 | dt: 13636.5974ms | tok/sec: 28.8353\n",
      "step  563 | train loss: 5.12 | val loss: 4.94 | perplexity: 140.30 | lr: 7.89e-04 | norm: 0.6386 | dt: 13684.3562ms | tok/sec: 28.7347\n",
      "step  564 | train loss: 5.14 | val loss: 4.94 | perplexity: 139.76 | lr: 7.90e-04 | norm: 0.4986 | dt: 13616.4305ms | tok/sec: 28.8781\n",
      "step  565 | train loss: 5.10 | val loss: 4.92 | perplexity: 136.96 | lr: 7.92e-04 | norm: 0.5343 | dt: 13644.1586ms | tok/sec: 28.8194\n",
      "step  566 | train loss: 5.01 | val loss: 4.91 | perplexity: 135.75 | lr: 7.93e-04 | norm: 0.4552 | dt: 13630.1422ms | tok/sec: 28.8490\n",
      "step  567 | train loss: 5.04 | val loss: 4.91 | perplexity: 135.71 | lr: 7.94e-04 | norm: 0.5107 | dt: 13621.9294ms | tok/sec: 28.8664\n",
      "step  568 | train loss: 5.23 | val loss: 4.92 | perplexity: 137.01 | lr: 7.96e-04 | norm: 0.4650 | dt: 13670.1131ms | tok/sec: 28.7646\n",
      "step  569 | train loss: 5.15 | val loss: 4.91 | perplexity: 136.14 | lr: 7.97e-04 | norm: 0.6019 | dt: 13667.6307ms | tok/sec: 28.7699\n",
      "step  570 | train loss: 4.95 | val loss: 4.91 | perplexity: 135.53 | lr: 7.99e-04 | norm: 0.4616 | dt: 13697.2506ms | tok/sec: 28.7077\n",
      "step  571 | train loss: 5.05 | val loss: 4.90 | perplexity: 134.50 | lr: 8.00e-04 | norm: 0.4641 | dt: 13746.0055ms | tok/sec: 28.6058\n",
      "step  572 | train loss: 5.18 | val loss: 4.91 | perplexity: 135.47 | lr: 8.01e-04 | norm: 0.5370 | dt: 13688.2894ms | tok/sec: 28.7265\n",
      "step  573 | train loss: 5.18 | val loss: 4.91 | perplexity: 135.74 | lr: 8.03e-04 | norm: 0.5781 | dt: 13673.6968ms | tok/sec: 28.7571\n",
      "step  574 | train loss: 5.45 | val loss: 4.92 | perplexity: 136.48 | lr: 8.04e-04 | norm: 0.4921 | dt: 13705.1094ms | tok/sec: 28.6912\n",
      "step  575 | train loss: 5.07 | val loss: 4.92 | perplexity: 137.29 | lr: 8.06e-04 | norm: 0.3208 | dt: 13656.0328ms | tok/sec: 28.7943\n",
      "step  576 | train loss: 4.82 | val loss: 4.93 | perplexity: 138.82 | lr: 8.07e-04 | norm: 0.5398 | dt: 13692.7090ms | tok/sec: 28.7172\n",
      "step  577 | train loss: 5.15 | val loss: 4.93 | perplexity: 138.59 | lr: 8.08e-04 | norm: 0.5561 | dt: 13641.5865ms | tok/sec: 28.8248\n",
      "step  578 | train loss: 4.97 | val loss: 4.93 | perplexity: 138.17 | lr: 8.10e-04 | norm: 0.5762 | dt: 13794.2514ms | tok/sec: 28.5058\n",
      "step  579 | train loss: 4.94 | val loss: 4.90 | perplexity: 134.78 | lr: 8.11e-04 | norm: 0.5264 | dt: 13670.1124ms | tok/sec: 28.7647\n",
      "step  580 | train loss: 5.17 | val loss: 4.92 | perplexity: 136.44 | lr: 8.13e-04 | norm: 0.4057 | dt: 13659.9274ms | tok/sec: 28.7861\n",
      "step  581 | train loss: 5.06 | val loss: 4.91 | perplexity: 135.38 | lr: 8.14e-04 | norm: 0.7550 | dt: 13651.2568ms | tok/sec: 28.8044\n",
      "step  582 | train loss: 5.10 | val loss: 4.93 | perplexity: 138.85 | lr: 8.15e-04 | norm: 0.6161 | dt: 13671.4649ms | tok/sec: 28.7618\n",
      "step  583 | train loss: 5.15 | val loss: 4.94 | perplexity: 139.81 | lr: 8.17e-04 | norm: 0.6562 | dt: 13693.1102ms | tok/sec: 28.7163\n",
      "step  584 | train loss: 5.34 | val loss: 4.95 | perplexity: 140.87 | lr: 8.18e-04 | norm: 0.5828 | dt: 13644.3403ms | tok/sec: 28.8190\n",
      "step  585 | train loss: 5.06 | val loss: 4.94 | perplexity: 139.79 | lr: 8.20e-04 | norm: 0.5052 | dt: 13666.8720ms | tok/sec: 28.7715\n",
      "step  586 | train loss: 4.95 | val loss: 4.92 | perplexity: 137.61 | lr: 8.21e-04 | norm: 0.5964 | dt: 13680.4798ms | tok/sec: 28.7429\n",
      "step  587 | train loss: 4.85 | val loss: 4.91 | perplexity: 136.03 | lr: 8.22e-04 | norm: 0.5736 | dt: 13669.2324ms | tok/sec: 28.7665\n",
      "step  588 | train loss: 5.21 | val loss: 4.90 | perplexity: 134.41 | lr: 8.24e-04 | norm: 0.7375 | dt: 13636.9367ms | tok/sec: 28.8346\n",
      "step  589 | train loss: 5.11 | val loss: 4.90 | perplexity: 134.89 | lr: 8.25e-04 | norm: 0.4660 | dt: 13615.1693ms | tok/sec: 28.8807\n",
      "step  590 | train loss: 4.89 | val loss: 4.90 | perplexity: 134.38 | lr: 8.27e-04 | norm: 0.5221 | dt: 13630.0826ms | tok/sec: 28.8491\n",
      "step  591 | train loss: 4.77 | val loss: 4.90 | perplexity: 134.65 | lr: 8.28e-04 | norm: 0.5026 | dt: 13628.2902ms | tok/sec: 28.8529\n",
      "step  592 | train loss: 4.98 | val loss: 4.90 | perplexity: 134.27 | lr: 8.29e-04 | norm: 0.5600 | dt: 13654.6168ms | tok/sec: 28.7973\n",
      "step  593 | train loss: 4.79 | val loss: 4.91 | perplexity: 135.06 | lr: 8.31e-04 | norm: 0.5622 | dt: 13621.9835ms | tok/sec: 28.8663\n",
      "step  594 | train loss: 5.05 | val loss: 4.91 | perplexity: 135.90 | lr: 8.32e-04 | norm: 0.4792 | dt: 13764.7409ms | tok/sec: 28.5669\n",
      "step  595 | train loss: 5.22 | val loss: 4.91 | perplexity: 135.72 | lr: 8.34e-04 | norm: 0.6412 | dt: 13708.7617ms | tok/sec: 28.6836\n",
      "step  596 | train loss: 5.42 | val loss: 4.93 | perplexity: 137.78 | lr: 8.35e-04 | norm: 0.5774 | dt: 13623.5731ms | tok/sec: 28.8629\n",
      "step  597 | train loss: 4.97 | val loss: 4.93 | perplexity: 138.22 | lr: 8.36e-04 | norm: 0.6966 | dt: 13605.5994ms | tok/sec: 28.9010\n",
      "step  598 | train loss: 5.30 | val loss: 4.93 | perplexity: 138.78 | lr: 8.38e-04 | norm: 0.5348 | dt: 13681.8495ms | tok/sec: 28.7400\n",
      "step  599 | train loss: 5.21 | val loss: 4.93 | perplexity: 138.87 | lr: 8.39e-04 | norm: 0.5198 | dt: 13656.3044ms | tok/sec: 28.7937\n",
      "step  600 | train loss: 5.19 | val loss: 4.93 | perplexity: 138.70 | lr: 8.41e-04 | norm: 0.5865 | dt: 13724.0963ms | tok/sec: 28.6515\n",
      "step  601 | train loss: 5.31 | val loss: 4.94 | perplexity: 139.46 | lr: 8.42e-04 | norm: 0.5031 | dt: 13657.1214ms | tok/sec: 28.7920\n",
      "step  602 | train loss: 5.10 | val loss: 4.93 | perplexity: 138.74 | lr: 8.43e-04 | norm: 0.4349 | dt: 13663.8584ms | tok/sec: 28.7778\n",
      "step  603 | train loss: 5.17 | val loss: 4.92 | perplexity: 137.51 | lr: 8.45e-04 | norm: 0.4641 | dt: 13705.2014ms | tok/sec: 28.6910\n",
      "step  604 | train loss: 5.18 | val loss: 4.92 | perplexity: 137.02 | lr: 8.46e-04 | norm: 0.3630 | dt: 13661.5024ms | tok/sec: 28.7828\n",
      "step  605 | train loss: 5.11 | val loss: 4.91 | perplexity: 135.53 | lr: 8.48e-04 | norm: 0.4399 | dt: 13704.3693ms | tok/sec: 28.6927\n",
      "step  606 | train loss: 5.28 | val loss: 4.91 | perplexity: 135.53 | lr: 8.49e-04 | norm: 0.5037 | dt: 13646.5619ms | tok/sec: 28.8143\n",
      "step  607 | train loss: 5.18 | val loss: 4.90 | perplexity: 134.77 | lr: 8.50e-04 | norm: 0.6273 | dt: 13643.5361ms | tok/sec: 28.8207\n",
      "step  608 | train loss: 5.19 | val loss: 4.92 | perplexity: 136.78 | lr: 8.52e-04 | norm: 0.5944 | dt: 13668.0739ms | tok/sec: 28.7689\n",
      "step  609 | train loss: 5.19 | val loss: 4.92 | perplexity: 136.83 | lr: 8.53e-04 | norm: 0.5171 | dt: 13640.2736ms | tok/sec: 28.8276\n",
      "step  610 | train loss: 5.19 | val loss: 4.92 | perplexity: 136.83 | lr: 8.55e-04 | norm: 0.4920 | dt: 13624.8050ms | tok/sec: 28.8603\n",
      "step  611 | train loss: 5.06 | val loss: 4.91 | perplexity: 136.28 | lr: 8.56e-04 | norm: 0.5111 | dt: 13643.1077ms | tok/sec: 28.8216\n",
      "step  612 | train loss: 4.99 | val loss: 4.92 | perplexity: 137.62 | lr: 8.57e-04 | norm: 0.5190 | dt: 13705.5867ms | tok/sec: 28.6902\n",
      "step  613 | train loss: 5.34 | val loss: 4.93 | perplexity: 138.65 | lr: 8.59e-04 | norm: 0.6663 | dt: 13658.3929ms | tok/sec: 28.7893\n",
      "step  614 | train loss: 5.30 | val loss: 4.92 | perplexity: 137.40 | lr: 8.60e-04 | norm: 0.6849 | dt: 13616.1408ms | tok/sec: 28.8787\n",
      "step  615 | train loss: 5.36 | val loss: 4.94 | perplexity: 139.99 | lr: 8.62e-04 | norm: 0.6861 | dt: 13641.4618ms | tok/sec: 28.8251\n",
      "step  616 | train loss: 5.07 | val loss: 4.94 | perplexity: 139.41 | lr: 8.63e-04 | norm: 0.7431 | dt: 13662.2643ms | tok/sec: 28.7812\n",
      "step  617 | train loss: 5.03 | val loss: 4.95 | perplexity: 141.01 | lr: 8.64e-04 | norm: 0.6552 | dt: 13668.5746ms | tok/sec: 28.7679\n",
      "step  618 | train loss: 4.78 | val loss: 4.94 | perplexity: 140.37 | lr: 8.66e-04 | norm: 0.7471 | dt: 13852.8819ms | tok/sec: 28.3851\n",
      "step  619 | train loss: 4.94 | val loss: 4.93 | perplexity: 138.23 | lr: 8.67e-04 | norm: 0.5630 | dt: 13706.5253ms | tok/sec: 28.6882\n",
      "step  620 | train loss: 4.98 | val loss: 4.94 | perplexity: 139.37 | lr: 8.69e-04 | norm: 0.5151 | dt: 13643.9669ms | tok/sec: 28.8198\n",
      "step  621 | train loss: 4.96 | val loss: 4.91 | perplexity: 136.24 | lr: 8.70e-04 | norm: 0.6659 | dt: 13824.3659ms | tok/sec: 28.4437\n",
      "step  622 | train loss: 5.15 | val loss: 4.91 | perplexity: 136.31 | lr: 8.71e-04 | norm: 0.6005 | dt: 13640.9893ms | tok/sec: 28.8261\n",
      "step  623 | train loss: 5.25 | val loss: 4.92 | perplexity: 136.42 | lr: 8.73e-04 | norm: 0.6660 | dt: 13658.4036ms | tok/sec: 28.7893\n",
      "step  624 | train loss: 5.18 | val loss: 4.92 | perplexity: 136.67 | lr: 8.74e-04 | norm: 0.5282 | dt: 13779.0613ms | tok/sec: 28.5372\n",
      "step  625 | train loss: 5.39 | val loss: 4.92 | perplexity: 137.60 | lr: 8.76e-04 | norm: 0.5574 | dt: 13597.0223ms | tok/sec: 28.9193\n",
      "step  626 | train loss: 4.95 | val loss: 4.92 | perplexity: 136.90 | lr: 8.77e-04 | norm: 0.6732 | dt: 13658.5972ms | tok/sec: 28.7889\n",
      "step  627 | train loss: 4.90 | val loss: 4.90 | perplexity: 134.85 | lr: 8.78e-04 | norm: 0.6640 | dt: 13617.8377ms | tok/sec: 28.8751\n",
      "step  628 | train loss: 5.00 | val loss: 4.91 | perplexity: 135.92 | lr: 8.80e-04 | norm: 0.6980 | dt: 13635.0055ms | tok/sec: 28.8387\n",
      "step  629 | train loss: 5.15 | val loss: 4.91 | perplexity: 135.12 | lr: 8.81e-04 | norm: 0.5909 | dt: 13625.5345ms | tok/sec: 28.8588\n",
      "step  630 | train loss: 4.83 | val loss: 4.93 | perplexity: 137.87 | lr: 8.83e-04 | norm: 0.5773 | dt: 13669.4143ms | tok/sec: 28.7661\n",
      "step  631 | train loss: 5.13 | val loss: 4.92 | perplexity: 137.62 | lr: 8.84e-04 | norm: 0.4483 | dt: 13647.1436ms | tok/sec: 28.8131\n",
      "step  632 | train loss: 5.35 | val loss: 4.92 | perplexity: 136.89 | lr: 8.85e-04 | norm: 0.4881 | dt: 13632.5810ms | tok/sec: 28.8438\n",
      "step  633 | train loss: 5.15 | val loss: 4.91 | perplexity: 135.23 | lr: 8.87e-04 | norm: 0.5265 | dt: 13653.9147ms | tok/sec: 28.7988\n",
      "step  634 | train loss: 5.08 | val loss: 4.89 | perplexity: 133.29 | lr: 8.88e-04 | norm: 0.5195 | dt: 13780.7932ms | tok/sec: 28.5336\n",
      "step  635 | train loss: 4.84 | val loss: 4.88 | perplexity: 131.68 | lr: 8.90e-04 | norm: 0.5379 | dt: 13627.9409ms | tok/sec: 28.8537\n",
      "step  636 | train loss: 5.13 | val loss: 4.88 | perplexity: 131.48 | lr: 8.91e-04 | norm: 0.4637 | dt: 13590.3711ms | tok/sec: 28.9334\n",
      "step  637 | train loss: 5.04 | val loss: 4.89 | perplexity: 133.59 | lr: 8.92e-04 | norm: 0.5634 | dt: 13619.7979ms | tok/sec: 28.8709\n",
      "step  638 | train loss: 5.00 | val loss: 4.88 | perplexity: 131.78 | lr: 8.94e-04 | norm: 0.5981 | dt: 13648.8297ms | tok/sec: 28.8095\n",
      "step  639 | train loss: 5.02 | val loss: 4.88 | perplexity: 131.78 | lr: 8.95e-04 | norm: 0.5572 | dt: 13662.8194ms | tok/sec: 28.7800\n",
      "step  640 | train loss: 5.11 | val loss: 4.87 | perplexity: 130.71 | lr: 8.97e-04 | norm: 0.6311 | dt: 13648.1426ms | tok/sec: 28.8110\n",
      "step  641 | train loss: 4.95 | val loss: 4.87 | perplexity: 130.45 | lr: 8.98e-04 | norm: 0.5212 | dt: 13614.6257ms | tok/sec: 28.8819\n",
      "step  642 | train loss: 5.20 | val loss: 4.87 | perplexity: 130.18 | lr: 8.99e-04 | norm: 0.4463 | dt: 13649.3571ms | tok/sec: 28.8084\n",
      "step  643 | train loss: 5.11 | val loss: 4.87 | perplexity: 130.24 | lr: 9.01e-04 | norm: 0.5408 | dt: 13649.3473ms | tok/sec: 28.8084\n",
      "step  644 | train loss: 5.23 | val loss: 4.88 | perplexity: 131.32 | lr: 9.02e-04 | norm: 0.5458 | dt: 13727.1769ms | tok/sec: 28.6451\n",
      "step  645 | train loss: 5.08 | val loss: 4.87 | perplexity: 130.75 | lr: 9.03e-04 | norm: 0.5533 | dt: 13619.0438ms | tok/sec: 28.8725\n",
      "step  646 | train loss: 4.96 | val loss: 4.87 | perplexity: 129.88 | lr: 9.05e-04 | norm: 0.7487 | dt: 13610.0316ms | tok/sec: 28.8916\n",
      "step  647 | train loss: 4.82 | val loss: 4.87 | perplexity: 130.02 | lr: 9.06e-04 | norm: 0.4508 | dt: 13636.5569ms | tok/sec: 28.8354\n",
      "step  648 | train loss: 5.01 | val loss: 4.85 | perplexity: 127.93 | lr: 9.08e-04 | norm: 0.4935 | dt: 13641.0809ms | tok/sec: 28.8259\n",
      "step  649 | train loss: 4.97 | val loss: 4.85 | perplexity: 127.87 | lr: 9.09e-04 | norm: 0.5800 | dt: 13603.2994ms | tok/sec: 28.9059\n",
      "step  650 | train loss: 5.13 | val loss: 4.86 | perplexity: 129.14 | lr: 9.10e-04 | norm: 0.5420 | dt: 13642.3557ms | tok/sec: 28.8232\n",
      "step  651 | train loss: 4.98 | val loss: 4.86 | perplexity: 129.08 | lr: 9.12e-04 | norm: 0.6175 | dt: 13631.8617ms | tok/sec: 28.8454\n",
      "step  652 | train loss: 5.18 | val loss: 4.86 | perplexity: 128.86 | lr: 9.13e-04 | norm: 0.4974 | dt: 13605.7167ms | tok/sec: 28.9008\n",
      "step  653 | train loss: 4.84 | val loss: 4.85 | perplexity: 128.08 | lr: 9.15e-04 | norm: 0.4524 | dt: 13646.2822ms | tok/sec: 28.8149\n",
      "step  654 | train loss: 5.21 | val loss: 4.86 | perplexity: 128.90 | lr: 9.16e-04 | norm: 0.4702 | dt: 13643.3284ms | tok/sec: 28.8211\n",
      "step  655 | train loss: 5.05 | val loss: 4.85 | perplexity: 127.34 | lr: 9.17e-04 | norm: 0.4547 | dt: 13594.1939ms | tok/sec: 28.9253\n",
      "step  656 | train loss: 4.99 | val loss: 4.85 | perplexity: 127.52 | lr: 9.19e-04 | norm: 0.4162 | dt: 13688.9501ms | tok/sec: 28.7251\n",
      "step  657 | train loss: 5.02 | val loss: 4.85 | perplexity: 127.77 | lr: 9.20e-04 | norm: 0.5577 | dt: 13648.9382ms | tok/sec: 28.8093\n",
      "step  658 | train loss: 4.95 | val loss: 4.84 | perplexity: 127.01 | lr: 9.22e-04 | norm: 0.5771 | dt: 13672.4999ms | tok/sec: 28.7596\n",
      "step  659 | train loss: 4.92 | val loss: 4.84 | perplexity: 126.73 | lr: 9.23e-04 | norm: 0.4621 | dt: 13631.3705ms | tok/sec: 28.8464\n",
      "step  660 | train loss: 4.86 | val loss: 4.83 | perplexity: 125.76 | lr: 9.24e-04 | norm: 0.5937 | dt: 13642.8204ms | tok/sec: 28.8222\n",
      "step  661 | train loss: 5.00 | val loss: 4.85 | perplexity: 127.19 | lr: 9.26e-04 | norm: 0.5662 | dt: 13623.4851ms | tok/sec: 28.8631\n",
      "step  662 | train loss: 5.77 | val loss: 4.94 | perplexity: 139.55 | lr: 9.27e-04 | norm: 0.6758 | dt: 13631.0658ms | tok/sec: 28.8470\n",
      "step  663 | train loss: 5.48 | val loss: 4.88 | perplexity: 131.50 | lr: 9.29e-04 | norm: 1.4005 | dt: 13649.1318ms | tok/sec: 28.8089\n",
      "step  664 | train loss: 5.11 | val loss: 4.87 | perplexity: 130.36 | lr: 9.30e-04 | norm: 0.7755 | dt: 13654.2523ms | tok/sec: 28.7981\n",
      "step  665 | train loss: 5.01 | val loss: 4.85 | perplexity: 127.43 | lr: 9.31e-04 | norm: 0.8385 | dt: 13764.9190ms | tok/sec: 28.5665\n",
      "step  666 | train loss: 5.14 | val loss: 4.84 | perplexity: 126.58 | lr: 9.33e-04 | norm: 0.6375 | dt: 13603.5128ms | tok/sec: 28.9055\n",
      "step  667 | train loss: 5.24 | val loss: 4.85 | perplexity: 127.14 | lr: 9.34e-04 | norm: 0.5073 | dt: 13616.5915ms | tok/sec: 28.8777\n",
      "step  668 | train loss: 4.96 | val loss: 4.85 | perplexity: 127.11 | lr: 9.36e-04 | norm: 0.5742 | dt: 13611.0721ms | tok/sec: 28.8894\n",
      "step  669 | train loss: 5.02 | val loss: 4.85 | perplexity: 127.34 | lr: 9.37e-04 | norm: 0.6143 | dt: 13729.3043ms | tok/sec: 28.6406\n",
      "step  670 | train loss: 5.01 | val loss: 4.87 | perplexity: 130.10 | lr: 9.38e-04 | norm: 0.6998 | dt: 13593.4546ms | tok/sec: 28.9269\n",
      "step  671 | train loss: 4.90 | val loss: 4.85 | perplexity: 128.19 | lr: 9.40e-04 | norm: 0.7747 | dt: 13654.1843ms | tok/sec: 28.7982\n",
      "step  672 | train loss: 4.92 | val loss: 4.84 | perplexity: 127.05 | lr: 9.41e-04 | norm: 0.6383 | dt: 13586.7224ms | tok/sec: 28.9412\n",
      "step  673 | train loss: 5.04 | val loss: 4.85 | perplexity: 127.42 | lr: 9.43e-04 | norm: 0.5812 | dt: 13621.3067ms | tok/sec: 28.8677\n",
      "step  674 | train loss: 5.05 | val loss: 4.85 | perplexity: 127.73 | lr: 9.44e-04 | norm: 0.8239 | dt: 13636.7209ms | tok/sec: 28.8351\n",
      "step  675 | train loss: 5.00 | val loss: 4.84 | perplexity: 126.47 | lr: 9.45e-04 | norm: 0.7933 | dt: 13611.9137ms | tok/sec: 28.8876\n",
      "step  676 | train loss: 5.33 | val loss: 4.84 | perplexity: 126.19 | lr: 9.47e-04 | norm: 0.5936 | dt: 13655.3044ms | tok/sec: 28.7958\n",
      "step  677 | train loss: 4.73 | val loss: 4.83 | perplexity: 125.19 | lr: 9.48e-04 | norm: 0.9591 | dt: 13589.8352ms | tok/sec: 28.9346\n",
      "step  678 | train loss: 4.81 | val loss: 4.84 | perplexity: 125.93 | lr: 9.50e-04 | norm: 0.7007 | dt: 13645.3047ms | tok/sec: 28.8169\n",
      "step  679 | train loss: 4.89 | val loss: 4.85 | perplexity: 128.13 | lr: 9.51e-04 | norm: 0.5662 | dt: 13597.6071ms | tok/sec: 28.9180\n",
      "step  680 | train loss: 4.83 | val loss: 4.84 | perplexity: 127.02 | lr: 9.52e-04 | norm: 0.5812 | dt: 13771.3039ms | tok/sec: 28.5533\n",
      "step  681 | train loss: 4.94 | val loss: 4.82 | perplexity: 124.45 | lr: 9.54e-04 | norm: 0.7643 | dt: 13626.1408ms | tok/sec: 28.8575\n",
      "step  682 | train loss: 4.76 | val loss: 4.81 | perplexity: 122.71 | lr: 9.55e-04 | norm: 0.7157 | dt: 13758.7376ms | tok/sec: 28.5794\n",
      "step  683 | train loss: 4.70 | val loss: 4.81 | perplexity: 122.83 | lr: 9.57e-04 | norm: 0.7629 | dt: 13803.8042ms | tok/sec: 28.4861\n",
      "step  684 | train loss: 4.59 | val loss: 4.83 | perplexity: 124.87 | lr: 9.58e-04 | norm: 0.5919 | dt: 13688.2820ms | tok/sec: 28.7265\n",
      "step  685 | train loss: 4.85 | val loss: 4.80 | perplexity: 121.47 | lr: 9.59e-04 | norm: 0.6160 | dt: 13668.6709ms | tok/sec: 28.7677\n",
      "step  686 | train loss: 4.51 | val loss: 4.80 | perplexity: 121.02 | lr: 9.61e-04 | norm: 0.6972 | dt: 13643.0316ms | tok/sec: 28.8217\n",
      "step  687 | train loss: 6.05 | val loss: 4.85 | perplexity: 127.17 | lr: 9.62e-04 | norm: 0.9594 | dt: 13656.9264ms | tok/sec: 28.7924\n",
      "step  688 | train loss: 4.79 | val loss: 4.81 | perplexity: 123.13 | lr: 9.64e-04 | norm: 0.9482 | dt: 13626.4715ms | tok/sec: 28.8568\n",
      "step  689 | train loss: 4.88 | val loss: 4.81 | perplexity: 123.31 | lr: 9.65e-04 | norm: 0.7434 | dt: 13656.6572ms | tok/sec: 28.7930\n",
      "step  690 | train loss: 5.01 | val loss: 4.80 | perplexity: 121.09 | lr: 9.66e-04 | norm: 0.8409 | dt: 13696.5008ms | tok/sec: 28.7092\n",
      "step  691 | train loss: 4.86 | val loss: 4.80 | perplexity: 121.47 | lr: 9.68e-04 | norm: 0.5373 | dt: 13695.6346ms | tok/sec: 28.7110\n",
      "step  692 | train loss: 4.66 | val loss: 4.81 | perplexity: 122.77 | lr: 9.69e-04 | norm: 0.7974 | dt: 13673.0494ms | tok/sec: 28.7585\n",
      "step  693 | train loss: 4.95 | val loss: 4.85 | perplexity: 127.83 | lr: 9.71e-04 | norm: 0.9819 | dt: 13649.6701ms | tok/sec: 28.8077\n",
      "step  694 | train loss: 5.01 | val loss: 4.84 | perplexity: 126.97 | lr: 9.72e-04 | norm: 0.8694 | dt: 13647.9261ms | tok/sec: 28.8114\n",
      "step  695 | train loss: 4.84 | val loss: 4.84 | perplexity: 126.35 | lr: 9.73e-04 | norm: 0.6213 | dt: 13643.9667ms | tok/sec: 28.8198\n",
      "step  696 | train loss: 4.89 | val loss: 4.82 | perplexity: 124.44 | lr: 9.75e-04 | norm: 0.7425 | dt: 13678.4601ms | tok/sec: 28.7471\n",
      "step  697 | train loss: 4.88 | val loss: 4.81 | perplexity: 123.00 | lr: 9.76e-04 | norm: 0.6084 | dt: 13659.4057ms | tok/sec: 28.7872\n",
      "step  698 | train loss: 4.69 | val loss: 4.80 | perplexity: 121.55 | lr: 9.78e-04 | norm: 0.5649 | dt: 13646.4407ms | tok/sec: 28.8145\n",
      "step  699 | train loss: 4.55 | val loss: 4.81 | perplexity: 122.88 | lr: 9.79e-04 | norm: 0.9268 | dt: 13653.2214ms | tok/sec: 28.8002\n",
      "step  700 | train loss: 4.88 | val loss: 4.82 | perplexity: 124.47 | lr: 9.80e-04 | norm: 0.7351 | dt: 13684.6325ms | tok/sec: 28.7341\n",
      "step  701 | train loss: 5.24 | val loss: 4.83 | perplexity: 125.14 | lr: 9.82e-04 | norm: 0.6405 | dt: 13639.5903ms | tok/sec: 28.8290\n",
      "step  702 | train loss: 4.84 | val loss: 4.81 | perplexity: 123.03 | lr: 9.83e-04 | norm: 1.0448 | dt: 13623.7683ms | tok/sec: 28.8625\n",
      "step  703 | train loss: 4.68 | val loss: 4.81 | perplexity: 122.68 | lr: 9.85e-04 | norm: 0.5957 | dt: 13622.9670ms | tok/sec: 28.8642\n",
      "step  704 | train loss: 5.19 | val loss: 4.82 | perplexity: 123.90 | lr: 9.86e-04 | norm: 0.6027 | dt: 13694.9196ms | tok/sec: 28.7125\n",
      "step  705 | train loss: 4.81 | val loss: 4.82 | perplexity: 124.55 | lr: 9.87e-04 | norm: 0.7779 | dt: 13640.2574ms | tok/sec: 28.8276\n",
      "step  706 | train loss: 4.55 | val loss: 4.82 | perplexity: 123.86 | lr: 9.89e-04 | norm: 0.9198 | dt: 13596.8611ms | tok/sec: 28.9196\n",
      "step  707 | train loss: 4.75 | val loss: 4.80 | perplexity: 121.34 | lr: 9.90e-04 | norm: 0.8768 | dt: 13704.9074ms | tok/sec: 28.6916\n",
      "step  708 | train loss: 4.71 | val loss: 4.82 | perplexity: 123.75 | lr: 9.92e-04 | norm: 0.8629 | dt: 13632.2646ms | tok/sec: 28.8445\n",
      "step  709 | train loss: 4.74 | val loss: 4.82 | perplexity: 124.39 | lr: 9.93e-04 | norm: 0.6269 | dt: 13747.9837ms | tok/sec: 28.6017\n",
      "step  710 | train loss: 4.74 | val loss: 4.81 | perplexity: 122.63 | lr: 9.94e-04 | norm: 0.6915 | dt: 13659.9162ms | tok/sec: 28.7861\n",
      "step  711 | train loss: 4.71 | val loss: 4.82 | perplexity: 123.84 | lr: 9.96e-04 | norm: 0.7125 | dt: 13654.1631ms | tok/sec: 28.7982\n",
      "step  712 | train loss: 4.75 | val loss: 4.82 | perplexity: 123.54 | lr: 9.97e-04 | norm: 0.7262 | dt: 13666.8854ms | tok/sec: 28.7714\n",
      "step  713 | train loss: 4.51 | val loss: 4.82 | perplexity: 124.27 | lr: 9.99e-04 | norm: 0.6362 | dt: 13787.5590ms | tok/sec: 28.5196\n",
      "step  714 | train loss: 5.28 | val loss: 4.82 | perplexity: 124.14 | lr: 1.00e-03 | norm: 0.6429 | dt: 13655.4675ms | tok/sec: 28.7955\n",
      "step  715 | train loss: 4.85 | val loss: 4.83 | perplexity: 124.94 | lr: 1.00e-03 | norm: 0.6652 | dt: 13669.6153ms | tok/sec: 28.7657\n",
      "step  716 | train loss: 4.76 | val loss: 4.81 | perplexity: 122.59 | lr: 1.00e-03 | norm: 0.6081 | dt: 13633.6572ms | tok/sec: 28.8416\n",
      "step  717 | train loss: 4.97 | val loss: 4.81 | perplexity: 122.60 | lr: 1.00e-03 | norm: 0.6759 | dt: 13638.4137ms | tok/sec: 28.8315\n",
      "step  718 | train loss: 4.83 | val loss: 4.82 | perplexity: 123.73 | lr: 1.00e-03 | norm: 0.7692 | dt: 13682.8606ms | tok/sec: 28.7379\n",
      "step  719 | train loss: 5.09 | val loss: 4.83 | perplexity: 125.64 | lr: 1.00e-03 | norm: 0.7366 | dt: 13654.4740ms | tok/sec: 28.7976\n",
      "step  720 | train loss: 4.64 | val loss: 4.81 | perplexity: 123.03 | lr: 1.00e-03 | norm: 0.7257 | dt: 13602.5481ms | tok/sec: 28.9075\n",
      "step  721 | train loss: 4.93 | val loss: 4.78 | perplexity: 119.61 | lr: 1.00e-03 | norm: 0.7999 | dt: 13654.7098ms | tok/sec: 28.7971\n",
      "step  722 | train loss: 4.76 | val loss: 4.79 | perplexity: 119.88 | lr: 1.00e-03 | norm: 0.6129 | dt: 13766.0894ms | tok/sec: 28.5641\n",
      "step  723 | train loss: 4.90 | val loss: 4.77 | perplexity: 118.37 | lr: 1.00e-03 | norm: 0.7355 | dt: 13640.4507ms | tok/sec: 28.8272\n",
      "step  724 | train loss: 4.83 | val loss: 4.78 | perplexity: 119.47 | lr: 1.00e-03 | norm: 0.5443 | dt: 13679.3370ms | tok/sec: 28.7453\n",
      "step  725 | train loss: 4.83 | val loss: 4.78 | perplexity: 118.85 | lr: 1.00e-03 | norm: 0.6395 | dt: 13639.6911ms | tok/sec: 28.8288\n",
      "step  726 | train loss: 4.89 | val loss: 4.77 | perplexity: 117.58 | lr: 1.00e-03 | norm: 0.5564 | dt: 13630.3678ms | tok/sec: 28.8485\n",
      "step  727 | train loss: 4.63 | val loss: 4.74 | perplexity: 114.90 | lr: 1.00e-03 | norm: 0.5521 | dt: 13671.9794ms | tok/sec: 28.7607\n",
      "step  728 | train loss: 4.89 | val loss: 4.74 | perplexity: 114.15 | lr: 1.00e-03 | norm: 0.5803 | dt: 13637.1896ms | tok/sec: 28.8341\n",
      "step  729 | train loss: 4.89 | val loss: 4.74 | perplexity: 114.62 | lr: 1.00e-03 | norm: 0.5991 | dt: 13650.6624ms | tok/sec: 28.8056\n",
      "step  730 | train loss: 4.69 | val loss: 4.74 | perplexity: 114.73 | lr: 1.00e-03 | norm: 0.5746 | dt: 13660.8293ms | tok/sec: 28.7842\n",
      "step  731 | train loss: 5.22 | val loss: 4.77 | perplexity: 117.51 | lr: 1.00e-03 | norm: 0.6594 | dt: 13638.7558ms | tok/sec: 28.8308\n",
      "step  732 | train loss: 5.22 | val loss: 4.76 | perplexity: 116.77 | lr: 1.00e-03 | norm: 0.9478 | dt: 13809.5808ms | tok/sec: 28.4741\n",
      "step  733 | train loss: 5.52 | val loss: 4.75 | perplexity: 115.83 | lr: 1.00e-03 | norm: 0.8551 | dt: 13655.9901ms | tok/sec: 28.7944\n",
      "step  734 | train loss: 5.03 | val loss: 4.73 | perplexity: 113.54 | lr: 1.00e-03 | norm: 0.6294 | dt: 13660.1996ms | tok/sec: 28.7855\n",
      "step  735 | train loss: 4.59 | val loss: 4.73 | perplexity: 113.83 | lr: 1.00e-03 | norm: 0.5339 | dt: 13657.2049ms | tok/sec: 28.7918\n",
      "step  736 | train loss: 4.81 | val loss: 4.74 | perplexity: 114.20 | lr: 1.00e-03 | norm: 0.7986 | dt: 13622.1883ms | tok/sec: 28.8658\n",
      "step  737 | train loss: 4.59 | val loss: 4.73 | perplexity: 113.58 | lr: 1.00e-03 | norm: 0.7092 | dt: 13786.3212ms | tok/sec: 28.5222\n",
      "step  738 | train loss: 4.73 | val loss: 4.71 | perplexity: 111.38 | lr: 1.00e-03 | norm: 0.5891 | dt: 13655.5748ms | tok/sec: 28.7953\n",
      "step  739 | train loss: 4.84 | val loss: 4.70 | perplexity: 110.34 | lr: 1.00e-03 | norm: 0.6331 | dt: 13668.1104ms | tok/sec: 28.7689\n",
      "step  740 | train loss: 4.78 | val loss: 4.70 | perplexity: 110.16 | lr: 1.00e-03 | norm: 0.6695 | dt: 13612.1898ms | tok/sec: 28.8870\n",
      "step  741 | train loss: 4.83 | val loss: 4.70 | perplexity: 110.18 | lr: 9.99e-04 | norm: 0.5354 | dt: 13669.2314ms | tok/sec: 28.7665\n",
      "step  742 | train loss: 5.21 | val loss: 4.71 | perplexity: 111.57 | lr: 9.99e-04 | norm: 0.8623 | dt: 13650.6095ms | tok/sec: 28.8057\n",
      "step  743 | train loss: 5.18 | val loss: 4.74 | perplexity: 114.03 | lr: 9.99e-04 | norm: 0.8703 | dt: 13703.1860ms | tok/sec: 28.6952\n",
      "step  744 | train loss: 4.76 | val loss: 4.72 | perplexity: 111.88 | lr: 9.99e-04 | norm: 0.6687 | dt: 13616.4646ms | tok/sec: 28.8780\n",
      "step  745 | train loss: 4.78 | val loss: 4.74 | perplexity: 114.72 | lr: 9.99e-04 | norm: 0.7567 | dt: 13629.3652ms | tok/sec: 28.8506\n",
      "step  746 | train loss: 4.81 | val loss: 4.73 | perplexity: 113.77 | lr: 9.99e-04 | norm: 0.6428 | dt: 13612.8850ms | tok/sec: 28.8856\n",
      "step  747 | train loss: 4.85 | val loss: 4.75 | perplexity: 115.95 | lr: 9.99e-04 | norm: 0.6721 | dt: 13619.2749ms | tok/sec: 28.8720\n",
      "step  748 | train loss: 4.50 | val loss: 4.72 | perplexity: 112.67 | lr: 9.99e-04 | norm: 0.8758 | dt: 13632.0364ms | tok/sec: 28.8450\n",
      "step  749 | train loss: 4.86 | val loss: 4.71 | perplexity: 110.59 | lr: 9.99e-04 | norm: 0.5989 | dt: 13607.9559ms | tok/sec: 28.8960\n",
      "step  750 | train loss: 4.71 | val loss: 4.71 | perplexity: 110.95 | lr: 9.99e-04 | norm: 0.7419 | dt: 13636.7660ms | tok/sec: 28.8350\n",
      "step  751 | train loss: 4.78 | val loss: 4.68 | perplexity: 107.83 | lr: 9.99e-04 | norm: 0.7850 | dt: 13612.3335ms | tok/sec: 28.8867\n",
      "step  752 | train loss: 4.79 | val loss: 4.68 | perplexity: 107.52 | lr: 9.99e-04 | norm: 0.5701 | dt: 13648.1752ms | tok/sec: 28.8109\n",
      "step  753 | train loss: 5.05 | val loss: 4.67 | perplexity: 106.52 | lr: 9.99e-04 | norm: 0.5655 | dt: 13600.0679ms | tok/sec: 28.9128\n",
      "step  754 | train loss: 4.90 | val loss: 4.66 | perplexity: 105.89 | lr: 9.99e-04 | norm: 0.5730 | dt: 13660.8016ms | tok/sec: 28.7843\n",
      "step  755 | train loss: 4.44 | val loss: 4.65 | perplexity: 104.20 | lr: 9.99e-04 | norm: 0.6401 | dt: 13651.0675ms | tok/sec: 28.8048\n",
      "step  756 | train loss: 4.49 | val loss: 4.65 | perplexity: 105.01 | lr: 9.99e-04 | norm: 0.6456 | dt: 13671.4554ms | tok/sec: 28.7618\n",
      "step  757 | train loss: 4.78 | val loss: 4.65 | perplexity: 104.94 | lr: 9.99e-04 | norm: 0.6680 | dt: 13648.6659ms | tok/sec: 28.8098\n",
      "step  758 | train loss: 4.52 | val loss: 4.65 | perplexity: 104.44 | lr: 9.99e-04 | norm: 0.5731 | dt: 13628.0775ms | tok/sec: 28.8534\n",
      "step  759 | train loss: 4.83 | val loss: 4.66 | perplexity: 105.19 | lr: 9.98e-04 | norm: 0.6855 | dt: 13657.3217ms | tok/sec: 28.7916\n",
      "step  760 | train loss: 5.17 | val loss: 4.65 | perplexity: 104.38 | lr: 9.98e-04 | norm: 0.6850 | dt: 13656.9014ms | tok/sec: 28.7925\n",
      "step  761 | train loss: 5.00 | val loss: 4.64 | perplexity: 103.53 | lr: 9.98e-04 | norm: 0.5866 | dt: 13632.4136ms | tok/sec: 28.8442\n",
      "step  762 | train loss: 4.92 | val loss: 4.63 | perplexity: 102.68 | lr: 9.98e-04 | norm: 0.4302 | dt: 13691.8483ms | tok/sec: 28.7190\n",
      "step  763 | train loss: 4.99 | val loss: 4.63 | perplexity: 102.28 | lr: 9.98e-04 | norm: 0.5214 | dt: 13650.7368ms | tok/sec: 28.8055\n",
      "step  764 | train loss: 4.88 | val loss: 4.62 | perplexity: 101.84 | lr: 9.98e-04 | norm: 0.5019 | dt: 13635.0913ms | tok/sec: 28.8385\n",
      "step  765 | train loss: 4.72 | val loss: 4.62 | perplexity: 101.50 | lr: 9.98e-04 | norm: 0.4891 | dt: 13618.0465ms | tok/sec: 28.8746\n",
      "step  766 | train loss: 4.48 | val loss: 4.63 | perplexity: 102.35 | lr: 9.98e-04 | norm: 0.4685 | dt: 13630.8079ms | tok/sec: 28.8476\n",
      "step  767 | train loss: 5.05 | val loss: 4.63 | perplexity: 102.59 | lr: 9.98e-04 | norm: 0.6594 | dt: 13608.6154ms | tok/sec: 28.8946\n",
      "step  768 | train loss: 5.07 | val loss: 4.63 | perplexity: 102.44 | lr: 9.98e-04 | norm: 0.7104 | dt: 13580.4777ms | tok/sec: 28.9545\n",
      "step  769 | train loss: 4.53 | val loss: 4.63 | perplexity: 102.37 | lr: 9.98e-04 | norm: 0.6783 | dt: 13600.0986ms | tok/sec: 28.9127\n",
      "step  770 | train loss: 5.00 | val loss: 4.61 | perplexity: 100.40 | lr: 9.98e-04 | norm: 0.6830 | dt: 13644.2590ms | tok/sec: 28.8192\n",
      "step  771 | train loss: 4.63 | val loss: 4.63 | perplexity: 102.15 | lr: 9.98e-04 | norm: 0.7412 | dt: 13639.8823ms | tok/sec: 28.8284\n",
      "step  772 | train loss: 4.90 | val loss: 4.66 | perplexity: 106.01 | lr: 9.97e-04 | norm: 0.6803 | dt: 13618.6440ms | tok/sec: 28.8734\n",
      "step  773 | train loss: 4.55 | val loss: 4.66 | perplexity: 106.14 | lr: 9.97e-04 | norm: 0.7359 | dt: 13629.6597ms | tok/sec: 28.8500\n",
      "step  774 | train loss: 4.34 | val loss: 4.65 | perplexity: 104.96 | lr: 9.97e-04 | norm: 1.2611 | dt: 13631.1007ms | tok/sec: 28.8470\n",
      "step  775 | train loss: 4.56 | val loss: 4.64 | perplexity: 103.20 | lr: 9.97e-04 | norm: 0.5346 | dt: 13656.9066ms | tok/sec: 28.7925\n",
      "step  776 | train loss: 4.63 | val loss: 4.62 | perplexity: 101.79 | lr: 9.97e-04 | norm: 0.4989 | dt: 13623.4901ms | tok/sec: 28.8631\n",
      "step  777 | train loss: 4.65 | val loss: 4.62 | perplexity: 101.53 | lr: 9.97e-04 | norm: 0.4399 | dt: 13760.4895ms | tok/sec: 28.5757\n",
      "step  778 | train loss: 4.84 | val loss: 4.62 | perplexity: 101.56 | lr: 9.97e-04 | norm: 0.4404 | dt: 13601.7995ms | tok/sec: 28.9091\n",
      "step  779 | train loss: 4.51 | val loss: 4.61 | perplexity: 100.37 | lr: 9.97e-04 | norm: 0.5021 | dt: 13634.1732ms | tok/sec: 28.8405\n",
      "step  780 | train loss: 4.82 | val loss: 4.61 | perplexity: 100.29 | lr: 9.97e-04 | norm: 0.4039 | dt: 13633.7881ms | tok/sec: 28.8413\n",
      "step  781 | train loss: 4.76 | val loss: 4.60 | perplexity: 99.71 | lr: 9.97e-04 | norm: 0.4687 | dt: 13618.5198ms | tok/sec: 28.8736\n",
      "step  782 | train loss: 4.48 | val loss: 4.60 | perplexity: 99.75 | lr: 9.96e-04 | norm: 0.5629 | dt: 13619.1108ms | tok/sec: 28.8724\n",
      "step  783 | train loss: 4.75 | val loss: 4.59 | perplexity: 98.15 | lr: 9.96e-04 | norm: 0.5346 | dt: 13791.0838ms | tok/sec: 28.5123\n",
      "step  784 | train loss: 4.68 | val loss: 4.58 | perplexity: 97.21 | lr: 9.96e-04 | norm: 0.4449 | dt: 13755.5017ms | tok/sec: 28.5861\n",
      "step  785 | train loss: 4.71 | val loss: 4.58 | perplexity: 97.17 | lr: 9.96e-04 | norm: 0.4833 | dt: 13639.3530ms | tok/sec: 28.8295\n",
      "step  786 | train loss: 4.47 | val loss: 4.58 | perplexity: 97.76 | lr: 9.96e-04 | norm: 0.4566 | dt: 13640.8935ms | tok/sec: 28.8263\n",
      "step  787 | train loss: 4.64 | val loss: 4.58 | perplexity: 97.75 | lr: 9.96e-04 | norm: 0.4984 | dt: 13625.3541ms | tok/sec: 28.8591\n",
      "step  788 | train loss: 4.67 | val loss: 4.58 | perplexity: 97.09 | lr: 9.96e-04 | norm: 0.5153 | dt: 13647.7225ms | tok/sec: 28.8118\n",
      "step  789 | train loss: 4.69 | val loss: 4.57 | perplexity: 96.87 | lr: 9.96e-04 | norm: 0.5804 | dt: 13622.7171ms | tok/sec: 28.8647\n",
      "step  790 | train loss: 4.49 | val loss: 4.57 | perplexity: 96.28 | lr: 9.96e-04 | norm: 0.6273 | dt: 13638.1211ms | tok/sec: 28.8321\n",
      "step  791 | train loss: 4.80 | val loss: 4.58 | perplexity: 97.17 | lr: 9.95e-04 | norm: 0.6083 | dt: 13738.8539ms | tok/sec: 28.6207\n",
      "step  792 | train loss: 4.64 | val loss: 4.56 | perplexity: 95.89 | lr: 9.95e-04 | norm: 0.5825 | dt: 13622.3648ms | tok/sec: 28.8655\n",
      "step  793 | train loss: 4.71 | val loss: 4.56 | perplexity: 95.39 | lr: 9.95e-04 | norm: 0.4322 | dt: 13642.6923ms | tok/sec: 28.8225\n",
      "step  794 | train loss: 4.13 | val loss: 4.56 | perplexity: 95.95 | lr: 9.95e-04 | norm: 0.4693 | dt: 13782.7733ms | tok/sec: 28.5295\n",
      "step  795 | train loss: 4.99 | val loss: 4.57 | perplexity: 96.20 | lr: 9.95e-04 | norm: 0.4754 | dt: 13650.7685ms | tok/sec: 28.8054\n",
      "step  796 | train loss: 4.65 | val loss: 4.56 | perplexity: 95.95 | lr: 9.95e-04 | norm: 0.5257 | dt: 13664.2501ms | tok/sec: 28.7770\n",
      "step  797 | train loss: 5.06 | val loss: 4.56 | perplexity: 95.74 | lr: 9.95e-04 | norm: 0.5109 | dt: 13626.4880ms | tok/sec: 28.8567\n",
      "step  798 | train loss: 4.53 | val loss: 4.55 | perplexity: 94.53 | lr: 9.95e-04 | norm: 0.4661 | dt: 13667.1035ms | tok/sec: 28.7710\n",
      "step  799 | train loss: 4.49 | val loss: 4.54 | perplexity: 93.74 | lr: 9.94e-04 | norm: 0.4416 | dt: 13811.0368ms | tok/sec: 28.4711\n",
      "step  800 | train loss: 4.57 | val loss: 4.54 | perplexity: 93.72 | lr: 9.94e-04 | norm: 0.6473 | dt: 13697.5410ms | tok/sec: 28.7071\n",
      "step  801 | train loss: 4.68 | val loss: 4.55 | perplexity: 94.66 | lr: 9.94e-04 | norm: 0.4138 | dt: 13670.1114ms | tok/sec: 28.7647\n",
      "step  802 | train loss: 4.49 | val loss: 4.57 | perplexity: 96.38 | lr: 9.94e-04 | norm: 0.5635 | dt: 13673.9492ms | tok/sec: 28.7566\n",
      "step  803 | train loss: 4.57 | val loss: 4.56 | perplexity: 95.68 | lr: 9.94e-04 | norm: 0.6538 | dt: 13673.7731ms | tok/sec: 28.7569\n",
      "step  804 | train loss: 4.79 | val loss: 4.56 | perplexity: 95.54 | lr: 9.94e-04 | norm: 0.5108 | dt: 13615.6409ms | tok/sec: 28.8797\n",
      "step  805 | train loss: 4.74 | val loss: 4.56 | perplexity: 95.19 | lr: 9.94e-04 | norm: 0.7087 | dt: 13638.6116ms | tok/sec: 28.8311\n",
      "step  806 | train loss: 5.24 | val loss: 4.56 | perplexity: 95.31 | lr: 9.94e-04 | norm: 0.5641 | dt: 13649.3986ms | tok/sec: 28.8083\n",
      "step  807 | train loss: 4.56 | val loss: 4.54 | perplexity: 94.03 | lr: 9.93e-04 | norm: 0.5681 | dt: 13693.6798ms | tok/sec: 28.7151\n",
      "step  808 | train loss: 4.64 | val loss: 4.54 | perplexity: 93.84 | lr: 9.93e-04 | norm: 0.4251 | dt: 13680.6526ms | tok/sec: 28.7425\n",
      "step  809 | train loss: 4.75 | val loss: 4.54 | perplexity: 93.67 | lr: 9.93e-04 | norm: 0.4921 | dt: 13697.3715ms | tok/sec: 28.7074\n",
      "step  810 | train loss: 4.71 | val loss: 4.54 | perplexity: 93.31 | lr: 9.93e-04 | norm: 0.4497 | dt: 13627.0518ms | tok/sec: 28.8555\n",
      "step  811 | train loss: 4.66 | val loss: 4.53 | perplexity: 93.03 | lr: 9.93e-04 | norm: 0.5208 | dt: 13649.4493ms | tok/sec: 28.8082\n",
      "step  812 | train loss: 4.70 | val loss: 4.55 | perplexity: 94.30 | lr: 9.93e-04 | norm: 0.4627 | dt: 13660.5151ms | tok/sec: 28.7849\n",
      "step  813 | train loss: 4.83 | val loss: 4.55 | perplexity: 94.64 | lr: 9.93e-04 | norm: 0.5452 | dt: 13625.9294ms | tok/sec: 28.8579\n",
      "step  814 | train loss: 4.53 | val loss: 4.55 | perplexity: 94.25 | lr: 9.92e-04 | norm: 0.6603 | dt: 13665.7016ms | tok/sec: 28.7739\n",
      "step  815 | train loss: 4.57 | val loss: 4.53 | perplexity: 92.77 | lr: 9.92e-04 | norm: 0.5823 | dt: 13806.5522ms | tok/sec: 28.4804\n",
      "step  816 | train loss: 4.66 | val loss: 4.54 | perplexity: 93.94 | lr: 9.92e-04 | norm: 0.6117 | dt: 13745.2571ms | tok/sec: 28.6074\n",
      "step  817 | train loss: 4.80 | val loss: 4.56 | perplexity: 95.78 | lr: 9.92e-04 | norm: 0.5302 | dt: 13703.3801ms | tok/sec: 28.6948\n",
      "step  818 | train loss: 4.56 | val loss: 4.54 | perplexity: 93.30 | lr: 9.92e-04 | norm: 0.6330 | dt: 13643.0688ms | tok/sec: 28.8217\n",
      "step  819 | train loss: 4.73 | val loss: 4.53 | perplexity: 92.43 | lr: 9.92e-04 | norm: 0.5168 | dt: 13673.5384ms | tok/sec: 28.7574\n",
      "step  820 | train loss: 4.83 | val loss: 4.53 | perplexity: 92.44 | lr: 9.91e-04 | norm: 0.4844 | dt: 13642.0436ms | tok/sec: 28.8238\n",
      "step  821 | train loss: 4.86 | val loss: 4.52 | perplexity: 91.72 | lr: 9.91e-04 | norm: 0.4370 | dt: 13677.9449ms | tok/sec: 28.7482\n",
      "step  822 | train loss: 4.84 | val loss: 4.51 | perplexity: 90.85 | lr: 9.91e-04 | norm: 0.5784 | dt: 13676.5380ms | tok/sec: 28.7511\n",
      "step  823 | train loss: 4.90 | val loss: 4.51 | perplexity: 90.55 | lr: 9.91e-04 | norm: 0.4740 | dt: 13624.6250ms | tok/sec: 28.8607\n",
      "step  824 | train loss: 4.52 | val loss: 4.51 | perplexity: 90.65 | lr: 9.91e-04 | norm: 0.4794 | dt: 13598.6974ms | tok/sec: 28.9157\n",
      "step  825 | train loss: 4.63 | val loss: 4.49 | perplexity: 88.98 | lr: 9.91e-04 | norm: 0.4636 | dt: 13625.1137ms | tok/sec: 28.8596\n",
      "step  826 | train loss: 4.84 | val loss: 4.49 | perplexity: 89.33 | lr: 9.90e-04 | norm: 0.4314 | dt: 13666.7154ms | tok/sec: 28.7718\n",
      "step  827 | train loss: 4.57 | val loss: 4.49 | perplexity: 88.91 | lr: 9.90e-04 | norm: 0.5283 | dt: 13635.2470ms | tok/sec: 28.8382\n",
      "step  828 | train loss: 4.85 | val loss: 4.49 | perplexity: 88.78 | lr: 9.90e-04 | norm: 0.4363 | dt: 13614.9530ms | tok/sec: 28.8812\n",
      "step  829 | train loss: 4.56 | val loss: 4.49 | perplexity: 89.20 | lr: 9.90e-04 | norm: 0.5533 | dt: 13793.9305ms | tok/sec: 28.5065\n",
      "step  830 | train loss: 4.90 | val loss: 4.49 | perplexity: 89.24 | lr: 9.90e-04 | norm: 0.5554 | dt: 13635.9138ms | tok/sec: 28.8368\n",
      "step  831 | train loss: 4.70 | val loss: 4.48 | perplexity: 88.56 | lr: 9.90e-04 | norm: 0.4946 | dt: 13640.4316ms | tok/sec: 28.8272\n",
      "step  832 | train loss: 4.63 | val loss: 4.48 | perplexity: 88.56 | lr: 9.89e-04 | norm: 0.5166 | dt: 13624.4853ms | tok/sec: 28.8610\n",
      "step  833 | train loss: 4.99 | val loss: 4.49 | perplexity: 88.78 | lr: 9.89e-04 | norm: 0.5290 | dt: 13627.4648ms | tok/sec: 28.8547\n",
      "step  834 | train loss: 4.15 | val loss: 4.49 | perplexity: 89.18 | lr: 9.89e-04 | norm: 0.5739 | dt: 13672.6305ms | tok/sec: 28.7594\n",
      "step  835 | train loss: 4.80 | val loss: 4.49 | perplexity: 88.94 | lr: 9.89e-04 | norm: 0.5428 | dt: 13656.0535ms | tok/sec: 28.7943\n",
      "step  836 | train loss: 4.76 | val loss: 4.49 | perplexity: 88.94 | lr: 9.89e-04 | norm: 0.5576 | dt: 13646.8129ms | tok/sec: 28.8138\n",
      "step  837 | train loss: 5.10 | val loss: 4.52 | perplexity: 92.15 | lr: 9.88e-04 | norm: 0.9938 | dt: 13662.4463ms | tok/sec: 28.7808\n",
      "step  838 | train loss: 4.90 | val loss: 4.52 | perplexity: 92.29 | lr: 9.88e-04 | norm: 0.9258 | dt: 13643.5697ms | tok/sec: 28.8206\n",
      "step  839 | train loss: 4.69 | val loss: 4.54 | perplexity: 93.29 | lr: 9.88e-04 | norm: 0.5834 | dt: 13649.0765ms | tok/sec: 28.8090\n",
      "step  840 | train loss: 5.13 | val loss: 4.52 | perplexity: 92.27 | lr: 9.88e-04 | norm: 0.7081 | dt: 13695.6398ms | tok/sec: 28.7110\n",
      "step  841 | train loss: 4.88 | val loss: 4.53 | perplexity: 92.84 | lr: 9.88e-04 | norm: 0.6582 | dt: 13719.4827ms | tok/sec: 28.6611\n",
      "step  842 | train loss: 4.58 | val loss: 4.50 | perplexity: 90.27 | lr: 9.87e-04 | norm: 0.7751 | dt: 13687.0837ms | tok/sec: 28.7290\n",
      "step  843 | train loss: 4.91 | val loss: 4.50 | perplexity: 89.58 | lr: 9.87e-04 | norm: 0.6425 | dt: 13601.5632ms | tok/sec: 28.9096\n",
      "step  844 | train loss: 4.67 | val loss: 4.48 | perplexity: 88.18 | lr: 9.87e-04 | norm: 0.5687 | dt: 13684.9394ms | tok/sec: 28.7335\n",
      "step  845 | train loss: 4.71 | val loss: 4.48 | perplexity: 88.11 | lr: 9.87e-04 | norm: 0.5388 | dt: 13657.1021ms | tok/sec: 28.7921\n",
      "step  846 | train loss: 4.55 | val loss: 4.49 | perplexity: 88.90 | lr: 9.87e-04 | norm: 0.5615 | dt: 13671.5438ms | tok/sec: 28.7616\n",
      "step  847 | train loss: 4.86 | val loss: 4.48 | perplexity: 88.11 | lr: 9.86e-04 | norm: 0.4815 | dt: 13624.3415ms | tok/sec: 28.8613\n",
      "step  848 | train loss: 5.63 | val loss: 4.49 | perplexity: 88.70 | lr: 9.86e-04 | norm: 0.8823 | dt: 13731.7111ms | tok/sec: 28.6356\n",
      "step  849 | train loss: 4.49 | val loss: 4.47 | perplexity: 87.68 | lr: 9.86e-04 | norm: 0.5875 | dt: 13616.5626ms | tok/sec: 28.8778\n",
      "step  850 | train loss: 4.73 | val loss: 4.48 | perplexity: 88.42 | lr: 9.86e-04 | norm: 0.4773 | dt: 13620.5084ms | tok/sec: 28.8694\n",
      "step  851 | train loss: 4.86 | val loss: 4.48 | perplexity: 88.05 | lr: 9.86e-04 | norm: 0.6743 | dt: 13655.0033ms | tok/sec: 28.7965\n",
      "step  852 | train loss: 4.58 | val loss: 4.48 | perplexity: 87.88 | lr: 9.85e-04 | norm: 0.5453 | dt: 13687.3939ms | tok/sec: 28.7283\n",
      "step  853 | train loss: 4.81 | val loss: 4.46 | perplexity: 86.66 | lr: 9.85e-04 | norm: 0.9282 | dt: 13609.6427ms | tok/sec: 28.8925\n",
      "step  854 | train loss: 4.63 | val loss: 4.47 | perplexity: 87.45 | lr: 9.85e-04 | norm: 0.4540 | dt: 13664.6392ms | tok/sec: 28.7762\n",
      "step  855 | train loss: 4.39 | val loss: 4.46 | perplexity: 86.13 | lr: 9.85e-04 | norm: 0.6144 | dt: 13687.3548ms | tok/sec: 28.7284\n",
      "step  856 | train loss: 4.36 | val loss: 4.45 | perplexity: 85.32 | lr: 9.85e-04 | norm: 0.4572 | dt: 13660.9330ms | tok/sec: 28.7840\n",
      "step  857 | train loss: 4.60 | val loss: 4.45 | perplexity: 85.25 | lr: 9.84e-04 | norm: 0.4389 | dt: 13674.8757ms | tok/sec: 28.7546\n",
      "step  858 | train loss: 4.47 | val loss: 4.44 | perplexity: 85.00 | lr: 9.84e-04 | norm: 0.3909 | dt: 13667.0945ms | tok/sec: 28.7710\n",
      "step  859 | train loss: 4.98 | val loss: 4.46 | perplexity: 86.28 | lr: 9.84e-04 | norm: 0.7892 | dt: 13624.4366ms | tok/sec: 28.8611\n",
      "step  860 | train loss: 4.74 | val loss: 4.46 | perplexity: 86.69 | lr: 9.84e-04 | norm: 0.5906 | dt: 13670.9824ms | tok/sec: 28.7628\n",
      "step  861 | train loss: 4.66 | val loss: 4.46 | perplexity: 86.17 | lr: 9.83e-04 | norm: 0.6156 | dt: 13633.7476ms | tok/sec: 28.8414\n",
      "step  862 | train loss: 4.56 | val loss: 4.46 | perplexity: 86.61 | lr: 9.83e-04 | norm: 0.5381 | dt: 13621.6700ms | tok/sec: 28.8669\n",
      "step  863 | train loss: 4.98 | val loss: 4.47 | perplexity: 87.43 | lr: 9.83e-04 | norm: 0.6311 | dt: 13599.5021ms | tok/sec: 28.9140\n",
      "step  864 | train loss: 4.49 | val loss: 4.48 | perplexity: 88.67 | lr: 9.83e-04 | norm: 0.5569 | dt: 13640.6510ms | tok/sec: 28.8268\n",
      "step  865 | train loss: 4.74 | val loss: 4.49 | perplexity: 89.45 | lr: 9.83e-04 | norm: 0.6314 | dt: 13656.4515ms | tok/sec: 28.7934\n",
      "step  866 | train loss: 4.70 | val loss: 4.46 | perplexity: 86.88 | lr: 9.82e-04 | norm: 0.6140 | dt: 13611.9542ms | tok/sec: 28.8875\n",
      "step  867 | train loss: 5.25 | val loss: 4.48 | perplexity: 88.03 | lr: 9.82e-04 | norm: 1.9258 | dt: 13608.1774ms | tok/sec: 28.8956\n",
      "step  868 | train loss: 4.39 | val loss: 4.47 | perplexity: 87.48 | lr: 9.82e-04 | norm: 0.5971 | dt: 13621.7263ms | tok/sec: 28.8668\n",
      "step  869 | train loss: 4.70 | val loss: 4.48 | perplexity: 88.09 | lr: 9.82e-04 | norm: 0.5328 | dt: 13634.7823ms | tok/sec: 28.8392\n",
      "step  870 | train loss: 4.66 | val loss: 4.47 | perplexity: 87.66 | lr: 9.81e-04 | norm: 0.5648 | dt: 13611.9659ms | tok/sec: 28.8875\n",
      "step  871 | train loss: 4.54 | val loss: 4.46 | perplexity: 86.18 | lr: 9.81e-04 | norm: 0.5368 | dt: 13680.1286ms | tok/sec: 28.7436\n",
      "step  872 | train loss: 4.60 | val loss: 4.45 | perplexity: 85.84 | lr: 9.81e-04 | norm: 0.4976 | dt: 13772.8982ms | tok/sec: 28.5500\n",
      "step  873 | train loss: 4.35 | val loss: 4.48 | perplexity: 88.43 | lr: 9.81e-04 | norm: 0.5897 | dt: 13638.8695ms | tok/sec: 28.8305\n",
      "step  874 | train loss: 4.86 | val loss: 4.45 | perplexity: 85.29 | lr: 9.80e-04 | norm: 0.7380 | dt: 13629.4622ms | tok/sec: 28.8504\n",
      "step  875 | train loss: 4.78 | val loss: 4.45 | perplexity: 85.31 | lr: 9.80e-04 | norm: 0.5119 | dt: 13635.4015ms | tok/sec: 28.8379\n",
      "step  876 | train loss: 4.65 | val loss: 4.42 | perplexity: 83.42 | lr: 9.80e-04 | norm: 0.6703 | dt: 13641.7673ms | tok/sec: 28.8244\n",
      "step  877 | train loss: 4.62 | val loss: 4.39 | perplexity: 80.84 | lr: 9.80e-04 | norm: 0.7167 | dt: 13655.5004ms | tok/sec: 28.7954\n",
      "step  878 | train loss: 4.46 | val loss: 4.39 | perplexity: 80.59 | lr: 9.79e-04 | norm: 0.4013 | dt: 13617.2397ms | tok/sec: 28.8763\n",
      "step  879 | train loss: 4.70 | val loss: 4.38 | perplexity: 79.86 | lr: 9.79e-04 | norm: 0.4889 | dt: 13609.2572ms | tok/sec: 28.8933\n",
      "step  880 | train loss: 4.43 | val loss: 4.38 | perplexity: 79.80 | lr: 9.79e-04 | norm: 0.5175 | dt: 13644.9621ms | tok/sec: 28.8177\n",
      "step  881 | train loss: 4.58 | val loss: 4.37 | perplexity: 79.34 | lr: 9.79e-04 | norm: 0.4463 | dt: 13593.3223ms | tok/sec: 28.9271\n",
      "step  882 | train loss: 4.62 | val loss: 4.39 | perplexity: 80.90 | lr: 9.78e-04 | norm: 0.4828 | dt: 13629.8382ms | tok/sec: 28.8496\n",
      "step  883 | train loss: 4.97 | val loss: 4.40 | perplexity: 81.69 | lr: 9.78e-04 | norm: 0.5263 | dt: 13759.8879ms | tok/sec: 28.5770\n",
      "step  884 | train loss: 4.65 | val loss: 4.38 | perplexity: 80.23 | lr: 9.78e-04 | norm: 0.5575 | dt: 13615.0994ms | tok/sec: 28.8809\n",
      "step  885 | train loss: 4.62 | val loss: 4.38 | perplexity: 79.55 | lr: 9.78e-04 | norm: 0.4805 | dt: 13746.5572ms | tok/sec: 28.6047\n",
      "step  886 | train loss: 4.41 | val loss: 4.38 | perplexity: 79.85 | lr: 9.77e-04 | norm: 0.6083 | dt: 13629.7510ms | tok/sec: 28.8498\n",
      "step  887 | train loss: 4.57 | val loss: 4.38 | perplexity: 79.84 | lr: 9.77e-04 | norm: 0.4779 | dt: 13612.6730ms | tok/sec: 28.8860\n",
      "step  888 | train loss: 4.58 | val loss: 4.38 | perplexity: 80.13 | lr: 9.77e-04 | norm: 0.4295 | dt: 13656.9412ms | tok/sec: 28.7924\n",
      "step  889 | train loss: 4.77 | val loss: 4.38 | perplexity: 79.69 | lr: 9.77e-04 | norm: 0.4312 | dt: 13622.5176ms | tok/sec: 28.8651\n",
      "step  890 | train loss: 4.53 | val loss: 4.36 | perplexity: 78.37 | lr: 9.76e-04 | norm: 0.4705 | dt: 13599.9110ms | tok/sec: 28.9131\n",
      "step  891 | train loss: 4.52 | val loss: 4.35 | perplexity: 77.37 | lr: 9.76e-04 | norm: 0.4247 | dt: 13616.4966ms | tok/sec: 28.8779\n",
      "step  892 | train loss: 4.88 | val loss: 4.34 | perplexity: 77.01 | lr: 9.76e-04 | norm: 0.4850 | dt: 13636.2104ms | tok/sec: 28.8362\n",
      "step  893 | train loss: 4.71 | val loss: 4.34 | perplexity: 76.73 | lr: 9.75e-04 | norm: 0.4187 | dt: 13659.2913ms | tok/sec: 28.7874\n",
      "step  894 | train loss: 4.44 | val loss: 4.34 | perplexity: 76.42 | lr: 9.75e-04 | norm: 0.4032 | dt: 13594.2833ms | tok/sec: 28.9251\n",
      "step  895 | train loss: 4.52 | val loss: 4.34 | perplexity: 76.99 | lr: 9.75e-04 | norm: 0.5470 | dt: 13775.1980ms | tok/sec: 28.5452\n",
      "step  896 | train loss: 5.49 | val loss: 4.37 | perplexity: 79.31 | lr: 9.75e-04 | norm: 0.9079 | dt: 13613.3130ms | tok/sec: 28.8847\n",
      "step  897 | train loss: 4.67 | val loss: 4.37 | perplexity: 79.35 | lr: 9.74e-04 | norm: 0.6678 | dt: 13669.8685ms | tok/sec: 28.7652\n",
      "step  898 | train loss: 4.53 | val loss: 4.37 | perplexity: 79.08 | lr: 9.74e-04 | norm: 0.5305 | dt: 13671.1855ms | tok/sec: 28.7624\n",
      "step  899 | train loss: 4.57 | val loss: 4.37 | perplexity: 79.08 | lr: 9.74e-04 | norm: 0.6540 | dt: 13666.4996ms | tok/sec: 28.7723\n",
      "step  900 | train loss: 4.73 | val loss: 4.38 | perplexity: 80.14 | lr: 9.73e-04 | norm: 0.7412 | dt: 13668.6161ms | tok/sec: 28.7678\n",
      "step  901 | train loss: 4.61 | val loss: 4.40 | perplexity: 81.10 | lr: 9.73e-04 | norm: 0.5970 | dt: 13616.1685ms | tok/sec: 28.8786\n",
      "step  902 | train loss: 4.88 | val loss: 4.38 | perplexity: 79.90 | lr: 9.73e-04 | norm: 0.7058 | dt: 13623.1177ms | tok/sec: 28.8639\n",
      "step  903 | train loss: 4.83 | val loss: 4.37 | perplexity: 79.03 | lr: 9.73e-04 | norm: 0.6429 | dt: 13636.4694ms | tok/sec: 28.8356\n",
      "step  904 | train loss: 4.57 | val loss: 4.36 | perplexity: 78.23 | lr: 9.72e-04 | norm: 0.5424 | dt: 13603.1342ms | tok/sec: 28.9063\n",
      "step  905 | train loss: 4.72 | val loss: 4.36 | perplexity: 77.94 | lr: 9.72e-04 | norm: 0.5337 | dt: 13632.5405ms | tok/sec: 28.8439\n",
      "step  906 | train loss: 4.87 | val loss: 4.36 | perplexity: 78.48 | lr: 9.72e-04 | norm: 0.4833 | dt: 13659.1187ms | tok/sec: 28.7878\n",
      "step  907 | train loss: 4.66 | val loss: 4.38 | perplexity: 79.84 | lr: 9.71e-04 | norm: 0.5487 | dt: 13624.5027ms | tok/sec: 28.8609\n",
      "step  908 | train loss: 4.97 | val loss: 4.40 | perplexity: 81.06 | lr: 9.71e-04 | norm: 0.6114 | dt: 13635.8612ms | tok/sec: 28.8369\n",
      "step  909 | train loss: 4.85 | val loss: 4.38 | perplexity: 79.96 | lr: 9.71e-04 | norm: 0.7336 | dt: 13649.6668ms | tok/sec: 28.8077\n",
      "step  910 | train loss: 4.61 | val loss: 4.37 | perplexity: 79.34 | lr: 9.71e-04 | norm: 0.5427 | dt: 13662.3244ms | tok/sec: 28.7810\n",
      "step  911 | train loss: 4.83 | val loss: 4.40 | perplexity: 81.71 | lr: 9.70e-04 | norm: 0.5536 | dt: 13781.3175ms | tok/sec: 28.5325\n",
      "step  912 | train loss: 4.42 | val loss: 4.38 | perplexity: 80.21 | lr: 9.70e-04 | norm: 0.6971 | dt: 13641.0422ms | tok/sec: 28.8259\n",
      "step  913 | train loss: 4.82 | val loss: 4.36 | perplexity: 78.41 | lr: 9.70e-04 | norm: 0.6399 | dt: 13630.5737ms | tok/sec: 28.8481\n",
      "step  914 | train loss: 5.00 | val loss: 4.37 | perplexity: 78.86 | lr: 9.69e-04 | norm: 0.4864 | dt: 13726.5615ms | tok/sec: 28.6464\n",
      "step  915 | train loss: 4.93 | val loss: 4.36 | perplexity: 78.51 | lr: 9.69e-04 | norm: 0.6599 | dt: 13699.7552ms | tok/sec: 28.7024\n",
      "step  916 | train loss: 4.61 | val loss: 4.38 | perplexity: 79.73 | lr: 9.69e-04 | norm: 0.5220 | dt: 13743.3352ms | tok/sec: 28.6114\n",
      "step  917 | train loss: 4.92 | val loss: 4.37 | perplexity: 79.03 | lr: 9.68e-04 | norm: 0.5836 | dt: 13804.0860ms | tok/sec: 28.4855\n",
      "step  918 | train loss: 4.68 | val loss: 4.36 | perplexity: 78.16 | lr: 9.68e-04 | norm: 0.4538 | dt: 13632.0555ms | tok/sec: 28.8450\n",
      "step  919 | train loss: 4.78 | val loss: 4.35 | perplexity: 77.27 | lr: 9.68e-04 | norm: 0.4075 | dt: 13785.5198ms | tok/sec: 28.5238\n",
      "step  920 | train loss: 4.48 | val loss: 4.34 | perplexity: 76.94 | lr: 9.68e-04 | norm: 0.4105 | dt: 13848.4757ms | tok/sec: 28.3942\n",
      "step  921 | train loss: 4.80 | val loss: 4.34 | perplexity: 76.56 | lr: 9.67e-04 | norm: 0.5288 | dt: 13668.6671ms | tok/sec: 28.7677\n",
      "step  922 | train loss: 4.53 | val loss: 4.35 | perplexity: 77.25 | lr: 9.67e-04 | norm: 0.4916 | dt: 13662.2195ms | tok/sec: 28.7813\n",
      "step  923 | train loss: 4.58 | val loss: 4.35 | perplexity: 77.52 | lr: 9.67e-04 | norm: 0.4889 | dt: 13684.9227ms | tok/sec: 28.7335\n",
      "step  924 | train loss: 4.68 | val loss: 4.34 | perplexity: 76.52 | lr: 9.66e-04 | norm: 0.4994 | dt: 13752.2647ms | tok/sec: 28.5928\n",
      "step  925 | train loss: 4.66 | val loss: 4.34 | perplexity: 76.48 | lr: 9.66e-04 | norm: 0.4608 | dt: 13698.1292ms | tok/sec: 28.7058\n",
      "step  926 | train loss: 4.13 | val loss: 4.34 | perplexity: 76.79 | lr: 9.66e-04 | norm: 0.4338 | dt: 13711.1163ms | tok/sec: 28.6786\n",
      "step  927 | train loss: 4.56 | val loss: 4.35 | perplexity: 77.10 | lr: 9.65e-04 | norm: 0.5610 | dt: 13659.4391ms | tok/sec: 28.7871\n",
      "step  928 | train loss: 4.62 | val loss: 4.35 | perplexity: 77.23 | lr: 9.65e-04 | norm: 0.4536 | dt: 13681.7358ms | tok/sec: 28.7402\n",
      "step  929 | train loss: 4.64 | val loss: 4.33 | perplexity: 75.67 | lr: 9.65e-04 | norm: 0.5045 | dt: 13669.5287ms | tok/sec: 28.7659\n",
      "step  930 | train loss: 4.68 | val loss: 4.32 | perplexity: 74.94 | lr: 9.64e-04 | norm: 0.4723 | dt: 13632.3521ms | tok/sec: 28.8443\n",
      "step  931 | train loss: 4.69 | val loss: 4.32 | perplexity: 75.55 | lr: 9.64e-04 | norm: 0.6213 | dt: 13648.2666ms | tok/sec: 28.8107\n",
      "step  932 | train loss: 4.61 | val loss: 4.34 | perplexity: 76.44 | lr: 9.64e-04 | norm: 0.5076 | dt: 13646.2076ms | tok/sec: 28.8150\n",
      "step  933 | train loss: 4.50 | val loss: 4.34 | perplexity: 76.90 | lr: 9.63e-04 | norm: 0.4435 | dt: 13670.1646ms | tok/sec: 28.7645\n",
      "step  934 | train loss: 4.36 | val loss: 4.33 | perplexity: 76.00 | lr: 9.63e-04 | norm: 0.4172 | dt: 13678.9732ms | tok/sec: 28.7460\n",
      "step  935 | train loss: 4.38 | val loss: 4.32 | perplexity: 75.26 | lr: 9.63e-04 | norm: 0.4413 | dt: 13656.9910ms | tok/sec: 28.7923\n",
      "step  936 | train loss: 4.60 | val loss: 4.32 | perplexity: 75.20 | lr: 9.62e-04 | norm: 0.4033 | dt: 13658.1278ms | tok/sec: 28.7899\n",
      "step  937 | train loss: 4.61 | val loss: 4.32 | perplexity: 74.86 | lr: 9.62e-04 | norm: 0.5266 | dt: 13636.5876ms | tok/sec: 28.8354\n",
      "step  938 | train loss: 4.40 | val loss: 4.32 | perplexity: 75.54 | lr: 9.62e-04 | norm: 0.5052 | dt: 13655.6656ms | tok/sec: 28.7951\n",
      "step  939 | train loss: 4.55 | val loss: 4.33 | perplexity: 76.04 | lr: 9.61e-04 | norm: 0.4382 | dt: 13609.5264ms | tok/sec: 28.8927\n",
      "step  940 | train loss: 4.59 | val loss: 4.33 | perplexity: 76.10 | lr: 9.61e-04 | norm: 0.5029 | dt: 13609.1840ms | tok/sec: 28.8934\n",
      "step  941 | train loss: 4.51 | val loss: 4.33 | perplexity: 76.26 | lr: 9.61e-04 | norm: 0.4195 | dt: 13663.5482ms | tok/sec: 28.7785\n",
      "step  942 | train loss: 4.49 | val loss: 4.34 | perplexity: 77.02 | lr: 9.60e-04 | norm: 0.6857 | dt: 13599.1828ms | tok/sec: 28.9147\n",
      "step  943 | train loss: 4.43 | val loss: 4.33 | perplexity: 76.24 | lr: 9.60e-04 | norm: 0.5778 | dt: 13666.9617ms | tok/sec: 28.7713\n",
      "step  944 | train loss: 4.67 | val loss: 4.34 | perplexity: 76.61 | lr: 9.60e-04 | norm: 0.4869 | dt: 13641.5918ms | tok/sec: 28.8248\n",
      "step  945 | train loss: 4.91 | val loss: 4.34 | perplexity: 76.98 | lr: 9.59e-04 | norm: 0.5638 | dt: 13647.1276ms | tok/sec: 28.8131\n",
      "step  946 | train loss: 4.45 | val loss: 4.34 | perplexity: 76.89 | lr: 9.59e-04 | norm: 0.5677 | dt: 13636.0087ms | tok/sec: 28.8366\n",
      "step  947 | train loss: 4.22 | val loss: 4.35 | perplexity: 77.58 | lr: 9.59e-04 | norm: 0.4398 | dt: 13672.2069ms | tok/sec: 28.7602\n",
      "step  948 | train loss: 4.33 | val loss: 4.35 | perplexity: 77.32 | lr: 9.58e-04 | norm: 0.4852 | dt: 13638.8311ms | tok/sec: 28.8306\n",
      "step  949 | train loss: 4.42 | val loss: 4.35 | perplexity: 77.63 | lr: 9.58e-04 | norm: 0.5301 | dt: 13648.2015ms | tok/sec: 28.8108\n",
      "step  950 | train loss: 4.40 | val loss: 4.35 | perplexity: 77.83 | lr: 9.57e-04 | norm: 0.4349 | dt: 13674.4881ms | tok/sec: 28.7554\n",
      "step  951 | train loss: 4.29 | val loss: 4.34 | perplexity: 76.95 | lr: 9.57e-04 | norm: 0.5407 | dt: 13656.3988ms | tok/sec: 28.7935\n",
      "step  952 | train loss: 4.92 | val loss: 4.35 | perplexity: 77.77 | lr: 9.57e-04 | norm: 0.5506 | dt: 13680.7005ms | tok/sec: 28.7424\n",
      "step  953 | train loss: 4.65 | val loss: 4.35 | perplexity: 77.26 | lr: 9.56e-04 | norm: 0.4993 | dt: 13640.3871ms | tok/sec: 28.8273\n",
      "step  954 | train loss: 4.42 | val loss: 4.34 | perplexity: 76.98 | lr: 9.56e-04 | norm: 0.4993 | dt: 13698.4968ms | tok/sec: 28.7050\n",
      "step  955 | train loss: 4.60 | val loss: 4.34 | perplexity: 76.88 | lr: 9.56e-04 | norm: 0.4030 | dt: 13591.6846ms | tok/sec: 28.9306\n",
      "step  956 | train loss: 4.80 | val loss: 4.34 | perplexity: 76.88 | lr: 9.55e-04 | norm: 0.8837 | dt: 13587.3635ms | tok/sec: 28.9398\n",
      "step  957 | train loss: 4.60 | val loss: 4.34 | perplexity: 76.58 | lr: 9.55e-04 | norm: 0.5170 | dt: 13617.2955ms | tok/sec: 28.8762\n",
      "step  958 | train loss: 4.16 | val loss: 4.33 | perplexity: 76.26 | lr: 9.55e-04 | norm: 0.6207 | dt: 13644.3114ms | tok/sec: 28.8190\n",
      "step  959 | train loss: 4.60 | val loss: 4.35 | perplexity: 77.13 | lr: 9.54e-04 | norm: 0.5758 | dt: 13790.7691ms | tok/sec: 28.5130\n",
      "step  960 | train loss: 4.56 | val loss: 4.34 | perplexity: 76.51 | lr: 9.54e-04 | norm: 0.6528 | dt: 13638.3648ms | tok/sec: 28.8316\n",
      "step  961 | train loss: 4.63 | val loss: 4.34 | perplexity: 76.40 | lr: 9.53e-04 | norm: 0.5260 | dt: 13654.5708ms | tok/sec: 28.7974\n",
      "step  962 | train loss: 4.38 | val loss: 4.33 | perplexity: 76.18 | lr: 9.53e-04 | norm: 0.5047 | dt: 13607.9466ms | tok/sec: 28.8961\n",
      "step  963 | train loss: 4.32 | val loss: 4.32 | perplexity: 75.18 | lr: 9.53e-04 | norm: 0.4723 | dt: 13627.7442ms | tok/sec: 28.8541\n",
      "step  964 | train loss: 4.29 | val loss: 4.32 | perplexity: 74.89 | lr: 9.52e-04 | norm: 0.5013 | dt: 13613.7350ms | tok/sec: 28.8838\n",
      "step  965 | train loss: 4.62 | val loss: 4.32 | perplexity: 75.20 | lr: 9.52e-04 | norm: 0.4042 | dt: 13630.4810ms | tok/sec: 28.8483\n",
      "step  966 | train loss: 4.26 | val loss: 4.33 | perplexity: 75.70 | lr: 9.52e-04 | norm: 0.5590 | dt: 13605.4473ms | tok/sec: 28.9014\n",
      "step  967 | train loss: 4.61 | val loss: 4.33 | perplexity: 75.63 | lr: 9.51e-04 | norm: 0.5186 | dt: 13641.8257ms | tok/sec: 28.8243\n",
      "step  968 | train loss: 4.77 | val loss: 4.33 | perplexity: 75.98 | lr: 9.51e-04 | norm: 0.4810 | dt: 13627.5601ms | tok/sec: 28.8545\n",
      "step  969 | train loss: 4.42 | val loss: 4.32 | perplexity: 75.13 | lr: 9.50e-04 | norm: 0.5109 | dt: 13614.1672ms | tok/sec: 28.8829\n",
      "step  970 | train loss: 4.37 | val loss: 4.32 | perplexity: 75.13 | lr: 9.50e-04 | norm: 0.4276 | dt: 13657.3446ms | tok/sec: 28.7915\n",
      "step  971 | train loss: 4.35 | val loss: 4.31 | perplexity: 74.69 | lr: 9.50e-04 | norm: 0.4203 | dt: 13610.1313ms | tok/sec: 28.8914\n",
      "step  972 | train loss: 4.37 | val loss: 4.30 | perplexity: 73.89 | lr: 9.49e-04 | norm: 0.3853 | dt: 13621.6679ms | tok/sec: 28.8669\n",
      "step  973 | train loss: 5.06 | val loss: 4.31 | perplexity: 74.59 | lr: 9.49e-04 | norm: 0.5420 | dt: 13678.8547ms | tok/sec: 28.7463\n",
      "step  974 | train loss: 4.68 | val loss: 4.32 | perplexity: 74.99 | lr: 9.49e-04 | norm: 0.4312 | dt: 13646.3041ms | tok/sec: 28.8148\n",
      "step  975 | train loss: 4.53 | val loss: 4.30 | perplexity: 73.84 | lr: 9.48e-04 | norm: 0.5056 | dt: 13595.1934ms | tok/sec: 28.9232\n",
      "step  976 | train loss: 4.53 | val loss: 4.31 | perplexity: 74.14 | lr: 9.48e-04 | norm: 0.4582 | dt: 13660.6610ms | tok/sec: 28.7846\n",
      "step  977 | train loss: 4.22 | val loss: 4.29 | perplexity: 73.25 | lr: 9.47e-04 | norm: 0.4429 | dt: 13650.7008ms | tok/sec: 28.8056\n",
      "step  978 | train loss: 4.60 | val loss: 4.29 | perplexity: 72.86 | lr: 9.47e-04 | norm: 0.4843 | dt: 13809.1278ms | tok/sec: 28.4751\n",
      "step  979 | train loss: 4.38 | val loss: 4.30 | perplexity: 73.40 | lr: 9.47e-04 | norm: 0.5668 | dt: 13647.9521ms | tok/sec: 28.8114\n",
      "step  980 | train loss: 4.34 | val loss: 4.30 | perplexity: 73.96 | lr: 9.46e-04 | norm: 0.4443 | dt: 13709.5034ms | tok/sec: 28.6820\n",
      "step  981 | train loss: 4.39 | val loss: 4.31 | perplexity: 74.56 | lr: 9.46e-04 | norm: 0.4724 | dt: 13764.5688ms | tok/sec: 28.5673\n",
      "step  982 | train loss: 4.38 | val loss: 4.31 | perplexity: 74.12 | lr: 9.45e-04 | norm: 0.3960 | dt: 13598.4471ms | tok/sec: 28.9162\n",
      "step  983 | train loss: 4.24 | val loss: 4.30 | perplexity: 73.53 | lr: 9.45e-04 | norm: 0.4859 | dt: 13613.0378ms | tok/sec: 28.8852\n",
      "step  984 | train loss: 4.44 | val loss: 4.29 | perplexity: 73.01 | lr: 9.45e-04 | norm: 0.3890 | dt: 13593.3545ms | tok/sec: 28.9271\n",
      "step  985 | train loss: 4.60 | val loss: 4.28 | perplexity: 72.53 | lr: 9.44e-04 | norm: 0.4079 | dt: 13653.0621ms | tok/sec: 28.8006\n",
      "step  986 | train loss: 4.10 | val loss: 4.28 | perplexity: 72.30 | lr: 9.44e-04 | norm: 0.5492 | dt: 13604.7740ms | tok/sec: 28.9028\n",
      "step  987 | train loss: 4.38 | val loss: 4.28 | perplexity: 72.41 | lr: 9.43e-04 | norm: 0.5414 | dt: 13651.3221ms | tok/sec: 28.8042\n",
      "step  988 | train loss: 4.59 | val loss: 4.29 | perplexity: 73.11 | lr: 9.43e-04 | norm: 0.4960 | dt: 13657.7611ms | tok/sec: 28.7907\n",
      "step  989 | train loss: 4.39 | val loss: 4.30 | perplexity: 73.48 | lr: 9.43e-04 | norm: 0.4522 | dt: 13649.8930ms | tok/sec: 28.8073\n",
      "step  990 | train loss: 4.75 | val loss: 4.30 | perplexity: 73.66 | lr: 9.42e-04 | norm: 0.4938 | dt: 13642.6473ms | tok/sec: 28.8226\n",
      "step  991 | train loss: 4.39 | val loss: 4.29 | perplexity: 73.22 | lr: 9.42e-04 | norm: 0.4772 | dt: 13645.9494ms | tok/sec: 28.8156\n",
      "step  992 | train loss: 4.41 | val loss: 4.30 | perplexity: 73.42 | lr: 9.41e-04 | norm: 0.4376 | dt: 13609.0033ms | tok/sec: 28.8938\n",
      "step  993 | train loss: 4.47 | val loss: 4.31 | perplexity: 74.51 | lr: 9.41e-04 | norm: 0.4757 | dt: 13620.3737ms | tok/sec: 28.8697\n",
      "step  994 | train loss: 4.59 | val loss: 4.30 | perplexity: 73.38 | lr: 9.40e-04 | norm: 0.5575 | dt: 13759.5017ms | tok/sec: 28.5778\n",
      "step  995 | train loss: 4.80 | val loss: 4.31 | perplexity: 74.07 | lr: 9.40e-04 | norm: 0.5913 | dt: 13602.1631ms | tok/sec: 28.9083\n",
      "step  996 | train loss: 4.35 | val loss: 4.30 | perplexity: 73.36 | lr: 9.40e-04 | norm: 0.5277 | dt: 13625.0520ms | tok/sec: 28.8598\n",
      "step  997 | train loss: 4.30 | val loss: 4.30 | perplexity: 73.59 | lr: 9.39e-04 | norm: 0.4601 | dt: 13594.9526ms | tok/sec: 28.9237\n",
      "step  998 | train loss: 4.32 | val loss: 4.31 | perplexity: 74.80 | lr: 9.39e-04 | norm: 0.5125 | dt: 13608.2964ms | tok/sec: 28.8953\n",
      "step  999 | train loss: 4.42 | val loss: 4.29 | perplexity: 73.00 | lr: 9.38e-04 | norm: 0.7428 | dt: 13593.2152ms | tok/sec: 28.9274\n",
      "step 1000 | train loss: 4.42 | val loss: 4.29 | perplexity: 72.92 | lr: 9.38e-04 | norm: 0.6442 | dt: 13615.3128ms | tok/sec: 28.8804\n",
      "step 1001 | train loss: 4.65 | val loss: 4.28 | perplexity: 72.51 | lr: 9.38e-04 | norm: 0.5934 | dt: 13597.5769ms | tok/sec: 28.9181\n",
      "step 1002 | train loss: 4.49 | val loss: 4.30 | perplexity: 73.99 | lr: 9.37e-04 | norm: 0.5002 | dt: 13620.8260ms | tok/sec: 28.8687\n",
      "step 1003 | train loss: 4.73 | val loss: 4.30 | perplexity: 73.51 | lr: 9.37e-04 | norm: 0.6037 | dt: 13622.8578ms | tok/sec: 28.8644\n",
      "step 1004 | train loss: 4.82 | val loss: 4.30 | perplexity: 73.46 | lr: 9.36e-04 | norm: 0.5729 | dt: 13666.4796ms | tok/sec: 28.7723\n",
      "step 1005 | train loss: 4.36 | val loss: 4.29 | perplexity: 73.28 | lr: 9.36e-04 | norm: 0.5342 | dt: 13687.9399ms | tok/sec: 28.7272\n",
      "step 1006 | train loss: 4.09 | val loss: 4.29 | perplexity: 73.08 | lr: 9.35e-04 | norm: 0.4009 | dt: 13639.0126ms | tok/sec: 28.8302\n",
      "step 1007 | train loss: 4.15 | val loss: 4.28 | perplexity: 72.30 | lr: 9.35e-04 | norm: 0.4574 | dt: 13746.5644ms | tok/sec: 28.6047\n",
      "step 1008 | train loss: 4.59 | val loss: 4.28 | perplexity: 72.20 | lr: 9.35e-04 | norm: 0.5060 | dt: 13586.3903ms | tok/sec: 28.9419\n",
      "step 1009 | train loss: 4.48 | val loss: 4.28 | perplexity: 72.36 | lr: 9.34e-04 | norm: 0.4274 | dt: 13586.1003ms | tok/sec: 28.9425\n",
      "step 1010 | train loss: 4.25 | val loss: 4.27 | perplexity: 71.74 | lr: 9.34e-04 | norm: 0.4348 | dt: 13616.7476ms | tok/sec: 28.8774\n",
      "step 1011 | train loss: 4.42 | val loss: 4.28 | perplexity: 72.12 | lr: 9.33e-04 | norm: 0.4032 | dt: 13728.6386ms | tok/sec: 28.6420\n",
      "step 1012 | train loss: 4.42 | val loss: 4.28 | perplexity: 71.99 | lr: 9.33e-04 | norm: 0.4818 | dt: 13612.2985ms | tok/sec: 28.8868\n",
      "step 1013 | train loss: 4.22 | val loss: 4.27 | perplexity: 71.45 | lr: 9.32e-04 | norm: 0.4551 | dt: 13641.6347ms | tok/sec: 28.8247\n",
      "step 1014 | train loss: 4.26 | val loss: 4.27 | perplexity: 71.72 | lr: 9.32e-04 | norm: 0.4021 | dt: 13748.6131ms | tok/sec: 28.6004\n",
      "step 1015 | train loss: 4.43 | val loss: 4.27 | perplexity: 71.82 | lr: 9.31e-04 | norm: 0.4020 | dt: 13638.1819ms | tok/sec: 28.8320\n",
      "step 1016 | train loss: 4.54 | val loss: 4.26 | perplexity: 71.00 | lr: 9.31e-04 | norm: 0.4060 | dt: 13628.3374ms | tok/sec: 28.8528\n",
      "step 1017 | train loss: 4.45 | val loss: 4.25 | perplexity: 70.35 | lr: 9.31e-04 | norm: 0.4147 | dt: 13615.7355ms | tok/sec: 28.8795\n",
      "step 1018 | train loss: 4.39 | val loss: 4.25 | perplexity: 70.35 | lr: 9.30e-04 | norm: 0.4607 | dt: 13612.3919ms | tok/sec: 28.8866\n",
      "step 1019 | train loss: 4.87 | val loss: 4.26 | perplexity: 71.15 | lr: 9.30e-04 | norm: 0.9427 | dt: 13649.5149ms | tok/sec: 28.8081\n",
      "step 1020 | train loss: 4.33 | val loss: 4.27 | perplexity: 71.55 | lr: 9.29e-04 | norm: 0.5401 | dt: 13636.7016ms | tok/sec: 28.8351\n",
      "step 1021 | train loss: 4.58 | val loss: 4.26 | perplexity: 70.62 | lr: 9.29e-04 | norm: 0.4896 | dt: 13693.6986ms | tok/sec: 28.7151\n",
      "step 1022 | train loss: 4.63 | val loss: 4.26 | perplexity: 71.02 | lr: 9.28e-04 | norm: 0.4454 | dt: 13634.9339ms | tok/sec: 28.8389\n",
      "step 1023 | train loss: 4.69 | val loss: 4.26 | perplexity: 70.46 | lr: 9.28e-04 | norm: 0.4500 | dt: 13588.8379ms | tok/sec: 28.9367\n",
      "step 1024 | train loss: 4.45 | val loss: 4.26 | perplexity: 70.58 | lr: 9.27e-04 | norm: 0.5128 | dt: 13662.8203ms | tok/sec: 28.7800\n",
      "step 1025 | train loss: 4.45 | val loss: 4.27 | perplexity: 71.68 | lr: 9.27e-04 | norm: 0.3746 | dt: 13622.8142ms | tok/sec: 28.8645\n",
      "step 1026 | train loss: 4.30 | val loss: 4.26 | perplexity: 71.03 | lr: 9.26e-04 | norm: 0.4716 | dt: 13610.6694ms | tok/sec: 28.8903\n",
      "step 1027 | train loss: 4.37 | val loss: 4.26 | perplexity: 71.11 | lr: 9.26e-04 | norm: 0.3930 | dt: 13686.8808ms | tok/sec: 28.7294\n",
      "step 1028 | train loss: 4.43 | val loss: 4.26 | perplexity: 70.47 | lr: 9.26e-04 | norm: 0.5013 | dt: 13671.5086ms | tok/sec: 28.7617\n",
      "step 1029 | train loss: 4.70 | val loss: 4.26 | perplexity: 70.94 | lr: 9.25e-04 | norm: 0.4573 | dt: 13681.2105ms | tok/sec: 28.7413\n",
      "step 1030 | train loss: 4.66 | val loss: 4.27 | perplexity: 71.29 | lr: 9.25e-04 | norm: 0.4379 | dt: 13690.4478ms | tok/sec: 28.7219\n",
      "step 1031 | train loss: 4.44 | val loss: 4.26 | perplexity: 70.93 | lr: 9.24e-04 | norm: 0.4919 | dt: 13666.9827ms | tok/sec: 28.7712\n",
      "step 1032 | train loss: 4.63 | val loss: 4.25 | perplexity: 70.31 | lr: 9.24e-04 | norm: 0.4356 | dt: 13643.7106ms | tok/sec: 28.8203\n",
      "step 1033 | train loss: 4.24 | val loss: 4.25 | perplexity: 70.17 | lr: 9.23e-04 | norm: 0.4912 | dt: 13734.0534ms | tok/sec: 28.6307\n",
      "step 1034 | train loss: 4.61 | val loss: 4.25 | perplexity: 70.13 | lr: 9.23e-04 | norm: 0.4178 | dt: 13709.8320ms | tok/sec: 28.6813\n",
      "step 1035 | train loss: 4.67 | val loss: 4.24 | perplexity: 69.74 | lr: 9.22e-04 | norm: 0.4625 | dt: 13614.2094ms | tok/sec: 28.8828\n",
      "step 1036 | train loss: 4.35 | val loss: 4.25 | perplexity: 70.00 | lr: 9.22e-04 | norm: 0.4317 | dt: 13638.9537ms | tok/sec: 28.8304\n",
      "step 1037 | train loss: 4.80 | val loss: 4.25 | perplexity: 69.86 | lr: 9.21e-04 | norm: 0.4438 | dt: 13667.3660ms | tok/sec: 28.7704\n",
      "step 1038 | train loss: 4.67 | val loss: 4.25 | perplexity: 70.30 | lr: 9.21e-04 | norm: 0.4134 | dt: 13674.7651ms | tok/sec: 28.7549\n",
      "step 1039 | train loss: 4.40 | val loss: 4.25 | perplexity: 69.85 | lr: 9.20e-04 | norm: 0.4533 | dt: 13781.5294ms | tok/sec: 28.5321\n",
      "step 1040 | train loss: 4.38 | val loss: 4.24 | perplexity: 69.32 | lr: 9.20e-04 | norm: 0.3592 | dt: 13669.7166ms | tok/sec: 28.7655\n",
      "step 1041 | train loss: 4.36 | val loss: 4.23 | perplexity: 68.80 | lr: 9.19e-04 | norm: 0.3961 | dt: 13657.6505ms | tok/sec: 28.7909\n",
      "step 1042 | train loss: 4.35 | val loss: 4.23 | perplexity: 68.58 | lr: 9.19e-04 | norm: 0.4061 | dt: 13708.2980ms | tok/sec: 28.6845\n",
      "step 1043 | train loss: 4.42 | val loss: 4.23 | perplexity: 68.97 | lr: 9.18e-04 | norm: 0.4404 | dt: 13654.3188ms | tok/sec: 28.7979\n",
      "step 1044 | train loss: 4.56 | val loss: 4.23 | perplexity: 68.98 | lr: 9.18e-04 | norm: 0.4707 | dt: 13646.4663ms | tok/sec: 28.8145\n",
      "step 1045 | train loss: 4.90 | val loss: 4.25 | perplexity: 70.36 | lr: 9.17e-04 | norm: 0.4808 | dt: 13650.4688ms | tok/sec: 28.8060\n",
      "step 1046 | train loss: 4.33 | val loss: 4.24 | perplexity: 69.55 | lr: 9.17e-04 | norm: 0.5448 | dt: 13739.3098ms | tok/sec: 28.6198\n",
      "step 1047 | train loss: 4.64 | val loss: 4.24 | perplexity: 69.45 | lr: 9.17e-04 | norm: 0.5099 | dt: 13632.8924ms | tok/sec: 28.8432\n",
      "step 1048 | train loss: 4.57 | val loss: 4.24 | perplexity: 69.63 | lr: 9.16e-04 | norm: 0.4824 | dt: 13634.1565ms | tok/sec: 28.8405\n",
      "step 1049 | train loss: 4.31 | val loss: 4.23 | perplexity: 68.97 | lr: 9.16e-04 | norm: 0.4525 | dt: 13637.2223ms | tok/sec: 28.8340\n",
      "step 1050 | train loss: 4.40 | val loss: 4.23 | perplexity: 68.81 | lr: 9.15e-04 | norm: 0.5046 | dt: 13644.9318ms | tok/sec: 28.8177\n",
      "step 1051 | train loss: 4.27 | val loss: 4.23 | perplexity: 68.92 | lr: 9.15e-04 | norm: 0.4812 | dt: 13692.4918ms | tok/sec: 28.7176\n",
      "step 1052 | train loss: 4.21 | val loss: 4.23 | perplexity: 68.86 | lr: 9.14e-04 | norm: 0.4678 | dt: 13653.9948ms | tok/sec: 28.7986\n",
      "step 1053 | train loss: 4.53 | val loss: 4.23 | perplexity: 68.89 | lr: 9.14e-04 | norm: 0.4661 | dt: 13651.1848ms | tok/sec: 28.8045\n",
      "step 1054 | train loss: 4.29 | val loss: 4.23 | perplexity: 68.75 | lr: 9.13e-04 | norm: 0.4443 | dt: 13677.1905ms | tok/sec: 28.7498\n",
      "step 1055 | train loss: 4.30 | val loss: 4.23 | perplexity: 68.76 | lr: 9.13e-04 | norm: 0.3774 | dt: 13679.5356ms | tok/sec: 28.7448\n",
      "step 1056 | train loss: 4.25 | val loss: 4.22 | perplexity: 67.89 | lr: 9.12e-04 | norm: 0.4597 | dt: 13632.0240ms | tok/sec: 28.8450\n",
      "step 1057 | train loss: 4.38 | val loss: 4.22 | perplexity: 67.83 | lr: 9.12e-04 | norm: 0.3756 | dt: 13831.0606ms | tok/sec: 28.4299\n",
      "step 1058 | train loss: 4.32 | val loss: 4.21 | perplexity: 67.30 | lr: 9.11e-04 | norm: 0.3977 | dt: 13632.1287ms | tok/sec: 28.8448\n",
      "step 1059 | train loss: 4.40 | val loss: 4.20 | perplexity: 66.84 | lr: 9.11e-04 | norm: 0.5402 | dt: 13629.3848ms | tok/sec: 28.8506\n",
      "step 1060 | train loss: 4.20 | val loss: 4.21 | perplexity: 67.24 | lr: 9.10e-04 | norm: 0.4214 | dt: 13606.2741ms | tok/sec: 28.8996\n",
      "step 1061 | train loss: 4.46 | val loss: 4.21 | perplexity: 67.38 | lr: 9.10e-04 | norm: 0.5302 | dt: 13646.0927ms | tok/sec: 28.8153\n",
      "step 1062 | train loss: 4.24 | val loss: 4.21 | perplexity: 67.27 | lr: 9.09e-04 | norm: 0.4837 | dt: 13694.8009ms | tok/sec: 28.7128\n",
      "step 1063 | train loss: 4.36 | val loss: 4.20 | perplexity: 66.80 | lr: 9.09e-04 | norm: 0.4766 | dt: 13670.7263ms | tok/sec: 28.7634\n",
      "step 1064 | train loss: 4.53 | val loss: 4.20 | perplexity: 66.76 | lr: 9.08e-04 | norm: 0.5096 | dt: 13712.3992ms | tok/sec: 28.6759\n",
      "step 1065 | train loss: 4.31 | val loss: 4.23 | perplexity: 68.39 | lr: 9.08e-04 | norm: 0.5370 | dt: 13639.6327ms | tok/sec: 28.8289\n",
      "step 1066 | train loss: 4.48 | val loss: 4.22 | perplexity: 67.88 | lr: 9.07e-04 | norm: 0.6013 | dt: 13672.0264ms | tok/sec: 28.7606\n",
      "step 1067 | train loss: 4.41 | val loss: 4.21 | perplexity: 67.03 | lr: 9.07e-04 | norm: 0.4955 | dt: 13641.9957ms | tok/sec: 28.8239\n",
      "step 1068 | train loss: 4.60 | val loss: 4.21 | perplexity: 67.35 | lr: 9.06e-04 | norm: 0.8389 | dt: 13651.6364ms | tok/sec: 28.8036\n",
      "step 1069 | train loss: 4.20 | val loss: 4.20 | perplexity: 66.82 | lr: 9.05e-04 | norm: 0.5613 | dt: 13634.6917ms | tok/sec: 28.8394\n",
      "step 1070 | train loss: 4.84 | val loss: 4.24 | perplexity: 69.54 | lr: 9.05e-04 | norm: 0.5393 | dt: 13714.8597ms | tok/sec: 28.6708\n",
      "step 1071 | train loss: 4.55 | val loss: 4.23 | perplexity: 68.89 | lr: 9.04e-04 | norm: 0.5534 | dt: 13654.4218ms | tok/sec: 28.7977\n",
      "step 1072 | train loss: 4.33 | val loss: 4.24 | perplexity: 69.10 | lr: 9.04e-04 | norm: 0.7626 | dt: 13758.7361ms | tok/sec: 28.5794\n",
      "step 1073 | train loss: 4.08 | val loss: 4.22 | perplexity: 67.99 | lr: 9.03e-04 | norm: 0.6699 | dt: 13640.8441ms | tok/sec: 28.8264\n",
      "step 1074 | train loss: 4.53 | val loss: 4.22 | perplexity: 67.98 | lr: 9.03e-04 | norm: 0.5353 | dt: 13652.9350ms | tok/sec: 28.8008\n",
      "step 1075 | train loss: 4.28 | val loss: 4.21 | perplexity: 67.34 | lr: 9.02e-04 | norm: 0.5819 | dt: 13593.1332ms | tok/sec: 28.9275\n",
      "step 1076 | train loss: 4.49 | val loss: 4.23 | perplexity: 68.67 | lr: 9.02e-04 | norm: 0.4837 | dt: 13647.6059ms | tok/sec: 28.8121\n",
      "step 1077 | train loss: 4.14 | val loss: 4.22 | perplexity: 68.10 | lr: 9.01e-04 | norm: 0.6205 | dt: 13640.7199ms | tok/sec: 28.8266\n",
      "step 1078 | train loss: 4.32 | val loss: 4.22 | perplexity: 68.35 | lr: 9.01e-04 | norm: 0.5137 | dt: 13774.4734ms | tok/sec: 28.5467\n",
      "step 1079 | train loss: 4.34 | val loss: 4.22 | perplexity: 68.17 | lr: 9.00e-04 | norm: 0.4951 | dt: 14454.4456ms | tok/sec: 27.2038\n",
      "step 1080 | train loss: 4.34 | val loss: 4.22 | perplexity: 67.77 | lr: 9.00e-04 | norm: 0.5600 | dt: 14239.6131ms | tok/sec: 27.6142\n",
      "step 1081 | train loss: 4.40 | val loss: 4.23 | perplexity: 68.72 | lr: 8.99e-04 | norm: 0.4201 | dt: 14214.4046ms | tok/sec: 27.6632\n",
      "step 1082 | train loss: 4.53 | val loss: 4.21 | perplexity: 67.50 | lr: 8.99e-04 | norm: 0.5729 | dt: 14222.1584ms | tok/sec: 27.6481\n",
      "step 1083 | train loss: 4.46 | val loss: 4.21 | perplexity: 67.60 | lr: 8.98e-04 | norm: 0.4559 | dt: 14205.6906ms | tok/sec: 27.6802\n",
      "step 1084 | train loss: 4.19 | val loss: 4.21 | perplexity: 67.08 | lr: 8.98e-04 | norm: 0.5903 | dt: 14208.9059ms | tok/sec: 27.6739\n",
      "step 1085 | train loss: 4.43 | val loss: 4.21 | perplexity: 67.11 | lr: 8.97e-04 | norm: 0.4258 | dt: 14264.9479ms | tok/sec: 27.5652\n",
      "step 1086 | train loss: 4.15 | val loss: 4.20 | perplexity: 67.01 | lr: 8.97e-04 | norm: 0.4807 | dt: 14341.7888ms | tok/sec: 27.4175\n",
      "step 1087 | train loss: 4.32 | val loss: 4.19 | perplexity: 66.03 | lr: 8.96e-04 | norm: 0.5031 | dt: 14197.7463ms | tok/sec: 27.6957\n",
      "step 1088 | train loss: 4.59 | val loss: 4.19 | perplexity: 66.06 | lr: 8.95e-04 | norm: 0.4614 | dt: 14173.2435ms | tok/sec: 27.7435\n",
      "step 1089 | train loss: 4.57 | val loss: 4.20 | perplexity: 66.78 | lr: 8.95e-04 | norm: 0.4793 | dt: 14287.4658ms | tok/sec: 27.5217\n",
      "step 1090 | train loss: 4.15 | val loss: 4.20 | perplexity: 66.42 | lr: 8.94e-04 | norm: 0.4653 | dt: 14209.0197ms | tok/sec: 27.6737\n",
      "step 1091 | train loss: 4.32 | val loss: 4.18 | perplexity: 65.19 | lr: 8.94e-04 | norm: 0.4290 | dt: 14201.3531ms | tok/sec: 27.6886\n",
      "step 1092 | train loss: 4.45 | val loss: 4.18 | perplexity: 65.40 | lr: 8.93e-04 | norm: 0.4741 | dt: 14272.9166ms | tok/sec: 27.5498\n",
      "step 1093 | train loss: 4.39 | val loss: 4.18 | perplexity: 65.34 | lr: 8.93e-04 | norm: 0.4893 | dt: 14192.1165ms | tok/sec: 27.7066\n",
      "step 1094 | train loss: 4.58 | val loss: 4.18 | perplexity: 65.57 | lr: 8.92e-04 | norm: 0.5457 | dt: 14193.5277ms | tok/sec: 27.7039\n",
      "step 1095 | train loss: 4.17 | val loss: 4.19 | perplexity: 66.17 | lr: 8.92e-04 | norm: 0.5466 | dt: 14186.1513ms | tok/sec: 27.7183\n",
      "step 1096 | train loss: 4.31 | val loss: 4.19 | perplexity: 66.06 | lr: 8.91e-04 | norm: 0.5706 | dt: 14208.5879ms | tok/sec: 27.6745\n",
      "step 1097 | train loss: 4.28 | val loss: 4.19 | perplexity: 65.93 | lr: 8.91e-04 | norm: 0.4472 | dt: 14203.1448ms | tok/sec: 27.6851\n",
      "step 1098 | train loss: 4.52 | val loss: 4.19 | perplexity: 66.13 | lr: 8.90e-04 | norm: 0.4007 | dt: 14216.8946ms | tok/sec: 27.6584\n",
      "step 1099 | train loss: 4.37 | val loss: 4.19 | perplexity: 65.86 | lr: 8.90e-04 | norm: 0.4436 | dt: 14191.1755ms | tok/sec: 27.7085\n",
      "step 1100 | train loss: 4.38 | val loss: 4.18 | perplexity: 65.66 | lr: 8.89e-04 | norm: 0.3978 | dt: 14216.0563ms | tok/sec: 27.6600\n",
      "step 1101 | train loss: 4.29 | val loss: 4.19 | perplexity: 66.16 | lr: 8.88e-04 | norm: 0.5207 | dt: 14195.4362ms | tok/sec: 27.7002\n",
      "step 1102 | train loss: 4.66 | val loss: 4.20 | perplexity: 66.67 | lr: 8.88e-04 | norm: 0.5151 | dt: 14189.1837ms | tok/sec: 27.7124\n",
      "step 1103 | train loss: 4.32 | val loss: 4.18 | perplexity: 65.05 | lr: 8.87e-04 | norm: 0.5575 | dt: 14219.4660ms | tok/sec: 27.6534\n",
      "step 1104 | train loss: 4.36 | val loss: 4.17 | perplexity: 64.83 | lr: 8.87e-04 | norm: 0.4957 | dt: 14223.3863ms | tok/sec: 27.6457\n",
      "step 1105 | train loss: 4.17 | val loss: 4.17 | perplexity: 64.70 | lr: 8.86e-04 | norm: 0.4539 | dt: 14257.7679ms | tok/sec: 27.5791\n",
      "step 1106 | train loss: 4.53 | val loss: 4.17 | perplexity: 64.98 | lr: 8.86e-04 | norm: 0.5165 | dt: 14229.9850ms | tok/sec: 27.6329\n",
      "step 1107 | train loss: 4.02 | val loss: 4.17 | perplexity: 65.02 | lr: 8.85e-04 | norm: 0.5235 | dt: 14210.6090ms | tok/sec: 27.6706\n",
      "step 1108 | train loss: 4.25 | val loss: 4.17 | perplexity: 64.66 | lr: 8.85e-04 | norm: 0.4142 | dt: 14205.3652ms | tok/sec: 27.6808\n",
      "step 1109 | train loss: 4.42 | val loss: 4.17 | perplexity: 64.54 | lr: 8.84e-04 | norm: 0.4346 | dt: 14232.3971ms | tok/sec: 27.6282\n",
      "step 1110 | train loss: 4.30 | val loss: 4.17 | perplexity: 64.49 | lr: 8.83e-04 | norm: 0.4729 | dt: 14216.1393ms | tok/sec: 27.6598\n",
      "step 1111 | train loss: 4.01 | val loss: 4.17 | perplexity: 64.45 | lr: 8.83e-04 | norm: 0.4173 | dt: 14206.2616ms | tok/sec: 27.6791\n",
      "step 1112 | train loss: 4.45 | val loss: 4.17 | perplexity: 64.56 | lr: 8.82e-04 | norm: 0.4645 | dt: 14204.2139ms | tok/sec: 27.6831\n",
      "step 1113 | train loss: 4.69 | val loss: 4.18 | perplexity: 65.05 | lr: 8.82e-04 | norm: 0.4848 | dt: 14223.6972ms | tok/sec: 27.6451\n",
      "step 1114 | train loss: 4.69 | val loss: 4.17 | perplexity: 64.74 | lr: 8.81e-04 | norm: 0.5504 | dt: 14244.7739ms | tok/sec: 27.6042\n",
      "step 1115 | train loss: 4.46 | val loss: 4.16 | perplexity: 64.19 | lr: 8.81e-04 | norm: 0.5644 | dt: 14251.0242ms | tok/sec: 27.5921\n",
      "step 1116 | train loss: 4.71 | val loss: 4.18 | perplexity: 65.32 | lr: 8.80e-04 | norm: 0.5266 | dt: 14258.1861ms | tok/sec: 27.5783\n",
      "step 1117 | train loss: 4.16 | val loss: 4.17 | perplexity: 64.43 | lr: 8.79e-04 | norm: 0.5233 | dt: 14207.2947ms | tok/sec: 27.6770\n",
      "step 1118 | train loss: 4.13 | val loss: 4.16 | perplexity: 64.15 | lr: 8.79e-04 | norm: 0.4491 | dt: 14207.6454ms | tok/sec: 27.6764\n",
      "step 1119 | train loss: 4.47 | val loss: 4.16 | perplexity: 64.26 | lr: 8.78e-04 | norm: 0.4593 | dt: 14276.0880ms | tok/sec: 27.5437\n",
      "step 1120 | train loss: 4.32 | val loss: 4.16 | perplexity: 63.80 | lr: 8.78e-04 | norm: 0.4064 | dt: 14245.7497ms | tok/sec: 27.6023\n",
      "step 1121 | train loss: 4.55 | val loss: 4.16 | perplexity: 63.87 | lr: 8.77e-04 | norm: 0.4632 | dt: 14268.1968ms | tok/sec: 27.5589\n",
      "step 1122 | train loss: 4.05 | val loss: 4.15 | perplexity: 63.32 | lr: 8.77e-04 | norm: 0.5033 | dt: 14192.3282ms | tok/sec: 27.7062\n",
      "step 1123 | train loss: 4.51 | val loss: 4.14 | perplexity: 62.96 | lr: 8.76e-04 | norm: 0.5710 | dt: 14205.4517ms | tok/sec: 27.6806\n",
      "step 1124 | train loss: 4.17 | val loss: 4.16 | perplexity: 63.90 | lr: 8.75e-04 | norm: 0.7297 | dt: 14216.4950ms | tok/sec: 27.6591\n",
      "step 1125 | train loss: 4.53 | val loss: 4.16 | perplexity: 64.15 | lr: 8.75e-04 | norm: 0.5844 | dt: 14224.9990ms | tok/sec: 27.6426\n",
      "step 1126 | train loss: 4.23 | val loss: 4.15 | perplexity: 63.54 | lr: 8.74e-04 | norm: 0.5901 | dt: 14216.3637ms | tok/sec: 27.6594\n",
      "step 1127 | train loss: 4.19 | val loss: 4.15 | perplexity: 63.50 | lr: 8.74e-04 | norm: 0.5118 | dt: 14188.8099ms | tok/sec: 27.7131\n",
      "step 1128 | train loss: 4.31 | val loss: 4.16 | perplexity: 63.79 | lr: 8.73e-04 | norm: 0.5157 | dt: 14172.8230ms | tok/sec: 27.7444\n",
      "step 1129 | train loss: 4.14 | val loss: 4.17 | perplexity: 64.74 | lr: 8.72e-04 | norm: 0.5034 | dt: 14181.7865ms | tok/sec: 27.7268\n",
      "step 1130 | train loss: 4.45 | val loss: 4.17 | perplexity: 64.81 | lr: 8.72e-04 | norm: 0.5147 | dt: 14173.5334ms | tok/sec: 27.7430\n",
      "step 1131 | train loss: 4.47 | val loss: 4.16 | perplexity: 64.07 | lr: 8.71e-04 | norm: 0.5414 | dt: 14190.4178ms | tok/sec: 27.7100\n",
      "step 1132 | train loss: 4.42 | val loss: 4.16 | perplexity: 64.15 | lr: 8.71e-04 | norm: 0.5935 | dt: 14236.2211ms | tok/sec: 27.6208\n",
      "step 1133 | train loss: 4.45 | val loss: 4.16 | perplexity: 64.29 | lr: 8.70e-04 | norm: 0.4631 | dt: 14201.2410ms | tok/sec: 27.6888\n",
      "step 1134 | train loss: 4.40 | val loss: 4.17 | perplexity: 64.60 | lr: 8.70e-04 | norm: 0.5061 | dt: 14238.0176ms | tok/sec: 27.6173\n",
      "step 1135 | train loss: 4.36 | val loss: 4.17 | perplexity: 64.71 | lr: 8.69e-04 | norm: 0.5443 | dt: 14243.6149ms | tok/sec: 27.6065\n",
      "step 1136 | train loss: 4.37 | val loss: 4.16 | perplexity: 63.91 | lr: 8.68e-04 | norm: 0.4315 | dt: 14252.5110ms | tok/sec: 27.5892\n",
      "step 1137 | train loss: 4.44 | val loss: 4.15 | perplexity: 63.14 | lr: 8.68e-04 | norm: 0.4194 | dt: 14208.8201ms | tok/sec: 27.6741\n",
      "step 1138 | train loss: 4.52 | val loss: 4.14 | perplexity: 62.91 | lr: 8.67e-04 | norm: 0.4380 | dt: 14196.4741ms | tok/sec: 27.6981\n",
      "step 1139 | train loss: 4.36 | val loss: 4.15 | perplexity: 63.35 | lr: 8.67e-04 | norm: 0.5792 | dt: 14181.0341ms | tok/sec: 27.7283\n",
      "step 1140 | train loss: 4.06 | val loss: 4.15 | perplexity: 63.45 | lr: 8.66e-04 | norm: 0.4174 | dt: 14184.7017ms | tok/sec: 27.7211\n",
      "step 1141 | train loss: 4.14 | val loss: 4.14 | perplexity: 62.72 | lr: 8.65e-04 | norm: 0.4659 | dt: 14228.7717ms | tok/sec: 27.6353\n",
      "step 1142 | train loss: 4.18 | val loss: 4.13 | perplexity: 62.48 | lr: 8.65e-04 | norm: 0.3820 | dt: 14179.1453ms | tok/sec: 27.7320\n",
      "step 1143 | train loss: 4.48 | val loss: 4.13 | perplexity: 62.12 | lr: 8.64e-04 | norm: 0.4539 | dt: 14215.7583ms | tok/sec: 27.6606\n",
      "step 1144 | train loss: 4.19 | val loss: 4.13 | perplexity: 62.31 | lr: 8.64e-04 | norm: 0.4492 | dt: 14194.8681ms | tok/sec: 27.7013\n",
      "step 1145 | train loss: 4.43 | val loss: 4.13 | perplexity: 62.11 | lr: 8.63e-04 | norm: 0.4311 | dt: 14232.8775ms | tok/sec: 27.6273\n",
      "step 1146 | train loss: 4.38 | val loss: 4.13 | perplexity: 62.21 | lr: 8.62e-04 | norm: 0.3938 | dt: 14241.4336ms | tok/sec: 27.6107\n",
      "step 1147 | train loss: 4.44 | val loss: 4.12 | perplexity: 61.36 | lr: 8.62e-04 | norm: 0.5066 | dt: 14254.3373ms | tok/sec: 27.5857\n",
      "step 1148 | train loss: 4.04 | val loss: 4.13 | perplexity: 62.01 | lr: 8.61e-04 | norm: 0.4569 | dt: 14203.4531ms | tok/sec: 27.6845\n",
      "step 1149 | train loss: 4.25 | val loss: 4.13 | perplexity: 62.18 | lr: 8.61e-04 | norm: 0.4181 | dt: 14182.8353ms | tok/sec: 27.7248\n",
      "step 1150 | train loss: 4.22 | val loss: 4.13 | perplexity: 62.15 | lr: 8.60e-04 | norm: 0.4494 | dt: 14182.6327ms | tok/sec: 27.7252\n",
      "step 1151 | train loss: 4.48 | val loss: 4.13 | perplexity: 62.27 | lr: 8.59e-04 | norm: 0.4433 | dt: 14130.9962ms | tok/sec: 27.8265\n",
      "step 1152 | train loss: 4.48 | val loss: 4.13 | perplexity: 62.31 | lr: 8.59e-04 | norm: 0.4275 | dt: 14154.7902ms | tok/sec: 27.7797\n",
      "step 1153 | train loss: 4.24 | val loss: 4.12 | perplexity: 61.86 | lr: 8.58e-04 | norm: 0.4280 | dt: 14192.4760ms | tok/sec: 27.7059\n",
      "step 1154 | train loss: 4.56 | val loss: 4.12 | perplexity: 61.86 | lr: 8.57e-04 | norm: 0.4230 | dt: 14216.8806ms | tok/sec: 27.6584\n",
      "step 1155 | train loss: 4.32 | val loss: 4.13 | perplexity: 61.91 | lr: 8.57e-04 | norm: 0.4237 | dt: 14197.1774ms | tok/sec: 27.6968\n",
      "step 1156 | train loss: 4.40 | val loss: 4.12 | perplexity: 61.41 | lr: 8.56e-04 | norm: 0.4457 | dt: 14210.9969ms | tok/sec: 27.6698\n",
      "step 1157 | train loss: 4.50 | val loss: 4.12 | perplexity: 61.67 | lr: 8.56e-04 | norm: 0.5344 | dt: 14219.7006ms | tok/sec: 27.6529\n",
      "step 1158 | train loss: 4.00 | val loss: 4.12 | perplexity: 61.37 | lr: 8.55e-04 | norm: 0.4792 | dt: 14186.6195ms | tok/sec: 27.7174\n",
      "step 1159 | train loss: 4.52 | val loss: 4.11 | perplexity: 61.20 | lr: 8.54e-04 | norm: 0.5233 | dt: 14177.8321ms | tok/sec: 27.7346\n",
      "step 1160 | train loss: 4.16 | val loss: 4.12 | perplexity: 61.61 | lr: 8.54e-04 | norm: 0.4532 | dt: 14154.5811ms | tok/sec: 27.7801\n",
      "step 1161 | train loss: 4.17 | val loss: 4.13 | perplexity: 62.18 | lr: 8.53e-04 | norm: 0.4162 | dt: 14184.1705ms | tok/sec: 27.7222\n",
      "step 1162 | train loss: 4.19 | val loss: 4.14 | perplexity: 62.52 | lr: 8.53e-04 | norm: 0.4298 | dt: 14165.5965ms | tok/sec: 27.7585\n",
      "step 1163 | train loss: 4.17 | val loss: 4.13 | perplexity: 61.88 | lr: 8.52e-04 | norm: 0.4555 | dt: 14187.6569ms | tok/sec: 27.7154\n",
      "step 1164 | train loss: 4.27 | val loss: 4.12 | perplexity: 61.74 | lr: 8.51e-04 | norm: 0.3929 | dt: 14189.7309ms | tok/sec: 27.7113\n",
      "step 1165 | train loss: 4.53 | val loss: 4.13 | perplexity: 62.15 | lr: 8.51e-04 | norm: 0.4339 | dt: 14201.6242ms | tok/sec: 27.6881\n",
      "step 1166 | train loss: 4.22 | val loss: 4.13 | perplexity: 62.43 | lr: 8.50e-04 | norm: 0.5688 | dt: 14183.0711ms | tok/sec: 27.7243\n",
      "step 1167 | train loss: 4.45 | val loss: 4.13 | perplexity: 62.07 | lr: 8.49e-04 | norm: 0.5776 | dt: 14195.4651ms | tok/sec: 27.7001\n",
      "step 1168 | train loss: 4.32 | val loss: 4.12 | perplexity: 61.58 | lr: 8.49e-04 | norm: 0.4653 | dt: 14186.3692ms | tok/sec: 27.7179\n",
      "step 1169 | train loss: 4.08 | val loss: 4.11 | perplexity: 61.05 | lr: 8.48e-04 | norm: 0.4712 | dt: 14210.0837ms | tok/sec: 27.6716\n",
      "step 1170 | train loss: 4.40 | val loss: 4.11 | perplexity: 61.12 | lr: 8.48e-04 | norm: 0.4995 | dt: 14186.1420ms | tok/sec: 27.7183\n",
      "step 1171 | train loss: 4.14 | val loss: 4.13 | perplexity: 62.16 | lr: 8.47e-04 | norm: 0.5177 | dt: 14210.0356ms | tok/sec: 27.6717\n",
      "step 1172 | train loss: 4.40 | val loss: 4.12 | perplexity: 61.41 | lr: 8.46e-04 | norm: 0.5250 | dt: 14203.6583ms | tok/sec: 27.6841\n",
      "step 1173 | train loss: 4.38 | val loss: 4.11 | perplexity: 60.91 | lr: 8.46e-04 | norm: 0.4993 | dt: 14188.8397ms | tok/sec: 27.7130\n",
      "step 1174 | train loss: 4.42 | val loss: 4.13 | perplexity: 62.14 | lr: 8.45e-04 | norm: 0.5693 | dt: 14181.1535ms | tok/sec: 27.7281\n",
      "step 1175 | train loss: 4.56 | val loss: 4.14 | perplexity: 62.91 | lr: 8.44e-04 | norm: 0.5474 | dt: 14191.0481ms | tok/sec: 27.7087\n",
      "step 1176 | train loss: 4.33 | val loss: 4.13 | perplexity: 62.15 | lr: 8.44e-04 | norm: 0.4980 | dt: 14188.2348ms | tok/sec: 27.7142\n",
      "step 1177 | train loss: 4.55 | val loss: 4.12 | perplexity: 61.71 | lr: 8.43e-04 | norm: 0.5329 | dt: 14157.4235ms | tok/sec: 27.7745\n",
      "step 1178 | train loss: 4.24 | val loss: 4.12 | perplexity: 61.38 | lr: 8.42e-04 | norm: 0.5376 | dt: 14262.2168ms | tok/sec: 27.5705\n",
      "step 1179 | train loss: 4.12 | val loss: 4.12 | perplexity: 61.33 | lr: 8.42e-04 | norm: 0.4131 | dt: 14180.9556ms | tok/sec: 27.7285\n",
      "step 1180 | train loss: 4.67 | val loss: 4.12 | perplexity: 61.55 | lr: 8.41e-04 | norm: 0.4194 | dt: 14163.1584ms | tok/sec: 27.7633\n",
      "step 1181 | train loss: 4.39 | val loss: 4.12 | perplexity: 61.37 | lr: 8.41e-04 | norm: 0.4541 | dt: 14176.9519ms | tok/sec: 27.7363\n",
      "step 1182 | train loss: 4.25 | val loss: 4.11 | perplexity: 60.99 | lr: 8.40e-04 | norm: 0.4510 | dt: 14200.8781ms | tok/sec: 27.6896\n",
      "step 1183 | train loss: 4.44 | val loss: 4.11 | perplexity: 60.71 | lr: 8.39e-04 | norm: 0.4256 | dt: 14208.4467ms | tok/sec: 27.6748\n",
      "step 1184 | train loss: 4.32 | val loss: 4.11 | perplexity: 60.81 | lr: 8.39e-04 | norm: 0.4466 | dt: 14198.2083ms | tok/sec: 27.6948\n",
      "step 1185 | train loss: 4.36 | val loss: 4.12 | perplexity: 61.26 | lr: 8.38e-04 | norm: 0.3990 | dt: 14259.7835ms | tok/sec: 27.5752\n",
      "step 1186 | train loss: 4.16 | val loss: 4.12 | perplexity: 61.46 | lr: 8.37e-04 | norm: 0.4369 | dt: 14253.0246ms | tok/sec: 27.5882\n",
      "step 1187 | train loss: 4.40 | val loss: 4.12 | perplexity: 61.30 | lr: 8.37e-04 | norm: 0.3982 | dt: 14262.2504ms | tok/sec: 27.5704\n",
      "step 1188 | train loss: 4.37 | val loss: 4.12 | perplexity: 61.39 | lr: 8.36e-04 | norm: 0.4181 | dt: 14300.4866ms | tok/sec: 27.4967\n",
      "step 1189 | train loss: 4.53 | val loss: 4.12 | perplexity: 61.40 | lr: 8.35e-04 | norm: 0.3860 | dt: 14256.9726ms | tok/sec: 27.5806\n",
      "step 1190 | train loss: 4.23 | val loss: 4.10 | perplexity: 60.34 | lr: 8.35e-04 | norm: 0.4583 | dt: 14232.9984ms | tok/sec: 27.6271\n",
      "step 1191 | train loss: 4.29 | val loss: 4.10 | perplexity: 60.43 | lr: 8.34e-04 | norm: 0.4548 | dt: 14231.7312ms | tok/sec: 27.6295\n",
      "step 1192 | train loss: 4.13 | val loss: 4.10 | perplexity: 60.22 | lr: 8.33e-04 | norm: 0.4711 | dt: 14209.1238ms | tok/sec: 27.6735\n",
      "step 1193 | train loss: 4.22 | val loss: 4.09 | perplexity: 59.86 | lr: 8.33e-04 | norm: 0.3946 | dt: 14229.6190ms | tok/sec: 27.6336\n",
      "step 1194 | train loss: 4.19 | val loss: 4.09 | perplexity: 59.46 | lr: 8.32e-04 | norm: 0.4371 | dt: 14211.4074ms | tok/sec: 27.6690\n",
      "step 1195 | train loss: 4.09 | val loss: 4.08 | perplexity: 59.17 | lr: 8.32e-04 | norm: 0.4134 | dt: 14227.1953ms | tok/sec: 27.6383\n",
      "step 1196 | train loss: 4.34 | val loss: 4.09 | perplexity: 59.87 | lr: 8.31e-04 | norm: 0.5373 | dt: 14209.3828ms | tok/sec: 27.6730\n",
      "step 1197 | train loss: 4.40 | val loss: 4.09 | perplexity: 59.96 | lr: 8.30e-04 | norm: 0.5074 | dt: 14215.5645ms | tok/sec: 27.6609\n",
      "step 1198 | train loss: 4.02 | val loss: 4.10 | perplexity: 60.29 | lr: 8.30e-04 | norm: 0.4195 | dt: 14207.8667ms | tok/sec: 27.6759\n",
      "step 1199 | train loss: 4.30 | val loss: 4.10 | perplexity: 60.22 | lr: 8.29e-04 | norm: 0.4209 | dt: 14205.7521ms | tok/sec: 27.6801\n",
      "step 1200 | train loss: 4.43 | val loss: 4.09 | perplexity: 59.70 | lr: 8.28e-04 | norm: 0.4091 | dt: 14226.9502ms | tok/sec: 27.6388\n",
      "step 1201 | train loss: 4.22 | val loss: 4.09 | perplexity: 59.53 | lr: 8.28e-04 | norm: 0.4822 | dt: 14209.5187ms | tok/sec: 27.6727\n",
      "step 1202 | train loss: 4.34 | val loss: 4.09 | perplexity: 59.52 | lr: 8.27e-04 | norm: 0.4242 | dt: 14185.1492ms | tok/sec: 27.7203\n",
      "step 1203 | train loss: 4.19 | val loss: 4.09 | perplexity: 59.49 | lr: 8.26e-04 | norm: 0.4097 | dt: 14233.0768ms | tok/sec: 27.6269\n",
      "step 1204 | train loss: 4.19 | val loss: 4.09 | perplexity: 59.49 | lr: 8.26e-04 | norm: 0.4452 | dt: 14280.7670ms | tok/sec: 27.5347\n",
      "step 1205 | train loss: 4.46 | val loss: 4.08 | perplexity: 59.40 | lr: 8.25e-04 | norm: 0.3873 | dt: 14233.5279ms | tok/sec: 27.6260\n",
      "step 1206 | train loss: 4.29 | val loss: 4.08 | perplexity: 59.14 | lr: 8.24e-04 | norm: 0.3686 | dt: 14198.3671ms | tok/sec: 27.6945\n",
      "step 1207 | train loss: 4.06 | val loss: 4.07 | perplexity: 58.75 | lr: 8.24e-04 | norm: 0.4080 | dt: 14179.3563ms | tok/sec: 27.7316\n",
      "step 1208 | train loss: 4.19 | val loss: 4.07 | perplexity: 58.62 | lr: 8.23e-04 | norm: 0.4346 | dt: 14189.8124ms | tok/sec: 27.7111\n",
      "step 1209 | train loss: 4.31 | val loss: 4.07 | perplexity: 58.36 | lr: 8.22e-04 | norm: 0.4191 | dt: 14201.8406ms | tok/sec: 27.6877\n",
      "step 1210 | train loss: 4.17 | val loss: 4.07 | perplexity: 58.41 | lr: 8.22e-04 | norm: 0.4130 | dt: 14204.9930ms | tok/sec: 27.6815\n",
      "step 1211 | train loss: 4.25 | val loss: 4.07 | perplexity: 58.59 | lr: 8.21e-04 | norm: 0.3737 | dt: 14196.0506ms | tok/sec: 27.6990\n",
      "step 1212 | train loss: 4.12 | val loss: 4.08 | perplexity: 59.01 | lr: 8.20e-04 | norm: 0.3887 | dt: 14211.6013ms | tok/sec: 27.6687\n",
      "step 1213 | train loss: 4.15 | val loss: 4.08 | perplexity: 58.92 | lr: 8.20e-04 | norm: 0.5267 | dt: 14239.8968ms | tok/sec: 27.6137\n",
      "step 1214 | train loss: 4.07 | val loss: 4.07 | perplexity: 58.65 | lr: 8.19e-04 | norm: 0.4501 | dt: 14197.7956ms | tok/sec: 27.6956\n",
      "step 1215 | train loss: 4.17 | val loss: 4.07 | perplexity: 58.40 | lr: 8.18e-04 | norm: 0.3932 | dt: 14196.8300ms | tok/sec: 27.6975\n",
      "step 1216 | train loss: 4.27 | val loss: 4.06 | perplexity: 58.24 | lr: 8.18e-04 | norm: 0.4076 | dt: 14217.8125ms | tok/sec: 27.6566\n",
      "step 1217 | train loss: 4.39 | val loss: 4.06 | perplexity: 57.99 | lr: 8.17e-04 | norm: 0.3761 | dt: 14247.8535ms | tok/sec: 27.5983\n",
      "step 1218 | train loss: 4.11 | val loss: 4.06 | perplexity: 57.78 | lr: 8.16e-04 | norm: 0.4251 | dt: 14214.8931ms | tok/sec: 27.6623\n",
      "step 1219 | train loss: 4.47 | val loss: 4.07 | perplexity: 58.40 | lr: 8.16e-04 | norm: 0.4643 | dt: 14196.1708ms | tok/sec: 27.6987\n",
      "step 1220 | train loss: 4.21 | val loss: 4.07 | perplexity: 58.47 | lr: 8.15e-04 | norm: 0.5218 | dt: 14216.0332ms | tok/sec: 27.6600\n",
      "step 1221 | train loss: 4.39 | val loss: 4.06 | perplexity: 58.07 | lr: 8.14e-04 | norm: 0.4285 | dt: 14194.9861ms | tok/sec: 27.7010\n",
      "step 1222 | train loss: 3.84 | val loss: 4.06 | perplexity: 57.80 | lr: 8.13e-04 | norm: 0.4347 | dt: 14208.8900ms | tok/sec: 27.6739\n",
      "step 1223 | train loss: 4.42 | val loss: 4.06 | perplexity: 58.18 | lr: 8.13e-04 | norm: 0.4431 | dt: 14235.5280ms | tok/sec: 27.6222\n",
      "step 1224 | train loss: 4.26 | val loss: 4.06 | perplexity: 57.83 | lr: 8.12e-04 | norm: 0.5532 | dt: 14260.8864ms | tok/sec: 27.5730\n",
      "step 1225 | train loss: 4.31 | val loss: 4.06 | perplexity: 57.77 | lr: 8.11e-04 | norm: 0.4208 | dt: 14211.0748ms | tok/sec: 27.6697\n",
      "step 1226 | train loss: 4.27 | val loss: 4.06 | perplexity: 58.06 | lr: 8.11e-04 | norm: 0.4631 | dt: 14246.6254ms | tok/sec: 27.6006\n",
      "step 1227 | train loss: 4.32 | val loss: 4.07 | perplexity: 58.27 | lr: 8.10e-04 | norm: 0.3689 | dt: 14260.4511ms | tok/sec: 27.5739\n",
      "step 1228 | train loss: 4.11 | val loss: 4.06 | perplexity: 58.01 | lr: 8.09e-04 | norm: 0.3777 | dt: 14225.1518ms | tok/sec: 27.6423\n",
      "step 1229 | train loss: 4.42 | val loss: 4.06 | perplexity: 58.06 | lr: 8.09e-04 | norm: 0.3959 | dt: 14214.0350ms | tok/sec: 27.6639\n",
      "step 1230 | train loss: 4.24 | val loss: 4.06 | perplexity: 57.84 | lr: 8.08e-04 | norm: 0.4486 | dt: 14195.1644ms | tok/sec: 27.7007\n",
      "step 1231 | train loss: 4.20 | val loss: 4.05 | perplexity: 57.52 | lr: 8.07e-04 | norm: 0.3870 | dt: 14191.3524ms | tok/sec: 27.7081\n",
      "step 1232 | train loss: 4.28 | val loss: 4.05 | perplexity: 57.65 | lr: 8.07e-04 | norm: 0.6286 | dt: 14216.5620ms | tok/sec: 27.6590\n",
      "step 1233 | train loss: 4.35 | val loss: 4.06 | perplexity: 58.20 | lr: 8.06e-04 | norm: 0.4820 | dt: 14228.8482ms | tok/sec: 27.6351\n",
      "step 1234 | train loss: 4.13 | val loss: 4.06 | perplexity: 58.02 | lr: 8.05e-04 | norm: 0.4596 | dt: 14190.6664ms | tok/sec: 27.7095\n",
      "step 1235 | train loss: 4.06 | val loss: 4.04 | perplexity: 56.85 | lr: 8.05e-04 | norm: 0.5435 | dt: 14310.4827ms | tok/sec: 27.4775\n",
      "step 1236 | train loss: 4.38 | val loss: 4.04 | perplexity: 56.98 | lr: 8.04e-04 | norm: 0.5195 | dt: 14227.5631ms | tok/sec: 27.6376\n",
      "step 1237 | train loss: 3.97 | val loss: 4.04 | perplexity: 56.60 | lr: 8.03e-04 | norm: 0.5165 | dt: 14212.3144ms | tok/sec: 27.6673\n",
      "step 1238 | train loss: 4.16 | val loss: 4.03 | perplexity: 56.33 | lr: 8.02e-04 | norm: 0.4440 | dt: 14218.9877ms | tok/sec: 27.6543\n",
      "step 1239 | train loss: 4.21 | val loss: 4.04 | perplexity: 56.80 | lr: 8.02e-04 | norm: 0.4186 | dt: 14196.6381ms | tok/sec: 27.6978\n",
      "step 1240 | train loss: 4.18 | val loss: 4.03 | perplexity: 56.43 | lr: 8.01e-04 | norm: 0.4044 | dt: 14198.8654ms | tok/sec: 27.6935\n",
      "step 1241 | train loss: 4.13 | val loss: 4.02 | perplexity: 55.91 | lr: 8.00e-04 | norm: 0.4511 | dt: 14229.6636ms | tok/sec: 27.6335\n",
      "step 1242 | train loss: 4.06 | val loss: 4.02 | perplexity: 55.88 | lr: 8.00e-04 | norm: 0.3733 | dt: 14208.0204ms | tok/sec: 27.6756\n",
      "step 1243 | train loss: 4.10 | val loss: 4.03 | perplexity: 56.00 | lr: 7.99e-04 | norm: 0.3631 | dt: 14244.5035ms | tok/sec: 27.6048\n",
      "step 1244 | train loss: 4.27 | val loss: 4.03 | perplexity: 56.03 | lr: 7.98e-04 | norm: 0.3954 | dt: 14230.1018ms | tok/sec: 27.6327\n",
      "step 1245 | train loss: 4.15 | val loss: 4.02 | perplexity: 55.43 | lr: 7.98e-04 | norm: 0.4283 | dt: 14222.4283ms | tok/sec: 27.6476\n",
      "step 1246 | train loss: 3.93 | val loss: 4.01 | perplexity: 55.17 | lr: 7.97e-04 | norm: 0.3905 | dt: 14229.8210ms | tok/sec: 27.6332\n",
      "step 1247 | train loss: 4.01 | val loss: 4.01 | perplexity: 55.00 | lr: 7.96e-04 | norm: 0.4028 | dt: 14220.9451ms | tok/sec: 27.6505\n",
      "step 1248 | train loss: 4.24 | val loss: 4.01 | perplexity: 55.13 | lr: 7.95e-04 | norm: 0.4169 | dt: 14317.8658ms | tok/sec: 27.4633\n",
      "step 1249 | train loss: 4.28 | val loss: 4.01 | perplexity: 55.05 | lr: 7.95e-04 | norm: 0.3615 | dt: 14255.5976ms | tok/sec: 27.5833\n",
      "step 1250 | train loss: 4.58 | val loss: 4.01 | perplexity: 55.37 | lr: 7.94e-04 | norm: 0.3873 | dt: 14255.4035ms | tok/sec: 27.5836\n",
      "step 1251 | train loss: 4.12 | val loss: 4.01 | perplexity: 55.05 | lr: 7.93e-04 | norm: 0.3795 | dt: 14339.4337ms | tok/sec: 27.4220\n",
      "step 1252 | train loss: 3.80 | val loss: 4.02 | perplexity: 55.60 | lr: 7.93e-04 | norm: 0.4880 | dt: 14224.9432ms | tok/sec: 27.6427\n",
      "step 1253 | train loss: 4.25 | val loss: 4.03 | perplexity: 55.99 | lr: 7.92e-04 | norm: 0.4099 | dt: 14226.7897ms | tok/sec: 27.6391\n",
      "step 1254 | train loss: 3.95 | val loss: 4.02 | perplexity: 55.73 | lr: 7.91e-04 | norm: 0.4250 | dt: 14231.6475ms | tok/sec: 27.6297\n",
      "step 1255 | train loss: 3.99 | val loss: 4.01 | perplexity: 55.11 | lr: 7.91e-04 | norm: 0.4126 | dt: 14230.9341ms | tok/sec: 27.6311\n",
      "step 1256 | train loss: 4.25 | val loss: 4.01 | perplexity: 55.32 | lr: 7.90e-04 | norm: 0.4074 | dt: 14230.8302ms | tok/sec: 27.6313\n",
      "step 1257 | train loss: 4.08 | val loss: 4.03 | perplexity: 56.05 | lr: 7.89e-04 | norm: 0.5598 | dt: 14228.2739ms | tok/sec: 27.6362\n",
      "step 1258 | train loss: 4.19 | val loss: 4.05 | perplexity: 57.67 | lr: 7.88e-04 | norm: 0.4309 | dt: 14253.6390ms | tok/sec: 27.5871\n",
      "step 1259 | train loss: 4.21 | val loss: 4.04 | perplexity: 56.64 | lr: 7.88e-04 | norm: 0.6495 | dt: 14230.4330ms | tok/sec: 27.6320\n",
      "step 1260 | train loss: 4.41 | val loss: 4.03 | perplexity: 56.27 | lr: 7.87e-04 | norm: 0.5222 | dt: 14225.4777ms | tok/sec: 27.6417\n",
      "step 1261 | train loss: 4.13 | val loss: 4.02 | perplexity: 55.96 | lr: 7.86e-04 | norm: 0.4342 | dt: 14245.1243ms | tok/sec: 27.6035\n",
      "step 1262 | train loss: 4.03 | val loss: 4.02 | perplexity: 55.63 | lr: 7.86e-04 | norm: 0.4789 | dt: 14242.6882ms | tok/sec: 27.6083\n",
      "step 1263 | train loss: 3.85 | val loss: 4.02 | perplexity: 55.50 | lr: 7.85e-04 | norm: 0.4169 | dt: 14268.6422ms | tok/sec: 27.5581\n",
      "step 1264 | train loss: 4.31 | val loss: 4.02 | perplexity: 55.55 | lr: 7.84e-04 | norm: 0.4459 | dt: 14310.1239ms | tok/sec: 27.4782\n",
      "step 1265 | train loss: 4.16 | val loss: 4.01 | perplexity: 55.30 | lr: 7.83e-04 | norm: 0.4079 | dt: 14247.8173ms | tok/sec: 27.5983\n",
      "step 1266 | train loss: 3.94 | val loss: 4.01 | perplexity: 54.88 | lr: 7.83e-04 | norm: 0.4121 | dt: 14227.0684ms | tok/sec: 27.6386\n",
      "step 1267 | train loss: 3.84 | val loss: 4.01 | perplexity: 55.31 | lr: 7.82e-04 | norm: 0.3726 | dt: 14224.1910ms | tok/sec: 27.6442\n",
      "step 1268 | train loss: 4.05 | val loss: 4.01 | perplexity: 55.20 | lr: 7.81e-04 | norm: 0.4565 | dt: 14250.9985ms | tok/sec: 27.5922\n",
      "step 1269 | train loss: 3.85 | val loss: 4.01 | perplexity: 55.19 | lr: 7.81e-04 | norm: 0.4332 | dt: 14231.0867ms | tok/sec: 27.6308\n",
      "step 1270 | train loss: 4.10 | val loss: 4.01 | perplexity: 55.37 | lr: 7.80e-04 | norm: 0.3875 | dt: 14275.2807ms | tok/sec: 27.5452\n",
      "step 1271 | train loss: 4.28 | val loss: 4.01 | perplexity: 55.37 | lr: 7.79e-04 | norm: 0.4210 | dt: 14301.2893ms | tok/sec: 27.4951\n",
      "step 1272 | train loss: 4.46 | val loss: 4.02 | perplexity: 55.47 | lr: 7.78e-04 | norm: 0.4592 | dt: 14269.8765ms | tok/sec: 27.5557\n",
      "step 1273 | train loss: 3.95 | val loss: 4.02 | perplexity: 55.97 | lr: 7.78e-04 | norm: 0.5177 | dt: 14236.9418ms | tok/sec: 27.6194\n",
      "step 1274 | train loss: 4.33 | val loss: 4.02 | perplexity: 55.84 | lr: 7.77e-04 | norm: 0.5051 | dt: 14263.4239ms | tok/sec: 27.5681\n",
      "step 1275 | train loss: 4.28 | val loss: 4.03 | perplexity: 56.05 | lr: 7.76e-04 | norm: 0.4176 | dt: 14262.1026ms | tok/sec: 27.5707\n",
      "step 1276 | train loss: 4.20 | val loss: 4.03 | perplexity: 56.06 | lr: 7.75e-04 | norm: 0.4248 | dt: 14245.5890ms | tok/sec: 27.6026\n",
      "step 1277 | train loss: 4.43 | val loss: 4.03 | perplexity: 56.42 | lr: 7.75e-04 | norm: 0.3849 | dt: 14244.9250ms | tok/sec: 27.6039\n",
      "step 1278 | train loss: 4.20 | val loss: 4.02 | perplexity: 55.89 | lr: 7.74e-04 | norm: 0.3813 | dt: 14243.6295ms | tok/sec: 27.6064\n",
      "step 1279 | train loss: 4.26 | val loss: 4.02 | perplexity: 55.48 | lr: 7.73e-04 | norm: 0.3485 | dt: 14238.3792ms | tok/sec: 27.6166\n",
      "step 1280 | train loss: 4.24 | val loss: 4.01 | perplexity: 55.23 | lr: 7.73e-04 | norm: 0.3465 | dt: 14269.8541ms | tok/sec: 27.5557\n",
      "step 1281 | train loss: 4.20 | val loss: 4.01 | perplexity: 54.98 | lr: 7.72e-04 | norm: 0.3724 | dt: 14265.7003ms | tok/sec: 27.5637\n",
      "step 1282 | train loss: 4.41 | val loss: 4.01 | perplexity: 55.12 | lr: 7.71e-04 | norm: 0.3792 | dt: 14234.5412ms | tok/sec: 27.6241\n",
      "step 1283 | train loss: 4.25 | val loss: 4.01 | perplexity: 54.98 | lr: 7.70e-04 | norm: 0.4702 | dt: 14269.2752ms | tok/sec: 27.5568\n",
      "step 1284 | train loss: 4.29 | val loss: 4.01 | perplexity: 55.06 | lr: 7.70e-04 | norm: 0.4176 | dt: 14231.9393ms | tok/sec: 27.6291\n",
      "step 1285 | train loss: 4.27 | val loss: 4.01 | perplexity: 55.42 | lr: 7.69e-04 | norm: 0.3628 | dt: 14218.6131ms | tok/sec: 27.6550\n",
      "step 1286 | train loss: 4.30 | val loss: 4.02 | perplexity: 55.58 | lr: 7.68e-04 | norm: 0.4494 | dt: 14215.9829ms | tok/sec: 27.6601\n",
      "step 1287 | train loss: 4.11 | val loss: 4.02 | perplexity: 55.57 | lr: 7.67e-04 | norm: 0.4006 | dt: 14250.3185ms | tok/sec: 27.5935\n",
      "step 1288 | train loss: 4.06 | val loss: 4.01 | perplexity: 55.41 | lr: 7.67e-04 | norm: 0.4440 | dt: 14224.9582ms | tok/sec: 27.6427\n",
      "step 1289 | train loss: 4.48 | val loss: 4.02 | perplexity: 55.45 | lr: 7.66e-04 | norm: 0.5301 | dt: 14234.8490ms | tok/sec: 27.6235\n",
      "step 1290 | train loss: 4.36 | val loss: 4.01 | perplexity: 55.27 | lr: 7.65e-04 | norm: 0.5217 | dt: 14232.7273ms | tok/sec: 27.6276\n",
      "step 1291 | train loss: 4.40 | val loss: 4.02 | perplexity: 55.84 | lr: 7.65e-04 | norm: 0.4161 | dt: 14243.2106ms | tok/sec: 27.6073\n",
      "step 1292 | train loss: 4.02 | val loss: 4.02 | perplexity: 55.82 | lr: 7.64e-04 | norm: 0.5567 | dt: 14229.7237ms | tok/sec: 27.6334\n",
      "step 1293 | train loss: 4.14 | val loss: 4.02 | perplexity: 55.97 | lr: 7.63e-04 | norm: 0.5263 | dt: 14230.2697ms | tok/sec: 27.6324\n",
      "step 1294 | train loss: 3.92 | val loss: 4.04 | perplexity: 56.76 | lr: 7.62e-04 | norm: 0.4062 | dt: 14233.3264ms | tok/sec: 27.6264\n",
      "step 1295 | train loss: 3.98 | val loss: 4.01 | perplexity: 55.31 | lr: 7.62e-04 | norm: 0.6241 | dt: 14284.6372ms | tok/sec: 27.5272\n",
      "step 1296 | train loss: 4.01 | val loss: 4.01 | perplexity: 54.98 | lr: 7.61e-04 | norm: 0.4161 | dt: 14226.5043ms | tok/sec: 27.6397\n",
      "step 1297 | train loss: 4.01 | val loss: 4.00 | perplexity: 54.81 | lr: 7.60e-04 | norm: 0.4645 | dt: 14233.3043ms | tok/sec: 27.6265\n",
      "step 1298 | train loss: 4.22 | val loss: 4.01 | perplexity: 55.18 | lr: 7.59e-04 | norm: 0.4951 | dt: 13644.1069ms | tok/sec: 28.8195\n",
      "step 1299 | train loss: 4.28 | val loss: 4.01 | perplexity: 55.40 | lr: 7.59e-04 | norm: 0.5194 | dt: 13660.0213ms | tok/sec: 28.7859\n",
      "step 1300 | train loss: 4.22 | val loss: 4.01 | perplexity: 55.40 | lr: 7.58e-04 | norm: 0.4345 | dt: 13601.8434ms | tok/sec: 28.9090\n",
      "step 1301 | train loss: 4.51 | val loss: 4.01 | perplexity: 55.33 | lr: 7.57e-04 | norm: 0.4856 | dt: 13651.3090ms | tok/sec: 28.8043\n",
      "step 1302 | train loss: 3.98 | val loss: 4.01 | perplexity: 54.93 | lr: 7.56e-04 | norm: 0.4307 | dt: 13647.2015ms | tok/sec: 28.8129\n",
      "step 1303 | train loss: 3.94 | val loss: 4.00 | perplexity: 54.43 | lr: 7.56e-04 | norm: 0.4189 | dt: 13667.8226ms | tok/sec: 28.7695\n",
      "step 1304 | train loss: 4.07 | val loss: 4.00 | perplexity: 54.65 | lr: 7.55e-04 | norm: 0.3803 | dt: 13642.4000ms | tok/sec: 28.8231\n",
      "step 1305 | train loss: 4.21 | val loss: 4.00 | perplexity: 54.46 | lr: 7.54e-04 | norm: 0.3693 | dt: 13620.5573ms | tok/sec: 28.8693\n",
      "step 1306 | train loss: 3.92 | val loss: 4.00 | perplexity: 54.50 | lr: 7.53e-04 | norm: 0.3779 | dt: 13645.5715ms | tok/sec: 28.8164\n",
      "step 1307 | train loss: 4.22 | val loss: 4.00 | perplexity: 54.75 | lr: 7.53e-04 | norm: 0.4683 | dt: 13610.2188ms | tok/sec: 28.8912\n",
      "step 1308 | train loss: 4.45 | val loss: 4.01 | perplexity: 54.93 | lr: 7.52e-04 | norm: 0.3540 | dt: 13657.0892ms | tok/sec: 28.7921\n",
      "step 1309 | train loss: 4.17 | val loss: 4.00 | perplexity: 54.33 | lr: 7.51e-04 | norm: 0.4925 | dt: 13672.3795ms | tok/sec: 28.7599\n",
      "step 1310 | train loss: 4.13 | val loss: 4.00 | perplexity: 54.35 | lr: 7.50e-04 | norm: 0.3426 | dt: 13760.7591ms | tok/sec: 28.5752\n",
      "step 1311 | train loss: 3.90 | val loss: 3.98 | perplexity: 53.71 | lr: 7.50e-04 | norm: 0.4249 | dt: 13683.6314ms | tok/sec: 28.7362\n",
      "step 1312 | train loss: 4.19 | val loss: 3.98 | perplexity: 53.56 | lr: 7.49e-04 | norm: 0.4225 | dt: 13636.5256ms | tok/sec: 28.8355\n",
      "step 1313 | train loss: 4.11 | val loss: 4.00 | perplexity: 54.33 | lr: 7.48e-04 | norm: 0.4011 | dt: 13628.0313ms | tok/sec: 28.8535\n",
      "step 1314 | train loss: 4.05 | val loss: 3.99 | perplexity: 54.19 | lr: 7.47e-04 | norm: 0.4340 | dt: 13619.7448ms | tok/sec: 28.8710\n",
      "step 1315 | train loss: 4.06 | val loss: 3.98 | perplexity: 53.64 | lr: 7.47e-04 | norm: 0.4267 | dt: 13650.7142ms | tok/sec: 28.8055\n",
      "step 1316 | train loss: 4.18 | val loss: 3.98 | perplexity: 53.50 | lr: 7.46e-04 | norm: 0.3924 | dt: 13765.3906ms | tok/sec: 28.5656\n",
      "step 1317 | train loss: 4.05 | val loss: 3.98 | perplexity: 53.67 | lr: 7.45e-04 | norm: 0.3759 | dt: 13641.0172ms | tok/sec: 28.8260\n",
      "step 1318 | train loss: 4.29 | val loss: 3.99 | perplexity: 53.81 | lr: 7.44e-04 | norm: 0.4043 | dt: 13627.6319ms | tok/sec: 28.8543\n",
      "step 1319 | train loss: 4.20 | val loss: 3.98 | perplexity: 53.63 | lr: 7.44e-04 | norm: 0.4168 | dt: 13660.9523ms | tok/sec: 28.7839\n",
      "step 1320 | train loss: 4.18 | val loss: 3.98 | perplexity: 53.46 | lr: 7.43e-04 | norm: 0.5129 | dt: 13646.3971ms | tok/sec: 28.8146\n",
      "step 1321 | train loss: 4.17 | val loss: 3.98 | perplexity: 53.38 | lr: 7.42e-04 | norm: 0.4551 | dt: 13606.6360ms | tok/sec: 28.8988\n",
      "step 1322 | train loss: 4.06 | val loss: 3.98 | perplexity: 53.36 | lr: 7.41e-04 | norm: 0.3743 | dt: 13640.5985ms | tok/sec: 28.8269\n",
      "step 1323 | train loss: 3.89 | val loss: 3.98 | perplexity: 53.50 | lr: 7.41e-04 | norm: 0.3324 | dt: 13681.4189ms | tok/sec: 28.7409\n",
      "step 1324 | train loss: 4.04 | val loss: 3.97 | perplexity: 52.90 | lr: 7.40e-04 | norm: 0.4077 | dt: 13647.4125ms | tok/sec: 28.8125\n",
      "step 1325 | train loss: 4.03 | val loss: 3.97 | perplexity: 52.79 | lr: 7.39e-04 | norm: 0.3993 | dt: 13653.5158ms | tok/sec: 28.7996\n",
      "step 1326 | train loss: 4.25 | val loss: 3.98 | perplexity: 53.32 | lr: 7.38e-04 | norm: 0.3759 | dt: 13654.8114ms | tok/sec: 28.7969\n",
      "step 1327 | train loss: 3.98 | val loss: 3.97 | perplexity: 52.98 | lr: 7.37e-04 | norm: 0.5264 | dt: 13755.3036ms | tok/sec: 28.5865\n",
      "step 1328 | train loss: 4.27 | val loss: 3.97 | perplexity: 53.18 | lr: 7.37e-04 | norm: 0.4060 | dt: 13659.4355ms | tok/sec: 28.7871\n",
      "step 1329 | train loss: 3.94 | val loss: 3.97 | perplexity: 53.05 | lr: 7.36e-04 | norm: 0.3793 | dt: 13697.2775ms | tok/sec: 28.7076\n",
      "step 1330 | train loss: 4.28 | val loss: 3.97 | perplexity: 52.86 | lr: 7.35e-04 | norm: 0.4033 | dt: 13639.8382ms | tok/sec: 28.8285\n",
      "step 1331 | train loss: 4.13 | val loss: 3.97 | perplexity: 52.96 | lr: 7.34e-04 | norm: 0.3311 | dt: 13645.6239ms | tok/sec: 28.8163\n",
      "step 1332 | train loss: 4.06 | val loss: 3.97 | perplexity: 52.84 | lr: 7.34e-04 | norm: 0.3622 | dt: 13636.1983ms | tok/sec: 28.8362\n",
      "step 1333 | train loss: 4.05 | val loss: 3.97 | perplexity: 52.77 | lr: 7.33e-04 | norm: 0.4262 | dt: 13631.9251ms | tok/sec: 28.8452\n",
      "step 1334 | train loss: 4.01 | val loss: 3.98 | perplexity: 53.26 | lr: 7.32e-04 | norm: 0.3855 | dt: 13639.1351ms | tok/sec: 28.8300\n",
      "step 1335 | train loss: 4.03 | val loss: 3.98 | perplexity: 53.32 | lr: 7.31e-04 | norm: 0.4093 | dt: 13657.3272ms | tok/sec: 28.7916\n",
      "step 1336 | train loss: 3.96 | val loss: 3.96 | perplexity: 52.56 | lr: 7.31e-04 | norm: 0.4142 | dt: 13641.9899ms | tok/sec: 28.8239\n",
      "step 1337 | train loss: 4.10 | val loss: 3.96 | perplexity: 52.43 | lr: 7.30e-04 | norm: 0.4207 | dt: 13676.1231ms | tok/sec: 28.7520\n",
      "step 1338 | train loss: 4.90 | val loss: 3.99 | perplexity: 53.87 | lr: 7.29e-04 | norm: 0.6008 | dt: 13677.8319ms | tok/sec: 28.7484\n",
      "step 1339 | train loss: 4.67 | val loss: 3.99 | perplexity: 53.94 | lr: 7.28e-04 | norm: 0.8221 | dt: 13624.5742ms | tok/sec: 28.8608\n",
      "step 1340 | train loss: 4.21 | val loss: 3.98 | perplexity: 53.65 | lr: 7.28e-04 | norm: 0.5061 | dt: 13616.8933ms | tok/sec: 28.8771\n",
      "step 1341 | train loss: 4.09 | val loss: 3.98 | perplexity: 53.53 | lr: 7.27e-04 | norm: 0.4944 | dt: 13614.2571ms | tok/sec: 28.8827\n",
      "step 1342 | train loss: 4.24 | val loss: 3.98 | perplexity: 53.76 | lr: 7.26e-04 | norm: 0.4788 | dt: 13649.5399ms | tok/sec: 28.8080\n",
      "step 1343 | train loss: 4.34 | val loss: 3.98 | perplexity: 53.66 | lr: 7.25e-04 | norm: 0.4598 | dt: 13665.9334ms | tok/sec: 28.7734\n",
      "step 1344 | train loss: 4.03 | val loss: 3.98 | perplexity: 53.34 | lr: 7.24e-04 | norm: 0.5479 | dt: 13632.2682ms | tok/sec: 28.8445\n",
      "step 1345 | train loss: 4.15 | val loss: 3.98 | perplexity: 53.45 | lr: 7.24e-04 | norm: 0.4468 | dt: 13659.4219ms | tok/sec: 28.7872\n",
      "step 1346 | train loss: 4.08 | val loss: 3.98 | perplexity: 53.31 | lr: 7.23e-04 | norm: 0.6468 | dt: 13624.8293ms | tok/sec: 28.8603\n",
      "step 1347 | train loss: 3.99 | val loss: 3.97 | perplexity: 53.18 | lr: 7.22e-04 | norm: 0.4769 | dt: 13656.8234ms | tok/sec: 28.7926\n",
      "step 1348 | train loss: 3.94 | val loss: 3.97 | perplexity: 52.85 | lr: 7.21e-04 | norm: 0.4578 | dt: 13650.9354ms | tok/sec: 28.8051\n",
      "step 1349 | train loss: 4.10 | val loss: 3.97 | perplexity: 52.92 | lr: 7.21e-04 | norm: 0.4205 | dt: 13643.0655ms | tok/sec: 28.8217\n",
      "step 1350 | train loss: 4.11 | val loss: 3.98 | perplexity: 53.35 | lr: 7.20e-04 | norm: 0.4196 | dt: 13650.6045ms | tok/sec: 28.8058\n",
      "step 1351 | train loss: 4.05 | val loss: 3.97 | perplexity: 53.10 | lr: 7.19e-04 | norm: 0.3659 | dt: 13667.3455ms | tok/sec: 28.7705\n",
      "step 1352 | train loss: 4.43 | val loss: 3.97 | perplexity: 52.97 | lr: 7.18e-04 | norm: 0.5146 | dt: 13647.4962ms | tok/sec: 28.8123\n",
      "step 1353 | train loss: 3.80 | val loss: 3.97 | perplexity: 53.07 | lr: 7.17e-04 | norm: 0.4242 | dt: 13607.4994ms | tok/sec: 28.8970\n",
      "step 1354 | train loss: 3.88 | val loss: 3.97 | perplexity: 53.21 | lr: 7.17e-04 | norm: 0.4444 | dt: 13620.4855ms | tok/sec: 28.8695\n",
      "step 1355 | train loss: 3.99 | val loss: 3.98 | perplexity: 53.45 | lr: 7.16e-04 | norm: 0.4275 | dt: 13651.0506ms | tok/sec: 28.8048\n",
      "step 1356 | train loss: 3.96 | val loss: 3.98 | perplexity: 53.45 | lr: 7.15e-04 | norm: 0.4745 | dt: 13672.8008ms | tok/sec: 28.7590\n",
      "step 1357 | train loss: 4.04 | val loss: 3.95 | perplexity: 52.19 | lr: 7.14e-04 | norm: 0.6102 | dt: 13669.6148ms | tok/sec: 28.7657\n",
      "step 1358 | train loss: 3.85 | val loss: 3.96 | perplexity: 52.25 | lr: 7.14e-04 | norm: 0.4534 | dt: 13655.2892ms | tok/sec: 28.7959\n",
      "step 1359 | train loss: 3.78 | val loss: 3.95 | perplexity: 51.94 | lr: 7.13e-04 | norm: 0.6063 | dt: 13654.5510ms | tok/sec: 28.7974\n",
      "step 1360 | train loss: 3.72 | val loss: 3.95 | perplexity: 51.72 | lr: 7.12e-04 | norm: 0.4196 | dt: 13641.2332ms | tok/sec: 28.8255\n",
      "step 1361 | train loss: 3.95 | val loss: 3.93 | perplexity: 50.92 | lr: 7.11e-04 | norm: 0.4130 | dt: 13645.9637ms | tok/sec: 28.8156\n",
      "step 1362 | train loss: 3.59 | val loss: 3.93 | perplexity: 50.81 | lr: 7.10e-04 | norm: 0.5388 | dt: 13645.5867ms | tok/sec: 28.8163\n",
      "step 1363 | train loss: 5.27 | val loss: 3.93 | perplexity: 50.94 | lr: 7.10e-04 | norm: 0.7457 | dt: 13645.1511ms | tok/sec: 28.8173\n",
      "step 1364 | train loss: 3.80 | val loss: 3.93 | perplexity: 50.86 | lr: 7.09e-04 | norm: 0.4326 | dt: 13680.6073ms | tok/sec: 28.7426\n",
      "step 1365 | train loss: 3.95 | val loss: 3.92 | perplexity: 50.32 | lr: 7.08e-04 | norm: 0.4448 | dt: 13648.2503ms | tok/sec: 28.8107\n",
      "step 1366 | train loss: 4.02 | val loss: 3.94 | perplexity: 51.17 | lr: 7.07e-04 | norm: 0.4766 | dt: 13654.4256ms | tok/sec: 28.7977\n",
      "step 1367 | train loss: 3.92 | val loss: 3.93 | perplexity: 51.02 | lr: 7.06e-04 | norm: 0.5252 | dt: 13647.5098ms | tok/sec: 28.8123\n",
      "step 1368 | train loss: 3.67 | val loss: 3.94 | perplexity: 51.43 | lr: 7.06e-04 | norm: 0.5025 | dt: 13772.1310ms | tok/sec: 28.5516\n",
      "step 1369 | train loss: 3.88 | val loss: 3.97 | perplexity: 52.98 | lr: 7.05e-04 | norm: 0.6743 | dt: 13695.8172ms | tok/sec: 28.7107\n",
      "step 1370 | train loss: 4.10 | val loss: 3.97 | perplexity: 52.97 | lr: 7.04e-04 | norm: 0.5623 | dt: 13647.8465ms | tok/sec: 28.8116\n",
      "step 1371 | train loss: 3.91 | val loss: 3.96 | perplexity: 52.69 | lr: 7.03e-04 | norm: 0.4735 | dt: 13677.6361ms | tok/sec: 28.7488\n",
      "step 1372 | train loss: 3.99 | val loss: 3.96 | perplexity: 52.35 | lr: 7.03e-04 | norm: 0.5143 | dt: 13624.7320ms | tok/sec: 28.8605\n",
      "step 1373 | train loss: 3.99 | val loss: 3.95 | perplexity: 51.82 | lr: 7.02e-04 | norm: 0.5063 | dt: 13651.0260ms | tok/sec: 28.8049\n",
      "step 1374 | train loss: 3.76 | val loss: 3.94 | perplexity: 51.48 | lr: 7.01e-04 | norm: 0.4004 | dt: 13637.0306ms | tok/sec: 28.8344\n",
      "step 1375 | train loss: 3.50 | val loss: 3.94 | perplexity: 51.48 | lr: 7.00e-04 | norm: 0.6082 | dt: 13646.8840ms | tok/sec: 28.8136\n",
      "step 1376 | train loss: 3.86 | val loss: 3.94 | perplexity: 51.19 | lr: 6.99e-04 | norm: 0.4898 | dt: 13659.0357ms | tok/sec: 28.7880\n",
      "step 1377 | train loss: 4.32 | val loss: 3.94 | perplexity: 51.33 | lr: 6.99e-04 | norm: 0.4525 | dt: 13644.6571ms | tok/sec: 28.8183\n",
      "step 1378 | train loss: 3.84 | val loss: 3.94 | perplexity: 51.48 | lr: 6.98e-04 | norm: 0.6752 | dt: 13640.5034ms | tok/sec: 28.8271\n",
      "step 1379 | train loss: 3.73 | val loss: 3.94 | perplexity: 51.24 | lr: 6.97e-04 | norm: 0.6219 | dt: 13664.9909ms | tok/sec: 28.7754\n",
      "step 1380 | train loss: 4.31 | val loss: 3.95 | perplexity: 51.87 | lr: 6.96e-04 | norm: 0.4375 | dt: 13660.8419ms | tok/sec: 28.7842\n",
      "step 1381 | train loss: 3.85 | val loss: 3.96 | perplexity: 52.30 | lr: 6.95e-04 | norm: 0.4981 | dt: 13653.2190ms | tok/sec: 28.8002\n",
      "step 1382 | train loss: 3.66 | val loss: 3.95 | perplexity: 51.79 | lr: 6.95e-04 | norm: 0.5458 | dt: 13644.1376ms | tok/sec: 28.8194\n",
      "step 1383 | train loss: 3.91 | val loss: 3.94 | perplexity: 51.42 | lr: 6.94e-04 | norm: 0.4314 | dt: 13637.3441ms | tok/sec: 28.8338\n",
      "step 1384 | train loss: 3.78 | val loss: 3.95 | perplexity: 51.75 | lr: 6.93e-04 | norm: 1.0619 | dt: 13677.5346ms | tok/sec: 28.7490\n",
      "step 1385 | train loss: 3.80 | val loss: 3.95 | perplexity: 51.71 | lr: 6.92e-04 | norm: 0.4920 | dt: 13671.4523ms | tok/sec: 28.7618\n",
      "step 1386 | train loss: 3.78 | val loss: 3.94 | perplexity: 51.41 | lr: 6.91e-04 | norm: 0.5211 | dt: 13660.3241ms | tok/sec: 28.7853\n",
      "step 1387 | train loss: 3.81 | val loss: 3.94 | perplexity: 51.40 | lr: 6.91e-04 | norm: 0.6087 | dt: 13644.3570ms | tok/sec: 28.8189\n",
      "step 1388 | train loss: 3.87 | val loss: 3.95 | perplexity: 51.76 | lr: 6.90e-04 | norm: 0.4129 | dt: 13628.9589ms | tok/sec: 28.8515\n",
      "step 1389 | train loss: 3.62 | val loss: 3.94 | perplexity: 51.56 | lr: 6.89e-04 | norm: 0.5024 | dt: 13646.4567ms | tok/sec: 28.8145\n",
      "step 1390 | train loss: 4.41 | val loss: 3.94 | perplexity: 51.40 | lr: 6.88e-04 | norm: 0.4775 | dt: 13770.0179ms | tok/sec: 28.5560\n",
      "step 1391 | train loss: 3.86 | val loss: 3.94 | perplexity: 51.28 | lr: 6.87e-04 | norm: 0.4697 | dt: 13665.4935ms | tok/sec: 28.7744\n",
      "step 1392 | train loss: 3.91 | val loss: 3.94 | perplexity: 51.20 | lr: 6.87e-04 | norm: 0.3828 | dt: 13629.5874ms | tok/sec: 28.8502\n",
      "step 1393 | train loss: 4.03 | val loss: 3.94 | perplexity: 51.34 | lr: 6.86e-04 | norm: 0.4130 | dt: 13791.0943ms | tok/sec: 28.5123\n",
      "step 1394 | train loss: 3.99 | val loss: 3.95 | perplexity: 51.68 | lr: 6.85e-04 | norm: 0.4716 | dt: 13682.8713ms | tok/sec: 28.7378\n",
      "step 1395 | train loss: 4.13 | val loss: 3.94 | perplexity: 51.66 | lr: 6.84e-04 | norm: 0.4889 | dt: 13756.8967ms | tok/sec: 28.5832\n",
      "step 1396 | train loss: 3.68 | val loss: 3.94 | perplexity: 51.45 | lr: 6.83e-04 | norm: 0.4114 | dt: 13654.6431ms | tok/sec: 28.7972\n",
      "step 1397 | train loss: 4.00 | val loss: 3.93 | perplexity: 50.72 | lr: 6.83e-04 | norm: 0.5582 | dt: 13651.2704ms | tok/sec: 28.8044\n",
      "step 1398 | train loss: 3.83 | val loss: 3.93 | perplexity: 50.86 | lr: 6.82e-04 | norm: 0.4675 | dt: 13661.8190ms | tok/sec: 28.7821\n",
      "step 1399 | train loss: 3.97 | val loss: 3.93 | perplexity: 50.73 | lr: 6.81e-04 | norm: 0.5335 | dt: 13653.8579ms | tok/sec: 28.7989\n",
      "step 1400 | train loss: 3.91 | val loss: 3.93 | perplexity: 50.86 | lr: 6.80e-04 | norm: 0.4794 | dt: 13641.6132ms | tok/sec: 28.8247\n",
      "step 1401 | train loss: 3.98 | val loss: 3.93 | perplexity: 50.85 | lr: 6.79e-04 | norm: 0.4417 | dt: 13662.7929ms | tok/sec: 28.7801\n",
      "step 1402 | train loss: 4.04 | val loss: 3.92 | perplexity: 50.61 | lr: 6.79e-04 | norm: 0.3813 | dt: 13624.4757ms | tok/sec: 28.8610\n",
      "step 1403 | train loss: 3.74 | val loss: 3.91 | perplexity: 50.06 | lr: 6.78e-04 | norm: 0.3830 | dt: 13636.7273ms | tok/sec: 28.8351\n",
      "step 1404 | train loss: 4.02 | val loss: 3.91 | perplexity: 49.76 | lr: 6.77e-04 | norm: 0.4176 | dt: 13695.9236ms | tok/sec: 28.7104\n",
      "step 1405 | train loss: 3.98 | val loss: 3.91 | perplexity: 49.66 | lr: 6.76e-04 | norm: 0.4682 | dt: 13660.5494ms | tok/sec: 28.7848\n",
      "step 1406 | train loss: 3.76 | val loss: 3.91 | perplexity: 49.91 | lr: 6.75e-04 | norm: 0.3616 | dt: 13628.0222ms | tok/sec: 28.8535\n",
      "step 1407 | train loss: 4.35 | val loss: 3.91 | perplexity: 50.07 | lr: 6.75e-04 | norm: 0.5571 | dt: 13656.7354ms | tok/sec: 28.7928\n",
      "step 1408 | train loss: 4.32 | val loss: 3.92 | perplexity: 50.18 | lr: 6.74e-04 | norm: 0.7099 | dt: 13647.1107ms | tok/sec: 28.8131\n",
      "step 1409 | train loss: 4.69 | val loss: 3.92 | perplexity: 50.34 | lr: 6.73e-04 | norm: 0.4873 | dt: 13650.1083ms | tok/sec: 28.8068\n",
      "step 1410 | train loss: 4.18 | val loss: 3.91 | perplexity: 50.01 | lr: 6.72e-04 | norm: 0.4383 | dt: 13631.9187ms | tok/sec: 28.8452\n",
      "step 1411 | train loss: 3.78 | val loss: 3.91 | perplexity: 49.89 | lr: 6.71e-04 | norm: 0.3929 | dt: 13644.8596ms | tok/sec: 28.8179\n",
      "step 1412 | train loss: 3.90 | val loss: 3.91 | perplexity: 49.87 | lr: 6.71e-04 | norm: 0.5441 | dt: 13664.2315ms | tok/sec: 28.7770\n",
      "step 1413 | train loss: 3.71 | val loss: 3.90 | perplexity: 49.51 | lr: 6.70e-04 | norm: 0.4291 | dt: 13684.2263ms | tok/sec: 28.7350\n",
      "step 1414 | train loss: 3.84 | val loss: 3.90 | perplexity: 49.30 | lr: 6.69e-04 | norm: 0.3901 | dt: 13772.4891ms | tok/sec: 28.5508\n",
      "step 1415 | train loss: 3.93 | val loss: 3.89 | perplexity: 49.09 | lr: 6.68e-04 | norm: 0.4344 | dt: 13648.4749ms | tok/sec: 28.8103\n",
      "step 1416 | train loss: 3.93 | val loss: 3.89 | perplexity: 49.03 | lr: 6.67e-04 | norm: 0.4291 | dt: 13652.2851ms | tok/sec: 28.8022\n",
      "step 1417 | train loss: 3.98 | val loss: 3.89 | perplexity: 49.04 | lr: 6.67e-04 | norm: 0.3559 | dt: 13645.6778ms | tok/sec: 28.8162\n",
      "step 1418 | train loss: 4.27 | val loss: 3.89 | perplexity: 49.10 | lr: 6.66e-04 | norm: 0.4510 | dt: 13655.7031ms | tok/sec: 28.7950\n",
      "step 1419 | train loss: 4.26 | val loss: 3.91 | perplexity: 50.01 | lr: 6.65e-04 | norm: 0.9447 | dt: 13684.9046ms | tok/sec: 28.7336\n",
      "step 1420 | train loss: 3.93 | val loss: 3.92 | perplexity: 50.22 | lr: 6.64e-04 | norm: 0.4604 | dt: 13665.2653ms | tok/sec: 28.7749\n",
      "step 1421 | train loss: 3.84 | val loss: 3.93 | perplexity: 50.80 | lr: 6.63e-04 | norm: 0.5357 | dt: 13662.7517ms | tok/sec: 28.7801\n",
      "step 1422 | train loss: 3.90 | val loss: 3.93 | perplexity: 50.72 | lr: 6.62e-04 | norm: 0.4240 | dt: 13635.8449ms | tok/sec: 28.8369\n",
      "step 1423 | train loss: 3.98 | val loss: 3.92 | perplexity: 50.44 | lr: 6.62e-04 | norm: 0.4605 | dt: 13685.6432ms | tok/sec: 28.7320\n",
      "step 1424 | train loss: 3.65 | val loss: 3.91 | perplexity: 50.03 | lr: 6.61e-04 | norm: 0.3590 | dt: 13655.3445ms | tok/sec: 28.7958\n",
      "step 1425 | train loss: 4.02 | val loss: 3.90 | perplexity: 49.35 | lr: 6.60e-04 | norm: 0.3957 | dt: 13640.8095ms | tok/sec: 28.8264\n",
      "step 1426 | train loss: 3.82 | val loss: 3.90 | perplexity: 49.37 | lr: 6.59e-04 | norm: 0.4620 | dt: 13634.4812ms | tok/sec: 28.8398\n",
      "step 1427 | train loss: 3.86 | val loss: 3.89 | perplexity: 48.99 | lr: 6.58e-04 | norm: 0.3992 | dt: 13658.2534ms | tok/sec: 28.7896\n",
      "step 1428 | train loss: 3.96 | val loss: 3.89 | perplexity: 48.93 | lr: 6.58e-04 | norm: 0.3730 | dt: 13699.6527ms | tok/sec: 28.7026\n",
      "step 1429 | train loss: 4.19 | val loss: 3.89 | perplexity: 48.81 | lr: 6.57e-04 | norm: 0.4797 | dt: 13644.0880ms | tok/sec: 28.8195\n",
      "step 1430 | train loss: 4.07 | val loss: 3.89 | perplexity: 48.74 | lr: 6.56e-04 | norm: 0.3240 | dt: 13624.5577ms | tok/sec: 28.8608\n",
      "step 1431 | train loss: 3.65 | val loss: 3.88 | perplexity: 48.63 | lr: 6.55e-04 | norm: 0.3751 | dt: 13976.1281ms | tok/sec: 28.1348\n",
      "step 1432 | train loss: 3.76 | val loss: 3.89 | perplexity: 48.71 | lr: 6.54e-04 | norm: 0.4846 | dt: 14585.5277ms | tok/sec: 26.9593\n",
      "step 1433 | train loss: 3.89 | val loss: 3.89 | perplexity: 48.83 | lr: 6.54e-04 | norm: 0.4578 | dt: 14347.9090ms | tok/sec: 27.4058\n",
      "step 1434 | train loss: 3.76 | val loss: 3.89 | perplexity: 48.95 | lr: 6.53e-04 | norm: 0.4183 | dt: 14356.0448ms | tok/sec: 27.3903\n",
      "step 1435 | train loss: 3.94 | val loss: 3.89 | perplexity: 49.01 | lr: 6.52e-04 | norm: 0.3653 | dt: 14336.7569ms | tok/sec: 27.4271\n",
      "step 1436 | train loss: 4.39 | val loss: 3.89 | perplexity: 48.89 | lr: 6.51e-04 | norm: 0.4378 | dt: 14319.7966ms | tok/sec: 27.4596\n",
      "step 1437 | train loss: 4.18 | val loss: 3.89 | perplexity: 48.85 | lr: 6.50e-04 | norm: 0.3547 | dt: 14358.8080ms | tok/sec: 27.3850\n",
      "step 1438 | train loss: 4.17 | val loss: 3.88 | perplexity: 48.29 | lr: 6.49e-04 | norm: 0.3956 | dt: 14321.6431ms | tok/sec: 27.4561\n",
      "step 1439 | train loss: 4.22 | val loss: 3.88 | perplexity: 48.24 | lr: 6.49e-04 | norm: 0.4050 | dt: 14321.1980ms | tok/sec: 27.4569\n",
      "step 1440 | train loss: 4.05 | val loss: 3.88 | perplexity: 48.20 | lr: 6.48e-04 | norm: 0.3947 | dt: 14401.1416ms | tok/sec: 27.3045\n",
      "step 1441 | train loss: 3.91 | val loss: 3.87 | perplexity: 48.15 | lr: 6.47e-04 | norm: 0.4237 | dt: 14337.9214ms | tok/sec: 27.4249\n",
      "step 1442 | train loss: 3.72 | val loss: 3.88 | perplexity: 48.63 | lr: 6.46e-04 | norm: 0.3647 | dt: 14347.4822ms | tok/sec: 27.4066\n",
      "step 1443 | train loss: 4.27 | val loss: 3.88 | perplexity: 48.33 | lr: 6.45e-04 | norm: 0.5264 | dt: 14359.1428ms | tok/sec: 27.3844\n",
      "step 1444 | train loss: 4.33 | val loss: 3.88 | perplexity: 48.40 | lr: 6.45e-04 | norm: 0.4757 | dt: 14363.7049ms | tok/sec: 27.3757\n",
      "step 1445 | train loss: 3.72 | val loss: 3.88 | perplexity: 48.63 | lr: 6.44e-04 | norm: 0.4503 | dt: 14330.5798ms | tok/sec: 27.4389\n",
      "step 1446 | train loss: 4.22 | val loss: 3.87 | perplexity: 48.12 | lr: 6.43e-04 | norm: 0.5275 | dt: 14370.5969ms | tok/sec: 27.3625\n",
      "step 1447 | train loss: 3.75 | val loss: 3.88 | perplexity: 48.34 | lr: 6.42e-04 | norm: 0.5519 | dt: 14294.5943ms | tok/sec: 27.5080\n",
      "step 1448 | train loss: 4.10 | val loss: 3.89 | perplexity: 48.81 | lr: 6.41e-04 | norm: 0.5185 | dt: 14310.1373ms | tok/sec: 27.4781\n",
      "step 1449 | train loss: 3.75 | val loss: 3.90 | perplexity: 49.37 | lr: 6.40e-04 | norm: 0.4378 | dt: 14324.3058ms | tok/sec: 27.4510\n",
      "step 1450 | train loss: 3.57 | val loss: 3.90 | perplexity: 49.56 | lr: 6.40e-04 | norm: 0.3808 | dt: 14320.9672ms | tok/sec: 27.4574\n",
      "step 1451 | train loss: 3.76 | val loss: 3.90 | perplexity: 49.33 | lr: 6.39e-04 | norm: 0.3760 | dt: 14310.5891ms | tok/sec: 27.4773\n",
      "step 1452 | train loss: 3.87 | val loss: 3.89 | perplexity: 49.06 | lr: 6.38e-04 | norm: 0.3901 | dt: 14319.5090ms | tok/sec: 27.4602\n",
      "step 1453 | train loss: 3.86 | val loss: 3.89 | perplexity: 48.80 | lr: 6.37e-04 | norm: 0.3330 | dt: 14301.2905ms | tok/sec: 27.4951\n",
      "step 1454 | train loss: 3.99 | val loss: 3.88 | perplexity: 48.43 | lr: 6.36e-04 | norm: 0.3659 | dt: 14310.4370ms | tok/sec: 27.4776\n",
      "step 1455 | train loss: 3.78 | val loss: 3.88 | perplexity: 48.42 | lr: 6.35e-04 | norm: 0.4041 | dt: 14309.8750ms | tok/sec: 27.4786\n",
      "step 1456 | train loss: 4.08 | val loss: 3.88 | perplexity: 48.46 | lr: 6.35e-04 | norm: 0.4602 | dt: 14312.3884ms | tok/sec: 27.4738\n",
      "step 1457 | train loss: 3.99 | val loss: 3.88 | perplexity: 48.37 | lr: 6.34e-04 | norm: 0.3777 | dt: 14293.5181ms | tok/sec: 27.5101\n",
      "step 1458 | train loss: 3.70 | val loss: 3.88 | perplexity: 48.19 | lr: 6.33e-04 | norm: 0.4247 | dt: 14317.7915ms | tok/sec: 27.4635\n",
      "step 1459 | train loss: 3.96 | val loss: 3.87 | perplexity: 48.11 | lr: 6.32e-04 | norm: 0.3893 | dt: 14323.7138ms | tok/sec: 27.4521\n",
      "step 1460 | train loss: 3.89 | val loss: 3.87 | perplexity: 47.76 | lr: 6.31e-04 | norm: 0.3886 | dt: 14294.6942ms | tok/sec: 27.5078\n",
      "step 1461 | train loss: 3.94 | val loss: 3.86 | perplexity: 47.67 | lr: 6.31e-04 | norm: 0.3686 | dt: 14305.3346ms | tok/sec: 27.4874\n",
      "step 1462 | train loss: 3.73 | val loss: 3.87 | perplexity: 48.06 | lr: 6.30e-04 | norm: 0.3740 | dt: 14490.0532ms | tok/sec: 27.1370\n",
      "step 1463 | train loss: 3.79 | val loss: 3.88 | perplexity: 48.33 | lr: 6.29e-04 | norm: 0.3609 | dt: 14320.5779ms | tok/sec: 27.4581\n",
      "step 1464 | train loss: 3.91 | val loss: 3.88 | perplexity: 48.33 | lr: 6.28e-04 | norm: 0.3372 | dt: 14337.9245ms | tok/sec: 27.4249\n",
      "step 1465 | train loss: 3.99 | val loss: 3.88 | perplexity: 48.29 | lr: 6.27e-04 | norm: 0.3779 | dt: 14323.7586ms | tok/sec: 27.4520\n",
      "step 1466 | train loss: 3.71 | val loss: 3.87 | perplexity: 47.82 | lr: 6.26e-04 | norm: 0.4098 | dt: 14312.4366ms | tok/sec: 27.4737\n",
      "step 1467 | train loss: 4.07 | val loss: 3.86 | perplexity: 47.67 | lr: 6.26e-04 | norm: 0.4275 | dt: 14315.4633ms | tok/sec: 27.4679\n",
      "step 1468 | train loss: 3.89 | val loss: 3.87 | perplexity: 47.93 | lr: 6.25e-04 | norm: 0.3083 | dt: 14308.0037ms | tok/sec: 27.4822\n",
      "step 1469 | train loss: 3.92 | val loss: 3.87 | perplexity: 47.87 | lr: 6.24e-04 | norm: 0.3316 | dt: 14312.2125ms | tok/sec: 27.4742\n",
      "step 1470 | train loss: 3.43 | val loss: 3.87 | perplexity: 47.85 | lr: 6.23e-04 | norm: 0.3976 | dt: 14300.0822ms | tok/sec: 27.4975\n",
      "step 1471 | train loss: 4.19 | val loss: 3.87 | perplexity: 47.82 | lr: 6.22e-04 | norm: 0.3502 | dt: 14282.9754ms | tok/sec: 27.5304\n",
      "step 1472 | train loss: 3.90 | val loss: 3.87 | perplexity: 47.74 | lr: 6.21e-04 | norm: 0.3763 | dt: 14320.3888ms | tok/sec: 27.4585\n",
      "step 1473 | train loss: 4.34 | val loss: 3.86 | perplexity: 47.64 | lr: 6.21e-04 | norm: 0.3644 | dt: 14408.7224ms | tok/sec: 27.2901\n",
      "step 1474 | train loss: 3.82 | val loss: 3.86 | perplexity: 47.47 | lr: 6.20e-04 | norm: 0.3004 | dt: 14338.6128ms | tok/sec: 27.4236\n",
      "step 1475 | train loss: 3.80 | val loss: 3.85 | perplexity: 47.07 | lr: 6.19e-04 | norm: 0.4177 | dt: 14356.9169ms | tok/sec: 27.3886\n",
      "step 1476 | train loss: 3.90 | val loss: 3.85 | perplexity: 47.16 | lr: 6.18e-04 | norm: 0.3646 | dt: 14345.0565ms | tok/sec: 27.4113\n",
      "step 1477 | train loss: 3.95 | val loss: 3.85 | perplexity: 47.13 | lr: 6.17e-04 | norm: 0.3499 | dt: 14368.1471ms | tok/sec: 27.3672\n",
      "step 1478 | train loss: 3.80 | val loss: 3.86 | perplexity: 47.28 | lr: 6.16e-04 | norm: 0.4645 | dt: 14348.2926ms | tok/sec: 27.4051\n",
      "step 1479 | train loss: 3.99 | val loss: 3.87 | perplexity: 47.90 | lr: 6.16e-04 | norm: 0.4843 | dt: 14353.7085ms | tok/sec: 27.3947\n",
      "step 1480 | train loss: 4.06 | val loss: 3.87 | perplexity: 48.07 | lr: 6.15e-04 | norm: 0.5299 | dt: 14399.0226ms | tok/sec: 27.3085\n",
      "step 1481 | train loss: 4.03 | val loss: 3.87 | perplexity: 48.16 | lr: 6.14e-04 | norm: 0.5497 | dt: 14325.9230ms | tok/sec: 27.4479\n",
      "step 1482 | train loss: 4.54 | val loss: 3.88 | perplexity: 48.41 | lr: 6.13e-04 | norm: 0.4539 | dt: 14362.9148ms | tok/sec: 27.3772\n",
      "step 1483 | train loss: 3.84 | val loss: 3.87 | perplexity: 48.06 | lr: 6.12e-04 | norm: 0.3909 | dt: 14374.9964ms | tok/sec: 27.3542\n",
      "step 1484 | train loss: 3.91 | val loss: 3.86 | perplexity: 47.70 | lr: 6.11e-04 | norm: 0.3924 | dt: 14385.6153ms | tok/sec: 27.3340\n",
      "step 1485 | train loss: 4.00 | val loss: 3.86 | perplexity: 47.55 | lr: 6.11e-04 | norm: 0.3688 | dt: 14354.6579ms | tok/sec: 27.3929\n",
      "step 1486 | train loss: 4.00 | val loss: 3.86 | perplexity: 47.45 | lr: 6.10e-04 | norm: 0.3237 | dt: 14376.3297ms | tok/sec: 27.3516\n",
      "step 1487 | train loss: 3.97 | val loss: 3.86 | perplexity: 47.35 | lr: 6.09e-04 | norm: 0.3238 | dt: 14325.3105ms | tok/sec: 27.4490\n",
      "step 1488 | train loss: 3.99 | val loss: 3.86 | perplexity: 47.33 | lr: 6.08e-04 | norm: 0.3955 | dt: 14313.7887ms | tok/sec: 27.4711\n",
      "step 1489 | train loss: 4.07 | val loss: 3.86 | perplexity: 47.49 | lr: 6.07e-04 | norm: 0.3208 | dt: 14353.1728ms | tok/sec: 27.3958\n",
      "step 1490 | train loss: 3.63 | val loss: 3.86 | perplexity: 47.50 | lr: 6.06e-04 | norm: 0.4288 | dt: 14328.8300ms | tok/sec: 27.4423\n",
      "step 1491 | train loss: 3.79 | val loss: 3.85 | perplexity: 47.17 | lr: 6.06e-04 | norm: 0.3967 | dt: 14333.3716ms | tok/sec: 27.4336\n",
      "step 1492 | train loss: 3.94 | val loss: 3.85 | perplexity: 47.10 | lr: 6.05e-04 | norm: 0.4301 | dt: 14393.2178ms | tok/sec: 27.3195\n",
      "step 1493 | train loss: 4.02 | val loss: 3.85 | perplexity: 47.13 | lr: 6.04e-04 | norm: 0.4216 | dt: 14337.3544ms | tok/sec: 27.4260\n",
      "step 1494 | train loss: 3.74 | val loss: 3.86 | perplexity: 47.41 | lr: 6.03e-04 | norm: 0.4300 | dt: 14320.2908ms | tok/sec: 27.4587\n",
      "step 1495 | train loss: 4.04 | val loss: 3.87 | perplexity: 47.77 | lr: 6.02e-04 | norm: 0.4172 | dt: 14318.7625ms | tok/sec: 27.4616\n",
      "step 1496 | train loss: 4.06 | val loss: 3.87 | perplexity: 47.90 | lr: 6.01e-04 | norm: 0.4024 | dt: 14315.3942ms | tok/sec: 27.4681\n",
      "step 1497 | train loss: 4.17 | val loss: 3.86 | perplexity: 47.29 | lr: 6.01e-04 | norm: 0.4629 | dt: 14320.2848ms | tok/sec: 27.4587\n",
      "step 1498 | train loss: 4.05 | val loss: 3.85 | perplexity: 47.14 | lr: 6.00e-04 | norm: 0.4279 | dt: 14387.4412ms | tok/sec: 27.3305\n",
      "step 1499 | train loss: 4.24 | val loss: 3.85 | perplexity: 47.06 | lr: 5.99e-04 | norm: 0.5072 | dt: 14320.8251ms | tok/sec: 27.4576\n",
      "step 1500 | train loss: 3.77 | val loss: 3.85 | perplexity: 46.96 | lr: 5.98e-04 | norm: 0.3974 | dt: 14345.1207ms | tok/sec: 27.4111\n",
      "step 1501 | train loss: 3.90 | val loss: 3.84 | perplexity: 46.54 | lr: 5.97e-04 | norm: 0.3881 | dt: 14378.7022ms | tok/sec: 27.3471\n",
      "step 1502 | train loss: 4.12 | val loss: 3.84 | perplexity: 46.54 | lr: 5.96e-04 | norm: 0.3735 | dt: 14341.4185ms | tok/sec: 27.4182\n",
      "step 1503 | train loss: 3.87 | val loss: 3.84 | perplexity: 46.74 | lr: 5.96e-04 | norm: 0.4140 | dt: 14319.3178ms | tok/sec: 27.4605\n",
      "step 1504 | train loss: 4.16 | val loss: 3.85 | perplexity: 46.80 | lr: 5.95e-04 | norm: 0.3627 | dt: 14340.0040ms | tok/sec: 27.4209\n",
      "step 1505 | train loss: 3.88 | val loss: 3.85 | perplexity: 46.79 | lr: 5.94e-04 | norm: 0.4267 | dt: 14322.5048ms | tok/sec: 27.4544\n",
      "step 1506 | train loss: 4.19 | val loss: 3.85 | perplexity: 46.89 | lr: 5.93e-04 | norm: 0.3656 | dt: 14366.9496ms | tok/sec: 27.3695\n",
      "step 1507 | train loss: 3.98 | val loss: 3.84 | perplexity: 46.63 | lr: 5.92e-04 | norm: 0.4070 | dt: 14357.7199ms | tok/sec: 27.3871\n",
      "step 1508 | train loss: 3.90 | val loss: 3.84 | perplexity: 46.68 | lr: 5.91e-04 | norm: 0.3652 | dt: 14355.8190ms | tok/sec: 27.3907\n",
      "step 1509 | train loss: 4.27 | val loss: 3.84 | perplexity: 46.53 | lr: 5.91e-04 | norm: 0.4421 | dt: 14333.6985ms | tok/sec: 27.4330\n",
      "step 1510 | train loss: 3.47 | val loss: 3.84 | perplexity: 46.70 | lr: 5.90e-04 | norm: 0.3740 | dt: 14332.6581ms | tok/sec: 27.4350\n",
      "step 1511 | train loss: 4.17 | val loss: 3.84 | perplexity: 46.41 | lr: 5.89e-04 | norm: 0.4528 | dt: 14300.7886ms | tok/sec: 27.4961\n",
      "step 1512 | train loss: 4.07 | val loss: 3.85 | perplexity: 46.94 | lr: 5.88e-04 | norm: 0.4255 | dt: 14336.8697ms | tok/sec: 27.4269\n",
      "step 1513 | train loss: 4.34 | val loss: 3.86 | perplexity: 47.33 | lr: 5.87e-04 | norm: 0.7445 | dt: 14394.3839ms | tok/sec: 27.3173\n",
      "step 1514 | train loss: 4.17 | val loss: 3.85 | perplexity: 47.11 | lr: 5.86e-04 | norm: 0.5313 | dt: 14310.3416ms | tok/sec: 27.4778\n",
      "step 1515 | train loss: 3.99 | val loss: 3.85 | perplexity: 46.99 | lr: 5.86e-04 | norm: 0.4450 | dt: 14337.1522ms | tok/sec: 27.4264\n",
      "step 1516 | train loss: 4.44 | val loss: 3.85 | perplexity: 47.06 | lr: 5.85e-04 | norm: 0.4777 | dt: 14434.0284ms | tok/sec: 27.2423\n",
      "step 1517 | train loss: 4.21 | val loss: 3.85 | perplexity: 47.12 | lr: 5.84e-04 | norm: 0.5871 | dt: 14312.0675ms | tok/sec: 27.4744\n",
      "step 1518 | train loss: 3.85 | val loss: 3.85 | perplexity: 47.18 | lr: 5.83e-04 | norm: 0.3871 | dt: 14318.2857ms | tok/sec: 27.4625\n",
      "step 1519 | train loss: 4.20 | val loss: 3.86 | perplexity: 47.25 | lr: 5.82e-04 | norm: 0.4126 | dt: 14289.4535ms | tok/sec: 27.5179\n",
      "step 1520 | train loss: 4.01 | val loss: 3.84 | perplexity: 46.52 | lr: 5.81e-04 | norm: 0.5253 | dt: 14300.4808ms | tok/sec: 27.4967\n",
      "step 1521 | train loss: 4.09 | val loss: 3.84 | perplexity: 46.62 | lr: 5.81e-04 | norm: 0.4152 | dt: 14321.4796ms | tok/sec: 27.4564\n",
      "step 1522 | train loss: 3.90 | val loss: 3.85 | perplexity: 46.84 | lr: 5.80e-04 | norm: 0.4958 | dt: 14316.2026ms | tok/sec: 27.4665\n",
      "step 1523 | train loss: 4.16 | val loss: 3.84 | perplexity: 46.49 | lr: 5.79e-04 | norm: 0.4210 | dt: 14300.9796ms | tok/sec: 27.4957\n",
      "step 1524 | train loss: 4.90 | val loss: 3.84 | perplexity: 46.58 | lr: 5.78e-04 | norm: 0.7046 | dt: 14296.7029ms | tok/sec: 27.5040\n",
      "step 1525 | train loss: 3.83 | val loss: 3.84 | perplexity: 46.56 | lr: 5.77e-04 | norm: 0.3609 | dt: 14338.1267ms | tok/sec: 27.4245\n",
      "step 1526 | train loss: 4.15 | val loss: 3.84 | perplexity: 46.48 | lr: 5.76e-04 | norm: 0.3558 | dt: 14321.4314ms | tok/sec: 27.4565\n",
      "step 1527 | train loss: 4.09 | val loss: 3.84 | perplexity: 46.48 | lr: 5.76e-04 | norm: 0.3626 | dt: 14338.1622ms | tok/sec: 27.4244\n",
      "step 1528 | train loss: 3.87 | val loss: 3.84 | perplexity: 46.46 | lr: 5.75e-04 | norm: 0.3271 | dt: 14321.0282ms | tok/sec: 27.4572\n",
      "step 1529 | train loss: 4.09 | val loss: 3.84 | perplexity: 46.30 | lr: 5.74e-04 | norm: 0.4546 | dt: 14319.5562ms | tok/sec: 27.4601\n",
      "step 1530 | train loss: 3.97 | val loss: 3.83 | perplexity: 46.15 | lr: 5.73e-04 | norm: 0.3318 | dt: 14315.2015ms | tok/sec: 27.4684\n",
      "step 1531 | train loss: 3.65 | val loss: 3.82 | perplexity: 45.82 | lr: 5.72e-04 | norm: 0.4779 | dt: 14325.3365ms | tok/sec: 27.4490\n",
      "step 1532 | train loss: 3.70 | val loss: 3.82 | perplexity: 45.77 | lr: 5.71e-04 | norm: 0.3206 | dt: 14338.3794ms | tok/sec: 27.4240\n",
      "step 1533 | train loss: 3.94 | val loss: 3.83 | perplexity: 45.95 | lr: 5.71e-04 | norm: 0.3447 | dt: 14319.6981ms | tok/sec: 27.4598\n",
      "step 1534 | train loss: 3.78 | val loss: 3.83 | perplexity: 45.92 | lr: 5.70e-04 | norm: 0.3210 | dt: 14319.5157ms | tok/sec: 27.4601\n",
      "step 1535 | train loss: 4.43 | val loss: 3.83 | perplexity: 46.26 | lr: 5.69e-04 | norm: 0.7227 | dt: 14304.4217ms | tok/sec: 27.4891\n",
      "step 1536 | train loss: 3.98 | val loss: 3.85 | perplexity: 46.97 | lr: 5.68e-04 | norm: 0.4035 | dt: 14297.1828ms | tok/sec: 27.5030\n",
      "step 1537 | train loss: 3.90 | val loss: 3.84 | perplexity: 46.55 | lr: 5.67e-04 | norm: 0.5419 | dt: 14324.7998ms | tok/sec: 27.4500\n",
      "step 1538 | train loss: 3.93 | val loss: 3.85 | perplexity: 46.88 | lr: 5.66e-04 | norm: 0.3748 | dt: 14315.3100ms | tok/sec: 27.4682\n",
      "step 1539 | train loss: 4.29 | val loss: 3.85 | perplexity: 47.08 | lr: 5.66e-04 | norm: 0.4750 | dt: 14305.0826ms | tok/sec: 27.4879\n",
      "step 1540 | train loss: 3.79 | val loss: 3.86 | perplexity: 47.40 | lr: 5.65e-04 | norm: 0.4758 | dt: 14308.8541ms | tok/sec: 27.4806\n",
      "step 1541 | train loss: 4.10 | val loss: 3.86 | perplexity: 47.63 | lr: 5.64e-04 | norm: 0.4112 | dt: 14343.0057ms | tok/sec: 27.4152\n",
      "step 1542 | train loss: 3.97 | val loss: 3.84 | perplexity: 46.45 | lr: 5.63e-04 | norm: 0.4955 | dt: 14310.3013ms | tok/sec: 27.4778\n",
      "step 1543 | train loss: 4.59 | val loss: 3.84 | perplexity: 46.57 | lr: 5.62e-04 | norm: 1.6430 | dt: 14316.1325ms | tok/sec: 27.4666\n",
      "step 1544 | train loss: 3.72 | val loss: 3.84 | perplexity: 46.71 | lr: 5.61e-04 | norm: 0.4085 | dt: 14299.8531ms | tok/sec: 27.4979\n",
      "step 1545 | train loss: 4.03 | val loss: 3.84 | perplexity: 46.37 | lr: 5.60e-04 | norm: 0.4542 | dt: 14323.8060ms | tok/sec: 27.4519\n",
      "step 1546 | train loss: 3.98 | val loss: 3.84 | perplexity: 46.30 | lr: 5.60e-04 | norm: 0.3806 | dt: 14571.4221ms | tok/sec: 26.9854\n",
      "step 1547 | train loss: 3.78 | val loss: 3.83 | perplexity: 46.02 | lr: 5.59e-04 | norm: 0.4080 | dt: 13869.7078ms | tok/sec: 28.3507\n",
      "step 1548 | train loss: 3.83 | val loss: 3.82 | perplexity: 45.78 | lr: 5.58e-04 | norm: 0.3562 | dt: 13668.2467ms | tok/sec: 28.7686\n",
      "step 1549 | train loss: 3.68 | val loss: 3.83 | perplexity: 46.05 | lr: 5.57e-04 | norm: 0.4340 | dt: 13644.8901ms | tok/sec: 28.8178\n",
      "step 1550 | train loss: 4.13 | val loss: 3.83 | perplexity: 46.02 | lr: 5.56e-04 | norm: 0.4041 | dt: 13646.7261ms | tok/sec: 28.8139\n",
      "step 1551 | train loss: 4.08 | val loss: 3.82 | perplexity: 45.65 | lr: 5.55e-04 | norm: 0.4657 | dt: 13793.7813ms | tok/sec: 28.5068\n",
      "step 1552 | train loss: 3.98 | val loss: 3.80 | perplexity: 44.90 | lr: 5.55e-04 | norm: 0.4256 | dt: 13709.4529ms | tok/sec: 28.6821\n",
      "step 1553 | train loss: 3.94 | val loss: 3.79 | perplexity: 44.40 | lr: 5.54e-04 | norm: 0.3639 | dt: 13878.0491ms | tok/sec: 28.3337\n",
      "step 1554 | train loss: 3.82 | val loss: 3.78 | perplexity: 44.01 | lr: 5.53e-04 | norm: 0.3511 | dt: 14264.5094ms | tok/sec: 27.5660\n",
      "step 1555 | train loss: 4.01 | val loss: 3.78 | perplexity: 43.66 | lr: 5.52e-04 | norm: 0.3986 | dt: 14258.2295ms | tok/sec: 27.5782\n",
      "step 1556 | train loss: 3.80 | val loss: 3.77 | perplexity: 43.50 | lr: 5.51e-04 | norm: 0.3889 | dt: 14246.7508ms | tok/sec: 27.6004\n",
      "step 1557 | train loss: 3.85 | val loss: 3.77 | perplexity: 43.38 | lr: 5.50e-04 | norm: 0.3651 | dt: 14237.0150ms | tok/sec: 27.6193\n",
      "step 1558 | train loss: 3.92 | val loss: 3.77 | perplexity: 43.46 | lr: 5.50e-04 | norm: 0.3673 | dt: 14274.1201ms | tok/sec: 27.5475\n",
      "step 1559 | train loss: 4.28 | val loss: 3.78 | perplexity: 43.70 | lr: 5.49e-04 | norm: 0.4198 | dt: 14239.1775ms | tok/sec: 27.6151\n",
      "step 1560 | train loss: 3.93 | val loss: 3.78 | perplexity: 43.84 | lr: 5.48e-04 | norm: 0.3407 | dt: 14248.4488ms | tok/sec: 27.5971\n",
      "step 1561 | train loss: 3.99 | val loss: 3.78 | perplexity: 43.89 | lr: 5.47e-04 | norm: 0.3766 | dt: 14271.0750ms | tok/sec: 27.5534\n",
      "step 1562 | train loss: 3.77 | val loss: 3.78 | perplexity: 43.78 | lr: 5.46e-04 | norm: 0.4271 | dt: 14256.3243ms | tok/sec: 27.5819\n",
      "step 1563 | train loss: 3.87 | val loss: 3.78 | perplexity: 43.76 | lr: 5.45e-04 | norm: 0.3392 | dt: 14231.6077ms | tok/sec: 27.6298\n",
      "step 1564 | train loss: 3.87 | val loss: 3.78 | perplexity: 43.86 | lr: 5.45e-04 | norm: 0.3456 | dt: 14264.1380ms | tok/sec: 27.5668\n",
      "step 1565 | train loss: 4.14 | val loss: 3.78 | perplexity: 43.76 | lr: 5.44e-04 | norm: 0.3164 | dt: 14219.9233ms | tok/sec: 27.6525\n",
      "step 1566 | train loss: 3.86 | val loss: 3.77 | perplexity: 43.24 | lr: 5.43e-04 | norm: 0.3943 | dt: 14232.7404ms | tok/sec: 27.6276\n",
      "step 1567 | train loss: 3.90 | val loss: 3.76 | perplexity: 42.99 | lr: 5.42e-04 | norm: 0.3115 | dt: 14259.6717ms | tok/sec: 27.5754\n",
      "step 1568 | train loss: 4.24 | val loss: 3.76 | perplexity: 42.86 | lr: 5.41e-04 | norm: 0.3672 | dt: 14267.7648ms | tok/sec: 27.5597\n",
      "step 1569 | train loss: 4.07 | val loss: 3.76 | perplexity: 42.88 | lr: 5.40e-04 | norm: 0.3571 | dt: 14254.6289ms | tok/sec: 27.5851\n",
      "step 1570 | train loss: 3.82 | val loss: 3.76 | perplexity: 43.01 | lr: 5.40e-04 | norm: 0.3510 | dt: 14246.9161ms | tok/sec: 27.6001\n",
      "step 1571 | train loss: 3.86 | val loss: 3.77 | perplexity: 43.19 | lr: 5.39e-04 | norm: 0.4365 | dt: 14251.2786ms | tok/sec: 27.5916\n",
      "step 1572 | train loss: 4.76 | val loss: 3.78 | perplexity: 43.65 | lr: 5.38e-04 | norm: 0.9465 | dt: 14242.4078ms | tok/sec: 27.6088\n",
      "step 1573 | train loss: 3.97 | val loss: 3.77 | perplexity: 43.58 | lr: 5.37e-04 | norm: 0.4217 | dt: 14234.6652ms | tok/sec: 27.6238\n",
      "step 1574 | train loss: 3.87 | val loss: 3.78 | perplexity: 43.60 | lr: 5.36e-04 | norm: 0.3668 | dt: 14227.2670ms | tok/sec: 27.6382\n",
      "step 1575 | train loss: 3.85 | val loss: 3.78 | perplexity: 43.61 | lr: 5.35e-04 | norm: 0.4133 | dt: 14200.4721ms | tok/sec: 27.6903\n",
      "step 1576 | train loss: 3.99 | val loss: 3.77 | perplexity: 43.35 | lr: 5.34e-04 | norm: 0.5307 | dt: 14194.0167ms | tok/sec: 27.7029\n",
      "step 1577 | train loss: 3.97 | val loss: 3.77 | perplexity: 43.42 | lr: 5.34e-04 | norm: 0.3871 | dt: 14201.5111ms | tok/sec: 27.6883\n",
      "step 1578 | train loss: 4.16 | val loss: 3.77 | perplexity: 43.43 | lr: 5.33e-04 | norm: 0.4364 | dt: 14190.8770ms | tok/sec: 27.7091\n",
      "step 1579 | train loss: 4.15 | val loss: 3.77 | perplexity: 43.50 | lr: 5.32e-04 | norm: 0.3704 | dt: 14222.4362ms | tok/sec: 27.6476\n",
      "step 1580 | train loss: 3.90 | val loss: 3.77 | perplexity: 43.31 | lr: 5.31e-04 | norm: 0.4122 | dt: 14207.2723ms | tok/sec: 27.6771\n",
      "step 1581 | train loss: 3.97 | val loss: 3.77 | perplexity: 43.30 | lr: 5.30e-04 | norm: 0.3603 | dt: 14187.6903ms | tok/sec: 27.7153\n",
      "step 1582 | train loss: 4.22 | val loss: 3.77 | perplexity: 43.46 | lr: 5.29e-04 | norm: 0.3660 | dt: 14201.8690ms | tok/sec: 27.6876\n",
      "step 1583 | train loss: 4.11 | val loss: 3.77 | perplexity: 43.52 | lr: 5.29e-04 | norm: 0.4890 | dt: 14320.3146ms | tok/sec: 27.4586\n",
      "step 1584 | train loss: 4.26 | val loss: 3.78 | perplexity: 43.75 | lr: 5.28e-04 | norm: 0.4579 | dt: 14360.1451ms | tok/sec: 27.3825\n",
      "step 1585 | train loss: 4.15 | val loss: 3.78 | perplexity: 43.86 | lr: 5.27e-04 | norm: 0.4707 | dt: 14247.0746ms | tok/sec: 27.5998\n",
      "step 1586 | train loss: 3.94 | val loss: 3.78 | perplexity: 43.60 | lr: 5.26e-04 | norm: 0.4484 | dt: 14242.9917ms | tok/sec: 27.6077\n",
      "step 1587 | train loss: 4.23 | val loss: 3.78 | perplexity: 43.65 | lr: 5.25e-04 | norm: 0.4621 | dt: 14248.3962ms | tok/sec: 27.5972\n",
      "step 1588 | train loss: 3.74 | val loss: 3.77 | perplexity: 43.44 | lr: 5.24e-04 | norm: 0.4094 | dt: 14252.9292ms | tok/sec: 27.5884\n",
      "step 1589 | train loss: 4.18 | val loss: 3.77 | perplexity: 43.25 | lr: 5.24e-04 | norm: 0.3855 | dt: 14269.7277ms | tok/sec: 27.5560\n",
      "step 1590 | train loss: 4.32 | val loss: 3.77 | perplexity: 43.43 | lr: 5.23e-04 | norm: 0.4090 | dt: 14270.2141ms | tok/sec: 27.5550\n",
      "step 1591 | train loss: 4.20 | val loss: 3.77 | perplexity: 43.56 | lr: 5.22e-04 | norm: 0.4173 | dt: 14278.1861ms | tok/sec: 27.5396\n",
      "step 1592 | train loss: 3.91 | val loss: 3.77 | perplexity: 43.57 | lr: 5.21e-04 | norm: 0.3923 | dt: 14273.0613ms | tok/sec: 27.5495\n",
      "step 1593 | train loss: 4.32 | val loss: 3.77 | perplexity: 43.59 | lr: 5.20e-04 | norm: 0.4121 | dt: 14269.5529ms | tok/sec: 27.5563\n",
      "step 1594 | train loss: 4.07 | val loss: 3.77 | perplexity: 43.42 | lr: 5.19e-04 | norm: 0.3821 | dt: 14265.9516ms | tok/sec: 27.5633\n",
      "step 1595 | train loss: 4.08 | val loss: 3.77 | perplexity: 43.44 | lr: 5.19e-04 | norm: 0.3378 | dt: 14251.7049ms | tok/sec: 27.5908\n",
      "step 1596 | train loss: 3.75 | val loss: 3.77 | perplexity: 43.47 | lr: 5.18e-04 | norm: 0.3423 | dt: 14256.4726ms | tok/sec: 27.5816\n",
      "step 1597 | train loss: 4.13 | val loss: 3.77 | perplexity: 43.20 | lr: 5.17e-04 | norm: 0.4421 | dt: 14198.5931ms | tok/sec: 27.6940\n",
      "step 1598 | train loss: 3.88 | val loss: 3.77 | perplexity: 43.37 | lr: 5.16e-04 | norm: 0.3891 | dt: 14215.4279ms | tok/sec: 27.6612\n",
      "step 1599 | train loss: 3.83 | val loss: 3.77 | perplexity: 43.42 | lr: 5.15e-04 | norm: 0.4334 | dt: 14237.4668ms | tok/sec: 27.6184\n",
      "step 1600 | train loss: 3.99 | val loss: 3.77 | perplexity: 43.44 | lr: 5.14e-04 | norm: 0.3757 | dt: 14229.6400ms | tok/sec: 27.6336\n",
      "step 1601 | train loss: 4.01 | val loss: 3.78 | perplexity: 43.67 | lr: 5.14e-04 | norm: 0.4173 | dt: 14189.4081ms | tok/sec: 27.7119\n",
      "step 1602 | train loss: 3.52 | val loss: 3.78 | perplexity: 43.68 | lr: 5.13e-04 | norm: 0.3755 | dt: 14211.3302ms | tok/sec: 27.6692\n",
      "step 1603 | train loss: 3.89 | val loss: 3.77 | perplexity: 43.46 | lr: 5.12e-04 | norm: 0.4894 | dt: 14206.7993ms | tok/sec: 27.6780\n",
      "step 1604 | train loss: 4.01 | val loss: 3.77 | perplexity: 43.58 | lr: 5.11e-04 | norm: 0.4095 | dt: 14183.4335ms | tok/sec: 27.7236\n",
      "step 1605 | train loss: 3.92 | val loss: 3.77 | perplexity: 43.38 | lr: 5.10e-04 | norm: 0.4586 | dt: 14206.3432ms | tok/sec: 27.6789\n",
      "step 1606 | train loss: 4.01 | val loss: 3.77 | perplexity: 43.31 | lr: 5.09e-04 | norm: 0.3441 | dt: 14189.5132ms | tok/sec: 27.7117\n",
      "step 1607 | train loss: 4.05 | val loss: 3.77 | perplexity: 43.20 | lr: 5.09e-04 | norm: 0.4606 | dt: 14185.7269ms | tok/sec: 27.7191\n",
      "step 1608 | train loss: 3.94 | val loss: 3.77 | perplexity: 43.20 | lr: 5.08e-04 | norm: 0.4165 | dt: 14196.6536ms | tok/sec: 27.6978\n",
      "step 1609 | train loss: 3.92 | val loss: 3.78 | perplexity: 43.62 | lr: 5.07e-04 | norm: 0.3145 | dt: 14202.7357ms | tok/sec: 27.6859\n",
      "step 1610 | train loss: 3.81 | val loss: 3.78 | perplexity: 43.64 | lr: 5.06e-04 | norm: 0.4045 | dt: 14190.6948ms | tok/sec: 27.7094\n",
      "step 1611 | train loss: 3.77 | val loss: 3.77 | perplexity: 43.32 | lr: 5.05e-04 | norm: 0.4216 | dt: 14210.5501ms | tok/sec: 27.6707\n",
      "step 1612 | train loss: 3.96 | val loss: 3.76 | perplexity: 43.04 | lr: 5.04e-04 | norm: 0.3641 | dt: 14216.5523ms | tok/sec: 27.6590\n",
      "step 1613 | train loss: 3.99 | val loss: 3.76 | perplexity: 43.01 | lr: 5.04e-04 | norm: 0.3392 | dt: 14228.0347ms | tok/sec: 27.6367\n",
      "step 1614 | train loss: 3.73 | val loss: 3.76 | perplexity: 43.12 | lr: 5.03e-04 | norm: 0.3896 | dt: 14241.4267ms | tok/sec: 27.6107\n",
      "step 1615 | train loss: 3.93 | val loss: 3.77 | perplexity: 43.27 | lr: 5.02e-04 | norm: 0.3402 | dt: 14207.6819ms | tok/sec: 27.6763\n",
      "step 1616 | train loss: 3.97 | val loss: 3.77 | perplexity: 43.35 | lr: 5.01e-04 | norm: 0.4007 | dt: 14191.9785ms | tok/sec: 27.7069\n",
      "step 1617 | train loss: 3.92 | val loss: 3.77 | perplexity: 43.34 | lr: 5.00e-04 | norm: 0.3322 | dt: 14161.4394ms | tok/sec: 27.7667\n",
      "step 1618 | train loss: 3.86 | val loss: 3.77 | perplexity: 43.31 | lr: 4.99e-04 | norm: 0.5070 | dt: 14195.5035ms | tok/sec: 27.7000\n",
      "step 1619 | train loss: 3.84 | val loss: 3.76 | perplexity: 43.12 | lr: 4.99e-04 | norm: 0.4033 | dt: 14178.3791ms | tok/sec: 27.7335\n",
      "step 1620 | train loss: 4.04 | val loss: 3.76 | perplexity: 43.02 | lr: 4.98e-04 | norm: 0.3806 | dt: 14193.4311ms | tok/sec: 27.7041\n",
      "step 1621 | train loss: 4.18 | val loss: 3.76 | perplexity: 43.04 | lr: 4.97e-04 | norm: 0.4638 | dt: 14218.3084ms | tok/sec: 27.6556\n",
      "step 1622 | train loss: 3.85 | val loss: 3.77 | perplexity: 43.20 | lr: 4.96e-04 | norm: 0.4427 | dt: 14162.5695ms | tok/sec: 27.7645\n",
      "step 1623 | train loss: 3.64 | val loss: 3.77 | perplexity: 43.50 | lr: 4.95e-04 | norm: 0.3756 | dt: 14186.0650ms | tok/sec: 27.7185\n",
      "step 1624 | train loss: 3.68 | val loss: 3.77 | perplexity: 43.59 | lr: 4.94e-04 | norm: 0.4358 | dt: 14180.5761ms | tok/sec: 27.7292\n",
      "step 1625 | train loss: 3.74 | val loss: 3.78 | perplexity: 43.79 | lr: 4.94e-04 | norm: 0.3887 | dt: 14176.5463ms | tok/sec: 27.7371\n",
      "step 1626 | train loss: 3.72 | val loss: 3.78 | perplexity: 43.86 | lr: 4.93e-04 | norm: 0.3988 | dt: 14206.1937ms | tok/sec: 27.6792\n",
      "step 1627 | train loss: 3.71 | val loss: 3.78 | perplexity: 43.71 | lr: 4.92e-04 | norm: 0.4575 | dt: 14186.5482ms | tok/sec: 27.7175\n",
      "step 1628 | train loss: 4.31 | val loss: 3.78 | perplexity: 43.83 | lr: 4.91e-04 | norm: 0.4462 | dt: 14176.5590ms | tok/sec: 27.7371\n",
      "step 1629 | train loss: 4.06 | val loss: 3.78 | perplexity: 43.86 | lr: 4.90e-04 | norm: 0.4014 | dt: 14205.2083ms | tok/sec: 27.6811\n",
      "step 1630 | train loss: 3.78 | val loss: 3.78 | perplexity: 43.63 | lr: 4.89e-04 | norm: 0.3954 | dt: 14190.5208ms | tok/sec: 27.7098\n",
      "step 1631 | train loss: 4.02 | val loss: 3.77 | perplexity: 43.48 | lr: 4.89e-04 | norm: 0.3418 | dt: 14196.0449ms | tok/sec: 27.6990\n",
      "step 1632 | train loss: 4.13 | val loss: 3.77 | perplexity: 43.49 | lr: 4.88e-04 | norm: 0.8648 | dt: 14198.3097ms | tok/sec: 27.6946\n",
      "step 1633 | train loss: 3.93 | val loss: 3.77 | perplexity: 43.22 | lr: 4.87e-04 | norm: 0.4449 | dt: 14204.2837ms | tok/sec: 27.6829\n",
      "step 1634 | train loss: 3.51 | val loss: 3.77 | perplexity: 43.20 | lr: 4.86e-04 | norm: 0.4356 | dt: 14191.0644ms | tok/sec: 27.7087\n",
      "step 1635 | train loss: 4.05 | val loss: 3.77 | perplexity: 43.54 | lr: 4.85e-04 | norm: 0.4299 | dt: 14192.4183ms | tok/sec: 27.7061\n",
      "step 1636 | train loss: 3.94 | val loss: 3.77 | perplexity: 43.47 | lr: 4.84e-04 | norm: 0.3733 | dt: 14201.4086ms | tok/sec: 27.6885\n",
      "step 1637 | train loss: 3.99 | val loss: 3.77 | perplexity: 43.25 | lr: 4.84e-04 | norm: 0.3910 | dt: 14185.8063ms | tok/sec: 27.7190\n",
      "step 1638 | train loss: 3.80 | val loss: 3.77 | perplexity: 43.36 | lr: 4.83e-04 | norm: 0.3118 | dt: 14177.6407ms | tok/sec: 27.7349\n",
      "step 1639 | train loss: 3.73 | val loss: 3.77 | perplexity: 43.31 | lr: 4.82e-04 | norm: 0.3524 | dt: 14218.7917ms | tok/sec: 27.6547\n",
      "step 1640 | train loss: 3.64 | val loss: 3.77 | perplexity: 43.20 | lr: 4.81e-04 | norm: 0.3486 | dt: 14203.2099ms | tok/sec: 27.6850\n",
      "step 1641 | train loss: 4.02 | val loss: 3.77 | perplexity: 43.18 | lr: 4.80e-04 | norm: 0.3523 | dt: 14194.9058ms | tok/sec: 27.7012\n",
      "step 1642 | train loss: 3.70 | val loss: 3.77 | perplexity: 43.22 | lr: 4.79e-04 | norm: 0.4396 | dt: 14189.8489ms | tok/sec: 27.7111\n",
      "step 1643 | train loss: 3.93 | val loss: 3.76 | perplexity: 43.11 | lr: 4.79e-04 | norm: 0.3630 | dt: 14197.7885ms | tok/sec: 27.6956\n",
      "step 1644 | train loss: 4.08 | val loss: 3.76 | perplexity: 42.95 | lr: 4.78e-04 | norm: 0.4080 | dt: 14172.4970ms | tok/sec: 27.7450\n",
      "step 1645 | train loss: 3.78 | val loss: 3.76 | perplexity: 42.96 | lr: 4.77e-04 | norm: 0.3646 | dt: 14182.5128ms | tok/sec: 27.7254\n",
      "step 1646 | train loss: 3.74 | val loss: 3.76 | perplexity: 42.98 | lr: 4.76e-04 | norm: 0.4621 | dt: 14195.1108ms | tok/sec: 27.7008\n",
      "step 1647 | train loss: 3.75 | val loss: 3.76 | perplexity: 42.82 | lr: 4.75e-04 | norm: 0.3672 | dt: 14222.4422ms | tok/sec: 27.6476\n",
      "step 1648 | train loss: 3.80 | val loss: 3.75 | perplexity: 42.71 | lr: 4.74e-04 | norm: 0.3127 | dt: 14330.9565ms | tok/sec: 27.4382\n",
      "step 1649 | train loss: 4.38 | val loss: 3.76 | perplexity: 42.95 | lr: 4.74e-04 | norm: 0.5062 | dt: 14317.0137ms | tok/sec: 27.4649\n",
      "step 1650 | train loss: 4.08 | val loss: 3.77 | perplexity: 43.34 | lr: 4.73e-04 | norm: 0.3766 | dt: 14321.5718ms | tok/sec: 27.4562\n",
      "step 1651 | train loss: 3.98 | val loss: 3.77 | perplexity: 43.50 | lr: 4.72e-04 | norm: 0.3930 | dt: 14286.8328ms | tok/sec: 27.5230\n",
      "step 1652 | train loss: 3.98 | val loss: 3.77 | perplexity: 43.38 | lr: 4.71e-04 | norm: 0.3893 | dt: 14307.0467ms | tok/sec: 27.4841\n",
      "step 1653 | train loss: 3.64 | val loss: 3.76 | perplexity: 43.13 | lr: 4.70e-04 | norm: 0.3343 | dt: 14313.2370ms | tok/sec: 27.4722\n",
      "step 1654 | train loss: 3.97 | val loss: 3.76 | perplexity: 42.79 | lr: 4.69e-04 | norm: 0.3868 | dt: 14325.3624ms | tok/sec: 27.4489\n",
      "step 1655 | train loss: 3.76 | val loss: 3.76 | perplexity: 42.79 | lr: 4.69e-04 | norm: 0.5229 | dt: 14332.8481ms | tok/sec: 27.4346\n",
      "step 1656 | train loss: 3.75 | val loss: 3.76 | perplexity: 43.03 | lr: 4.68e-04 | norm: 0.3273 | dt: 14326.1492ms | tok/sec: 27.4474\n",
      "step 1657 | train loss: 3.78 | val loss: 3.76 | perplexity: 43.10 | lr: 4.67e-04 | norm: 0.3889 | dt: 14308.0590ms | tok/sec: 27.4821\n",
      "step 1658 | train loss: 3.81 | val loss: 3.76 | perplexity: 43.04 | lr: 4.66e-04 | norm: 0.3436 | dt: 14318.6924ms | tok/sec: 27.4617\n",
      "step 1659 | train loss: 3.68 | val loss: 3.76 | perplexity: 43.06 | lr: 4.65e-04 | norm: 0.3995 | dt: 14305.6977ms | tok/sec: 27.4867\n",
      "step 1660 | train loss: 3.88 | val loss: 3.76 | perplexity: 42.86 | lr: 4.65e-04 | norm: 0.3559 | dt: 14297.9248ms | tok/sec: 27.5016\n",
      "step 1661 | train loss: 4.05 | val loss: 3.75 | perplexity: 42.68 | lr: 4.64e-04 | norm: 0.3828 | dt: 14300.8108ms | tok/sec: 27.4961\n",
      "step 1662 | train loss: 3.47 | val loss: 3.75 | perplexity: 42.60 | lr: 4.63e-04 | norm: 0.3919 | dt: 14315.5572ms | tok/sec: 27.4677\n",
      "step 1663 | train loss: 3.84 | val loss: 3.75 | perplexity: 42.55 | lr: 4.62e-04 | norm: 0.3501 | dt: 14315.0144ms | tok/sec: 27.4688\n",
      "step 1664 | train loss: 3.97 | val loss: 3.75 | perplexity: 42.59 | lr: 4.61e-04 | norm: 0.3956 | dt: 14306.4065ms | tok/sec: 27.4853\n",
      "step 1665 | train loss: 3.80 | val loss: 3.76 | perplexity: 42.88 | lr: 4.60e-04 | norm: 0.3196 | dt: 14334.5273ms | tok/sec: 27.4314\n",
      "step 1666 | train loss: 4.09 | val loss: 3.77 | perplexity: 43.26 | lr: 4.60e-04 | norm: 0.3755 | dt: 14312.5992ms | tok/sec: 27.4734\n",
      "step 1667 | train loss: 3.79 | val loss: 3.77 | perplexity: 43.23 | lr: 4.59e-04 | norm: 0.4085 | dt: 14318.5806ms | tok/sec: 27.4619\n",
      "step 1668 | train loss: 3.86 | val loss: 3.76 | perplexity: 43.11 | lr: 4.58e-04 | norm: 0.3245 | dt: 14300.9584ms | tok/sec: 27.4958\n",
      "step 1669 | train loss: 3.88 | val loss: 3.76 | perplexity: 42.97 | lr: 4.57e-04 | norm: 0.3834 | dt: 14341.7294ms | tok/sec: 27.4176\n",
      "step 1670 | train loss: 3.98 | val loss: 3.76 | perplexity: 42.76 | lr: 4.56e-04 | norm: 0.4273 | dt: 14280.1342ms | tok/sec: 27.5359\n",
      "step 1671 | train loss: 4.21 | val loss: 3.76 | perplexity: 42.88 | lr: 4.55e-04 | norm: 0.4331 | dt: 14290.2732ms | tok/sec: 27.5163\n",
      "step 1672 | train loss: 3.76 | val loss: 3.76 | perplexity: 42.87 | lr: 4.55e-04 | norm: 0.3977 | dt: 14299.0692ms | tok/sec: 27.4994\n",
      "step 1673 | train loss: 3.75 | val loss: 3.76 | perplexity: 42.91 | lr: 4.54e-04 | norm: 0.3660 | dt: 14283.3700ms | tok/sec: 27.5296\n",
      "step 1674 | train loss: 3.72 | val loss: 3.76 | perplexity: 43.02 | lr: 4.53e-04 | norm: 0.3846 | dt: 14280.4437ms | tok/sec: 27.5353\n",
      "step 1675 | train loss: 3.78 | val loss: 3.75 | perplexity: 42.42 | lr: 4.52e-04 | norm: 0.5278 | dt: 14307.9619ms | tok/sec: 27.4823\n",
      "step 1676 | train loss: 3.77 | val loss: 3.75 | perplexity: 42.45 | lr: 4.51e-04 | norm: 0.3717 | dt: 14306.4878ms | tok/sec: 27.4852\n",
      "step 1677 | train loss: 4.08 | val loss: 3.75 | perplexity: 42.61 | lr: 4.51e-04 | norm: 0.4135 | dt: 14281.7984ms | tok/sec: 27.5327\n",
      "step 1678 | train loss: 3.94 | val loss: 3.75 | perplexity: 42.71 | lr: 4.50e-04 | norm: 0.4493 | dt: 14298.6724ms | tok/sec: 27.5002\n",
      "step 1679 | train loss: 4.13 | val loss: 3.76 | perplexity: 42.92 | lr: 4.49e-04 | norm: 0.4384 | dt: 14321.2645ms | tok/sec: 27.4568\n",
      "step 1680 | train loss: 4.20 | val loss: 3.77 | perplexity: 43.25 | lr: 4.48e-04 | norm: 0.4690 | dt: 14327.9297ms | tok/sec: 27.4440\n",
      "step 1681 | train loss: 3.78 | val loss: 3.77 | perplexity: 43.23 | lr: 4.47e-04 | norm: 0.4094 | dt: 14299.9229ms | tok/sec: 27.4978\n",
      "step 1682 | train loss: 3.52 | val loss: 3.76 | perplexity: 43.15 | lr: 4.46e-04 | norm: 0.3156 | dt: 14285.5740ms | tok/sec: 27.5254\n",
      "step 1683 | train loss: 3.58 | val loss: 3.76 | perplexity: 42.88 | lr: 4.46e-04 | norm: 0.3512 | dt: 14292.6328ms | tok/sec: 27.5118\n",
      "step 1684 | train loss: 4.04 | val loss: 3.75 | perplexity: 42.62 | lr: 4.45e-04 | norm: 0.3815 | dt: 14282.0992ms | tok/sec: 27.5321\n",
      "step 1685 | train loss: 3.92 | val loss: 3.75 | perplexity: 42.55 | lr: 4.44e-04 | norm: 0.3338 | dt: 14291.3818ms | tok/sec: 27.5142\n",
      "step 1686 | train loss: 3.71 | val loss: 3.75 | perplexity: 42.43 | lr: 4.43e-04 | norm: 0.3462 | dt: 14429.3401ms | tok/sec: 27.2511\n",
      "step 1687 | train loss: 3.86 | val loss: 3.75 | perplexity: 42.35 | lr: 4.42e-04 | norm: 0.3557 | dt: 14269.2223ms | tok/sec: 27.5569\n",
      "step 1688 | train loss: 3.81 | val loss: 3.75 | perplexity: 42.53 | lr: 4.42e-04 | norm: 0.3655 | dt: 14263.2101ms | tok/sec: 27.5685\n",
      "step 1689 | train loss: 3.59 | val loss: 3.75 | perplexity: 42.68 | lr: 4.41e-04 | norm: 0.3522 | dt: 14293.4351ms | tok/sec: 27.5103\n",
      "step 1690 | train loss: 3.69 | val loss: 3.76 | perplexity: 42.77 | lr: 4.40e-04 | norm: 0.3632 | dt: 14260.3769ms | tok/sec: 27.5740\n",
      "step 1691 | train loss: 3.87 | val loss: 3.76 | perplexity: 42.79 | lr: 4.39e-04 | norm: 0.3614 | dt: 14273.7710ms | tok/sec: 27.5482\n",
      "step 1692 | train loss: 3.95 | val loss: 3.75 | perplexity: 42.58 | lr: 4.38e-04 | norm: 0.3426 | dt: 14271.9049ms | tok/sec: 27.5518\n",
      "step 1693 | train loss: 3.86 | val loss: 3.75 | perplexity: 42.42 | lr: 4.38e-04 | norm: 0.3205 | dt: 14277.8456ms | tok/sec: 27.5403\n",
      "step 1694 | train loss: 3.82 | val loss: 3.75 | perplexity: 42.37 | lr: 4.37e-04 | norm: 0.3433 | dt: 14277.4575ms | tok/sec: 27.5410\n",
      "step 1695 | train loss: 4.02 | val loss: 3.75 | perplexity: 42.57 | lr: 4.36e-04 | norm: 0.6692 | dt: 14281.9397ms | tok/sec: 27.5324\n",
      "step 1696 | train loss: 3.79 | val loss: 3.76 | perplexity: 42.97 | lr: 4.35e-04 | norm: 0.3418 | dt: 14286.4373ms | tok/sec: 27.5237\n",
      "step 1697 | train loss: 3.94 | val loss: 3.76 | perplexity: 42.76 | lr: 4.34e-04 | norm: 0.4100 | dt: 14323.9031ms | tok/sec: 27.4517\n",
      "step 1698 | train loss: 4.04 | val loss: 3.75 | perplexity: 42.67 | lr: 4.33e-04 | norm: 0.4004 | dt: 14336.0913ms | tok/sec: 27.4284\n",
      "step 1699 | train loss: 4.13 | val loss: 3.75 | perplexity: 42.46 | lr: 4.33e-04 | norm: 0.3832 | dt: 14270.5591ms | tok/sec: 27.5544\n",
      "step 1700 | train loss: 3.87 | val loss: 3.75 | perplexity: 42.45 | lr: 4.32e-04 | norm: 0.3342 | dt: 14279.8426ms | tok/sec: 27.5364\n",
      "step 1701 | train loss: 3.89 | val loss: 3.75 | perplexity: 42.37 | lr: 4.31e-04 | norm: 0.3552 | dt: 14301.5609ms | tok/sec: 27.4946\n",
      "step 1702 | train loss: 3.71 | val loss: 3.74 | perplexity: 42.24 | lr: 4.30e-04 | norm: 0.3819 | dt: 14343.9722ms | tok/sec: 27.4133\n",
      "step 1703 | train loss: 3.78 | val loss: 3.74 | perplexity: 42.27 | lr: 4.29e-04 | norm: 0.3356 | dt: 14283.0212ms | tok/sec: 27.5303\n",
      "step 1704 | train loss: 3.89 | val loss: 3.74 | perplexity: 42.19 | lr: 4.29e-04 | norm: 0.3798 | dt: 14304.4434ms | tok/sec: 27.4891\n",
      "step 1705 | train loss: 4.19 | val loss: 3.75 | perplexity: 42.34 | lr: 4.28e-04 | norm: 0.3670 | dt: 14329.4501ms | tok/sec: 27.4411\n",
      "step 1706 | train loss: 4.05 | val loss: 3.75 | perplexity: 42.53 | lr: 4.27e-04 | norm: 0.3409 | dt: 14299.8526ms | tok/sec: 27.4979\n",
      "step 1707 | train loss: 3.83 | val loss: 3.75 | perplexity: 42.43 | lr: 4.26e-04 | norm: 0.3926 | dt: 14357.1262ms | tok/sec: 27.3882\n",
      "step 1708 | train loss: 4.05 | val loss: 3.74 | perplexity: 42.25 | lr: 4.25e-04 | norm: 0.3406 | dt: 14315.8612ms | tok/sec: 27.4672\n",
      "step 1709 | train loss: 3.71 | val loss: 3.74 | perplexity: 42.19 | lr: 4.25e-04 | norm: 0.3546 | dt: 14292.3849ms | tok/sec: 27.5123\n",
      "step 1710 | train loss: 4.08 | val loss: 3.74 | perplexity: 42.12 | lr: 4.24e-04 | norm: 0.4150 | dt: 14317.4181ms | tok/sec: 27.4642\n",
      "step 1711 | train loss: 4.08 | val loss: 3.74 | perplexity: 41.95 | lr: 4.23e-04 | norm: 0.3885 | dt: 14273.8662ms | tok/sec: 27.5480\n",
      "step 1712 | train loss: 3.78 | val loss: 3.74 | perplexity: 41.99 | lr: 4.22e-04 | norm: 0.3606 | dt: 14283.9406ms | tok/sec: 27.5285\n",
      "step 1713 | train loss: 4.18 | val loss: 3.75 | perplexity: 42.33 | lr: 4.21e-04 | norm: 0.3187 | dt: 14270.5829ms | tok/sec: 27.5543\n",
      "step 1714 | train loss: 4.06 | val loss: 3.75 | perplexity: 42.58 | lr: 4.21e-04 | norm: 0.3712 | dt: 14275.6610ms | tok/sec: 27.5445\n",
      "step 1715 | train loss: 3.91 | val loss: 3.75 | perplexity: 42.62 | lr: 4.20e-04 | norm: 0.3450 | dt: 14285.8310ms | tok/sec: 27.5249\n",
      "step 1716 | train loss: 3.87 | val loss: 3.75 | perplexity: 42.38 | lr: 4.19e-04 | norm: 0.3299 | dt: 14258.4577ms | tok/sec: 27.5777\n",
      "step 1717 | train loss: 3.78 | val loss: 3.74 | perplexity: 41.95 | lr: 4.18e-04 | norm: 0.3473 | dt: 14270.8390ms | tok/sec: 27.5538\n",
      "step 1718 | train loss: 3.83 | val loss: 3.73 | perplexity: 41.82 | lr: 4.17e-04 | norm: 0.3073 | dt: 14301.0833ms | tok/sec: 27.4955\n",
      "step 1719 | train loss: 3.87 | val loss: 3.73 | perplexity: 41.74 | lr: 4.17e-04 | norm: 0.3545 | dt: 14291.5282ms | tok/sec: 27.5139\n",
      "step 1720 | train loss: 3.99 | val loss: 3.73 | perplexity: 41.69 | lr: 4.16e-04 | norm: 0.3816 | dt: 14281.7593ms | tok/sec: 27.5327\n",
      "step 1721 | train loss: 4.30 | val loss: 3.74 | perplexity: 42.04 | lr: 4.15e-04 | norm: 0.5003 | dt: 14296.5491ms | tok/sec: 27.5043\n",
      "step 1722 | train loss: 3.78 | val loss: 3.74 | perplexity: 42.13 | lr: 4.14e-04 | norm: 0.3731 | dt: 14270.7686ms | tok/sec: 27.5539\n",
      "step 1723 | train loss: 4.09 | val loss: 3.74 | perplexity: 41.96 | lr: 4.13e-04 | norm: 0.4282 | dt: 14286.1416ms | tok/sec: 27.5243\n",
      "step 1724 | train loss: 4.02 | val loss: 3.74 | perplexity: 42.00 | lr: 4.13e-04 | norm: 0.3275 | dt: 14316.0684ms | tok/sec: 27.4668\n",
      "step 1725 | train loss: 3.77 | val loss: 3.74 | perplexity: 41.99 | lr: 4.12e-04 | norm: 0.3843 | dt: 14279.4902ms | tok/sec: 27.5371\n",
      "step 1726 | train loss: 3.82 | val loss: 3.74 | perplexity: 42.06 | lr: 4.11e-04 | norm: 0.3201 | dt: 14331.9674ms | tok/sec: 27.4363\n",
      "step 1727 | train loss: 3.73 | val loss: 3.74 | perplexity: 42.02 | lr: 4.10e-04 | norm: 0.3969 | dt: 14277.7417ms | tok/sec: 27.5405\n",
      "step 1728 | train loss: 3.67 | val loss: 3.74 | perplexity: 41.91 | lr: 4.09e-04 | norm: 0.3733 | dt: 14289.5844ms | tok/sec: 27.5177\n",
      "step 1729 | train loss: 4.03 | val loss: 3.73 | perplexity: 41.85 | lr: 4.09e-04 | norm: 0.4078 | dt: 14256.1228ms | tok/sec: 27.5823\n",
      "step 1730 | train loss: 3.77 | val loss: 3.74 | perplexity: 41.92 | lr: 4.08e-04 | norm: 0.2778 | dt: 14278.9340ms | tok/sec: 27.5382\n",
      "step 1731 | train loss: 3.78 | val loss: 3.74 | perplexity: 41.98 | lr: 4.07e-04 | norm: 0.3017 | dt: 14276.2232ms | tok/sec: 27.5434\n",
      "step 1732 | train loss: 3.68 | val loss: 3.74 | perplexity: 41.90 | lr: 4.06e-04 | norm: 0.2983 | dt: 14277.6968ms | tok/sec: 27.5406\n",
      "step 1733 | train loss: 3.82 | val loss: 3.73 | perplexity: 41.75 | lr: 4.05e-04 | norm: 0.3151 | dt: 14265.1763ms | tok/sec: 27.5647\n",
      "step 1734 | train loss: 3.82 | val loss: 3.72 | perplexity: 41.46 | lr: 4.05e-04 | norm: 0.3611 | dt: 14307.7209ms | tok/sec: 27.4828\n",
      "step 1735 | train loss: 3.86 | val loss: 3.72 | perplexity: 41.47 | lr: 4.04e-04 | norm: 0.3813 | dt: 14316.2255ms | tok/sec: 27.4665\n",
      "step 1736 | train loss: 3.74 | val loss: 3.73 | perplexity: 41.52 | lr: 4.03e-04 | norm: 0.3381 | dt: 14301.8727ms | tok/sec: 27.4940\n",
      "step 1737 | train loss: 3.85 | val loss: 3.73 | perplexity: 41.50 | lr: 4.02e-04 | norm: 0.3625 | dt: 14294.9612ms | tok/sec: 27.5073\n",
      "step 1738 | train loss: 3.72 | val loss: 3.72 | perplexity: 41.41 | lr: 4.01e-04 | norm: 0.3272 | dt: 14281.5125ms | tok/sec: 27.5332\n",
      "step 1739 | train loss: 3.78 | val loss: 3.72 | perplexity: 41.30 | lr: 4.01e-04 | norm: 0.3272 | dt: 14289.3004ms | tok/sec: 27.5182\n",
      "step 1740 | train loss: 3.92 | val loss: 3.72 | perplexity: 41.28 | lr: 4.00e-04 | norm: 0.3898 | dt: 14374.6645ms | tok/sec: 27.3548\n",
      "step 1741 | train loss: 3.77 | val loss: 3.72 | perplexity: 41.28 | lr: 3.99e-04 | norm: 0.4186 | dt: 14305.2845ms | tok/sec: 27.4875\n",
      "step 1742 | train loss: 3.88 | val loss: 3.72 | perplexity: 41.26 | lr: 3.98e-04 | norm: 0.3948 | dt: 14318.2869ms | tok/sec: 27.4625\n",
      "step 1743 | train loss: 3.88 | val loss: 3.72 | perplexity: 41.25 | lr: 3.97e-04 | norm: 0.3219 | dt: 14298.7278ms | tok/sec: 27.5001\n",
      "step 1744 | train loss: 4.02 | val loss: 3.73 | perplexity: 41.54 | lr: 3.97e-04 | norm: 1.2900 | dt: 14293.0872ms | tok/sec: 27.5109\n",
      "step 1745 | train loss: 3.67 | val loss: 3.73 | perplexity: 41.50 | lr: 3.96e-04 | norm: 0.4177 | dt: 14323.2007ms | tok/sec: 27.4531\n",
      "step 1746 | train loss: 4.21 | val loss: 3.73 | perplexity: 41.55 | lr: 3.95e-04 | norm: 0.4471 | dt: 14291.6849ms | tok/sec: 27.5136\n",
      "step 1747 | train loss: 3.98 | val loss: 3.73 | perplexity: 41.62 | lr: 3.94e-04 | norm: 0.3422 | dt: 14298.3766ms | tok/sec: 27.5007\n",
      "step 1748 | train loss: 3.73 | val loss: 3.73 | perplexity: 41.61 | lr: 3.94e-04 | norm: 0.3938 | dt: 14282.3625ms | tok/sec: 27.5316\n",
      "step 1749 | train loss: 3.53 | val loss: 3.73 | perplexity: 41.48 | lr: 3.93e-04 | norm: 0.3547 | dt: 14286.5150ms | tok/sec: 27.5236\n",
      "step 1750 | train loss: 4.00 | val loss: 3.72 | perplexity: 41.46 | lr: 3.92e-04 | norm: 0.3942 | dt: 14330.5602ms | tok/sec: 27.4390\n",
      "step 1751 | train loss: 3.77 | val loss: 3.73 | perplexity: 41.51 | lr: 3.91e-04 | norm: 0.3367 | dt: 14309.4165ms | tok/sec: 27.4795\n",
      "step 1752 | train loss: 3.87 | val loss: 3.72 | perplexity: 41.42 | lr: 3.90e-04 | norm: 0.3942 | dt: 14270.8187ms | tok/sec: 27.5539\n",
      "step 1753 | train loss: 3.57 | val loss: 3.72 | perplexity: 41.26 | lr: 3.90e-04 | norm: 0.3505 | dt: 14296.1133ms | tok/sec: 27.5051\n",
      "step 1754 | train loss: 3.73 | val loss: 3.72 | perplexity: 41.23 | lr: 3.89e-04 | norm: 0.3387 | dt: 14301.0492ms | tok/sec: 27.4956\n",
      "step 1755 | train loss: 3.82 | val loss: 3.72 | perplexity: 41.24 | lr: 3.88e-04 | norm: 0.3380 | dt: 14268.5053ms | tok/sec: 27.5583\n",
      "step 1756 | train loss: 3.85 | val loss: 3.72 | perplexity: 41.16 | lr: 3.87e-04 | norm: 0.3526 | dt: 14303.9508ms | tok/sec: 27.4900\n",
      "step 1757 | train loss: 3.85 | val loss: 3.72 | perplexity: 41.12 | lr: 3.86e-04 | norm: 0.2976 | dt: 14288.2414ms | tok/sec: 27.5203\n",
      "step 1758 | train loss: 3.96 | val loss: 3.72 | perplexity: 41.10 | lr: 3.86e-04 | norm: 0.3440 | dt: 14282.6726ms | tok/sec: 27.5310\n",
      "step 1759 | train loss: 3.87 | val loss: 3.72 | perplexity: 41.14 | lr: 3.85e-04 | norm: 0.3592 | dt: 14290.0429ms | tok/sec: 27.5168\n",
      "step 1760 | train loss: 3.58 | val loss: 3.72 | perplexity: 41.14 | lr: 3.84e-04 | norm: 0.3382 | dt: 14308.1758ms | tok/sec: 27.4819\n",
      "step 1761 | train loss: 3.90 | val loss: 3.72 | perplexity: 41.08 | lr: 3.83e-04 | norm: 0.2876 | dt: 14284.1587ms | tok/sec: 27.5281\n",
      "step 1762 | train loss: 3.65 | val loss: 3.71 | perplexity: 41.05 | lr: 3.83e-04 | norm: 0.3107 | dt: 14289.6278ms | tok/sec: 27.5176\n",
      "step 1763 | train loss: 3.81 | val loss: 3.71 | perplexity: 41.03 | lr: 3.82e-04 | norm: 0.2807 | dt: 14304.5762ms | tok/sec: 27.4888\n",
      "step 1764 | train loss: 4.03 | val loss: 3.71 | perplexity: 41.02 | lr: 3.81e-04 | norm: 0.3315 | dt: 14292.0375ms | tok/sec: 27.5129\n",
      "step 1765 | train loss: 4.07 | val loss: 3.72 | perplexity: 41.07 | lr: 3.80e-04 | norm: 0.3337 | dt: 14338.4731ms | tok/sec: 27.4238\n",
      "step 1766 | train loss: 3.65 | val loss: 3.72 | perplexity: 41.08 | lr: 3.79e-04 | norm: 0.3076 | dt: 14277.0915ms | tok/sec: 27.5417\n",
      "step 1767 | train loss: 3.81 | val loss: 3.71 | perplexity: 40.87 | lr: 3.79e-04 | norm: 0.2963 | dt: 14274.2312ms | tok/sec: 27.5473\n",
      "step 1768 | train loss: 3.84 | val loss: 3.71 | perplexity: 40.88 | lr: 3.78e-04 | norm: 0.3251 | dt: 14290.1371ms | tok/sec: 27.5166\n",
      "step 1769 | train loss: 3.83 | val loss: 3.71 | perplexity: 40.90 | lr: 3.77e-04 | norm: 0.3282 | dt: 14292.1021ms | tok/sec: 27.5128\n",
      "step 1770 | train loss: 3.91 | val loss: 3.71 | perplexity: 40.93 | lr: 3.76e-04 | norm: 0.3950 | dt: 14311.5494ms | tok/sec: 27.4754\n",
      "step 1771 | train loss: 3.57 | val loss: 3.71 | perplexity: 40.94 | lr: 3.76e-04 | norm: 0.3310 | dt: 14297.7972ms | tok/sec: 27.5019\n",
      "step 1772 | train loss: 3.73 | val loss: 3.71 | perplexity: 40.94 | lr: 3.75e-04 | norm: 0.3920 | dt: 14312.2609ms | tok/sec: 27.4741\n",
      "step 1773 | train loss: 3.77 | val loss: 3.72 | perplexity: 41.12 | lr: 3.74e-04 | norm: 0.3326 | dt: 14321.6362ms | tok/sec: 27.4561\n",
      "step 1774 | train loss: 3.98 | val loss: 3.72 | perplexity: 41.28 | lr: 3.73e-04 | norm: 0.3184 | dt: 14298.9304ms | tok/sec: 27.4997\n",
      "step 1775 | train loss: 3.83 | val loss: 3.72 | perplexity: 41.10 | lr: 3.72e-04 | norm: 0.3806 | dt: 14277.0860ms | tok/sec: 27.5418\n",
      "step 1776 | train loss: 3.87 | val loss: 3.71 | perplexity: 40.88 | lr: 3.72e-04 | norm: 0.3427 | dt: 14309.3879ms | tok/sec: 27.4796\n",
      "step 1777 | train loss: 3.75 | val loss: 3.71 | perplexity: 40.79 | lr: 3.71e-04 | norm: 0.3705 | dt: 14309.2842ms | tok/sec: 27.4798\n",
      "step 1778 | train loss: 4.08 | val loss: 3.71 | perplexity: 40.85 | lr: 3.70e-04 | norm: 0.3913 | dt: 14319.5016ms | tok/sec: 27.4602\n",
      "step 1779 | train loss: 3.76 | val loss: 3.71 | perplexity: 40.79 | lr: 3.69e-04 | norm: 0.3257 | dt: 14317.0118ms | tok/sec: 27.4649\n",
      "step 1780 | train loss: 3.85 | val loss: 3.71 | perplexity: 40.71 | lr: 3.69e-04 | norm: 0.3594 | dt: 14306.4129ms | tok/sec: 27.4853\n",
      "step 1781 | train loss: 3.61 | val loss: 3.70 | perplexity: 40.61 | lr: 3.68e-04 | norm: 0.3457 | dt: 14281.4856ms | tok/sec: 27.5333\n",
      "step 1782 | train loss: 3.96 | val loss: 3.71 | perplexity: 40.74 | lr: 3.67e-04 | norm: 0.3740 | dt: 14318.1927ms | tok/sec: 27.4627\n",
      "step 1783 | train loss: 3.57 | val loss: 3.71 | perplexity: 40.84 | lr: 3.66e-04 | norm: 0.3834 | dt: 14303.5419ms | tok/sec: 27.4908\n",
      "step 1784 | train loss: 3.79 | val loss: 3.71 | perplexity: 40.76 | lr: 3.66e-04 | norm: 0.3576 | dt: 14309.0250ms | tok/sec: 27.4803\n",
      "step 1785 | train loss: 3.84 | val loss: 3.70 | perplexity: 40.63 | lr: 3.65e-04 | norm: 0.3740 | dt: 14300.0445ms | tok/sec: 27.4975\n",
      "step 1786 | train loss: 3.72 | val loss: 3.70 | perplexity: 40.65 | lr: 3.64e-04 | norm: 0.3709 | dt: 14295.5418ms | tok/sec: 27.5062\n",
      "step 1787 | train loss: 3.51 | val loss: 3.71 | perplexity: 40.72 | lr: 3.63e-04 | norm: 0.3313 | dt: 14302.5837ms | tok/sec: 27.4927\n",
      "step 1788 | train loss: 3.93 | val loss: 3.71 | perplexity: 40.68 | lr: 3.63e-04 | norm: 0.3967 | dt: 14318.1789ms | tok/sec: 27.4627\n",
      "step 1789 | train loss: 4.15 | val loss: 3.71 | perplexity: 40.77 | lr: 3.62e-04 | norm: 0.3974 | dt: 14307.1148ms | tok/sec: 27.4839\n",
      "step 1790 | train loss: 4.15 | val loss: 3.71 | perplexity: 40.76 | lr: 3.61e-04 | norm: 0.3652 | dt: 14422.3669ms | tok/sec: 27.2643\n",
      "step 1791 | train loss: 3.95 | val loss: 3.71 | perplexity: 40.72 | lr: 3.60e-04 | norm: 0.3757 | dt: 14176.2288ms | tok/sec: 27.7377\n",
      "step 1792 | train loss: 4.19 | val loss: 3.71 | perplexity: 40.77 | lr: 3.59e-04 | norm: 0.3633 | dt: 14105.7343ms | tok/sec: 27.8763\n",
      "step 1793 | train loss: 3.61 | val loss: 3.70 | perplexity: 40.63 | lr: 3.59e-04 | norm: 0.3879 | dt: 14092.5157ms | tok/sec: 27.9025\n",
      "step 1794 | train loss: 3.56 | val loss: 3.70 | perplexity: 40.61 | lr: 3.58e-04 | norm: 0.3516 | dt: 14088.8095ms | tok/sec: 27.9098\n",
      "step 1795 | train loss: 3.93 | val loss: 3.70 | perplexity: 40.61 | lr: 3.57e-04 | norm: 0.3706 | dt: 14112.1118ms | tok/sec: 27.8637\n",
      "step 1796 | train loss: 3.76 | val loss: 3.70 | perplexity: 40.59 | lr: 3.56e-04 | norm: 0.3045 | dt: 14141.0153ms | tok/sec: 27.8068\n",
      "step 1797 | train loss: 4.00 | val loss: 3.70 | perplexity: 40.61 | lr: 3.56e-04 | norm: 0.3480 | dt: 14110.2297ms | tok/sec: 27.8674\n",
      "step 1798 | train loss: 3.58 | val loss: 3.70 | perplexity: 40.56 | lr: 3.55e-04 | norm: 0.3113 | dt: 14109.3819ms | tok/sec: 27.8691\n",
      "step 1799 | train loss: 3.93 | val loss: 3.70 | perplexity: 40.36 | lr: 3.54e-04 | norm: 0.5617 | dt: 14129.5092ms | tok/sec: 27.8294\n",
      "step 1800 | train loss: 3.63 | val loss: 3.70 | perplexity: 40.33 | lr: 3.53e-04 | norm: 0.3996 | dt: 14099.3774ms | tok/sec: 27.8889\n",
      "step 1801 | train loss: 3.97 | val loss: 3.70 | perplexity: 40.27 | lr: 3.53e-04 | norm: 0.3980 | dt: 14111.1500ms | tok/sec: 27.8656\n",
      "step 1802 | train loss: 3.62 | val loss: 3.70 | perplexity: 40.29 | lr: 3.52e-04 | norm: 0.3679 | dt: 14113.9791ms | tok/sec: 27.8600\n",
      "step 1803 | train loss: 3.65 | val loss: 3.70 | perplexity: 40.46 | lr: 3.51e-04 | norm: 0.3390 | dt: 14147.7110ms | tok/sec: 27.7936\n",
      "step 1804 | train loss: 3.79 | val loss: 3.70 | perplexity: 40.40 | lr: 3.50e-04 | norm: 0.3747 | dt: 14115.0267ms | tok/sec: 27.8580\n",
      "step 1805 | train loss: 3.68 | val loss: 3.70 | perplexity: 40.36 | lr: 3.50e-04 | norm: 0.3466 | dt: 14112.1325ms | tok/sec: 27.8637\n",
      "step 1806 | train loss: 3.97 | val loss: 3.70 | perplexity: 40.49 | lr: 3.49e-04 | norm: 0.3543 | dt: 14105.8714ms | tok/sec: 27.8761\n",
      "step 1807 | train loss: 4.02 | val loss: 3.70 | perplexity: 40.42 | lr: 3.48e-04 | norm: 0.4133 | dt: 14119.7305ms | tok/sec: 27.8487\n",
      "step 1808 | train loss: 3.91 | val loss: 3.70 | perplexity: 40.42 | lr: 3.47e-04 | norm: 0.3757 | dt: 14121.7825ms | tok/sec: 27.8446\n",
      "step 1809 | train loss: 3.93 | val loss: 3.70 | perplexity: 40.42 | lr: 3.47e-04 | norm: 0.3235 | dt: 14112.7589ms | tok/sec: 27.8624\n",
      "step 1810 | train loss: 3.82 | val loss: 3.70 | perplexity: 40.48 | lr: 3.46e-04 | norm: 0.3337 | dt: 14129.4372ms | tok/sec: 27.8296\n",
      "step 1811 | train loss: 3.84 | val loss: 3.70 | perplexity: 40.49 | lr: 3.45e-04 | norm: 0.4290 | dt: 14092.0763ms | tok/sec: 27.9033\n",
      "step 1812 | train loss: 3.84 | val loss: 3.70 | perplexity: 40.39 | lr: 3.44e-04 | norm: 0.3237 | dt: 14105.6707ms | tok/sec: 27.8764\n",
      "step 1813 | train loss: 3.89 | val loss: 3.69 | perplexity: 40.22 | lr: 3.44e-04 | norm: 0.3014 | dt: 14117.0681ms | tok/sec: 27.8539\n",
      "step 1814 | train loss: 3.95 | val loss: 3.69 | perplexity: 40.15 | lr: 3.43e-04 | norm: 0.3158 | dt: 14115.9327ms | tok/sec: 27.8562\n",
      "step 1815 | train loss: 3.80 | val loss: 3.69 | perplexity: 40.17 | lr: 3.42e-04 | norm: 0.3676 | dt: 14117.0511ms | tok/sec: 27.8540\n",
      "step 1816 | train loss: 3.62 | val loss: 3.69 | perplexity: 40.20 | lr: 3.41e-04 | norm: 0.3166 | dt: 14139.3163ms | tok/sec: 27.8101\n",
      "step 1817 | train loss: 3.64 | val loss: 3.69 | perplexity: 40.18 | lr: 3.41e-04 | norm: 0.3124 | dt: 14109.4763ms | tok/sec: 27.8689\n",
      "step 1818 | train loss: 3.65 | val loss: 3.69 | perplexity: 40.07 | lr: 3.40e-04 | norm: 0.2961 | dt: 14106.8382ms | tok/sec: 27.8741\n",
      "step 1819 | train loss: 3.91 | val loss: 3.69 | perplexity: 39.98 | lr: 3.39e-04 | norm: 0.3302 | dt: 14102.0007ms | tok/sec: 27.8837\n",
      "step 1820 | train loss: 3.71 | val loss: 3.69 | perplexity: 40.05 | lr: 3.38e-04 | norm: 0.3093 | dt: 14091.8906ms | tok/sec: 27.9037\n",
      "step 1821 | train loss: 3.92 | val loss: 3.69 | perplexity: 39.96 | lr: 3.38e-04 | norm: 0.3326 | dt: 14129.2477ms | tok/sec: 27.8299\n",
      "step 1822 | train loss: 3.89 | val loss: 3.69 | perplexity: 39.93 | lr: 3.37e-04 | norm: 0.3016 | dt: 14108.2332ms | tok/sec: 27.8714\n",
      "step 1823 | train loss: 3.93 | val loss: 3.68 | perplexity: 39.74 | lr: 3.36e-04 | norm: 0.3325 | dt: 14099.9966ms | tok/sec: 27.8877\n",
      "step 1824 | train loss: 3.51 | val loss: 3.68 | perplexity: 39.68 | lr: 3.35e-04 | norm: 0.2950 | dt: 14108.2287ms | tok/sec: 27.8714\n",
      "step 1825 | train loss: 3.77 | val loss: 3.68 | perplexity: 39.73 | lr: 3.35e-04 | norm: 0.2851 | dt: 14105.1970ms | tok/sec: 27.8774\n",
      "step 1826 | train loss: 3.78 | val loss: 3.68 | perplexity: 39.75 | lr: 3.34e-04 | norm: 0.3079 | dt: 14112.7973ms | tok/sec: 27.8624\n",
      "step 1827 | train loss: 3.97 | val loss: 3.68 | perplexity: 39.83 | lr: 3.33e-04 | norm: 0.2701 | dt: 14117.7368ms | tok/sec: 27.8526\n",
      "step 1828 | train loss: 3.95 | val loss: 3.69 | perplexity: 39.91 | lr: 3.33e-04 | norm: 0.3154 | dt: 14122.9804ms | tok/sec: 27.8423\n",
      "step 1829 | train loss: 3.76 | val loss: 3.69 | perplexity: 39.96 | lr: 3.32e-04 | norm: 0.2812 | dt: 14105.3538ms | tok/sec: 27.8771\n",
      "step 1830 | train loss: 4.05 | val loss: 3.69 | perplexity: 39.89 | lr: 3.31e-04 | norm: 0.3623 | dt: 14123.9686ms | tok/sec: 27.8403\n",
      "step 1831 | train loss: 3.85 | val loss: 3.69 | perplexity: 39.85 | lr: 3.30e-04 | norm: 0.2699 | dt: 14505.4092ms | tok/sec: 27.1082\n",
      "step 1832 | train loss: 3.90 | val loss: 3.68 | perplexity: 39.70 | lr: 3.30e-04 | norm: 0.3815 | dt: 14103.4484ms | tok/sec: 27.8808\n",
      "step 1833 | train loss: 3.98 | val loss: 3.68 | perplexity: 39.75 | lr: 3.29e-04 | norm: 0.3827 | dt: 14076.2348ms | tok/sec: 27.9347\n",
      "step 1834 | train loss: 3.58 | val loss: 3.68 | perplexity: 39.79 | lr: 3.28e-04 | norm: 0.2897 | dt: 14082.5806ms | tok/sec: 27.9222\n",
      "step 1835 | train loss: 3.99 | val loss: 3.68 | perplexity: 39.69 | lr: 3.27e-04 | norm: 0.3517 | dt: 14029.3078ms | tok/sec: 28.0282\n",
      "step 1836 | train loss: 3.65 | val loss: 3.68 | perplexity: 39.70 | lr: 3.27e-04 | norm: 0.2914 | dt: 14074.9032ms | tok/sec: 27.9374\n",
      "step 1837 | train loss: 3.66 | val loss: 3.68 | perplexity: 39.81 | lr: 3.26e-04 | norm: 0.3058 | dt: 14082.9620ms | tok/sec: 27.9214\n",
      "step 1838 | train loss: 3.72 | val loss: 3.69 | perplexity: 40.01 | lr: 3.25e-04 | norm: 0.3343 | dt: 14073.2403ms | tok/sec: 27.9407\n",
      "step 1839 | train loss: 3.66 | val loss: 3.69 | perplexity: 40.10 | lr: 3.25e-04 | norm: 0.3229 | dt: 14078.6235ms | tok/sec: 27.9300\n",
      "step 1840 | train loss: 3.78 | val loss: 3.69 | perplexity: 40.05 | lr: 3.24e-04 | norm: 0.3201 | dt: 14107.9788ms | tok/sec: 27.8719\n",
      "step 1841 | train loss: 4.04 | val loss: 3.69 | perplexity: 40.09 | lr: 3.23e-04 | norm: 0.3045 | dt: 14103.6699ms | tok/sec: 27.8804\n",
      "step 1842 | train loss: 3.64 | val loss: 3.69 | perplexity: 40.09 | lr: 3.22e-04 | norm: 0.3178 | dt: 14124.2533ms | tok/sec: 27.8398\n",
      "step 1843 | train loss: 3.90 | val loss: 3.69 | perplexity: 40.14 | lr: 3.22e-04 | norm: 0.3544 | dt: 14096.7116ms | tok/sec: 27.8942\n",
      "step 1844 | train loss: 3.77 | val loss: 3.69 | perplexity: 39.99 | lr: 3.21e-04 | norm: 0.3673 | dt: 14120.9197ms | tok/sec: 27.8463\n",
      "step 1845 | train loss: 3.63 | val loss: 3.68 | perplexity: 39.66 | lr: 3.20e-04 | norm: 0.3864 | dt: 14114.8913ms | tok/sec: 27.8582\n",
      "step 1846 | train loss: 3.92 | val loss: 3.68 | perplexity: 39.68 | lr: 3.19e-04 | norm: 0.3633 | dt: 14113.6179ms | tok/sec: 27.8608\n",
      "step 1847 | train loss: 3.60 | val loss: 3.68 | perplexity: 39.77 | lr: 3.19e-04 | norm: 0.3305 | dt: 14117.9233ms | tok/sec: 27.8523\n",
      "step 1848 | train loss: 3.87 | val loss: 3.68 | perplexity: 39.63 | lr: 3.18e-04 | norm: 0.3616 | dt: 14117.6353ms | tok/sec: 27.8528\n",
      "step 1849 | train loss: 3.83 | val loss: 3.68 | perplexity: 39.60 | lr: 3.17e-04 | norm: 0.3132 | dt: 14108.0410ms | tok/sec: 27.8718\n",
      "step 1850 | train loss: 3.95 | val loss: 3.68 | perplexity: 39.75 | lr: 3.17e-04 | norm: 0.3931 | dt: 14238.8716ms | tok/sec: 27.6157\n",
      "step 1851 | train loss: 4.00 | val loss: 3.69 | perplexity: 39.86 | lr: 3.16e-04 | norm: 0.3858 | dt: 14121.7561ms | tok/sec: 27.8447\n",
      "step 1852 | train loss: 3.86 | val loss: 3.69 | perplexity: 39.92 | lr: 3.15e-04 | norm: 0.3118 | dt: 14114.8896ms | tok/sec: 27.8582\n",
      "step 1853 | train loss: 4.10 | val loss: 3.69 | perplexity: 39.87 | lr: 3.14e-04 | norm: 0.3758 | dt: 14117.5766ms | tok/sec: 27.8529\n",
      "step 1854 | train loss: 3.74 | val loss: 3.68 | perplexity: 39.84 | lr: 3.14e-04 | norm: 0.3328 | dt: 14157.6331ms | tok/sec: 27.7741\n",
      "step 1855 | train loss: 3.59 | val loss: 3.69 | perplexity: 39.85 | lr: 3.13e-04 | norm: 0.3214 | dt: 14129.3643ms | tok/sec: 27.8297\n",
      "step 1856 | train loss: 4.16 | val loss: 3.69 | perplexity: 39.87 | lr: 3.12e-04 | norm: 0.3178 | dt: 14120.6660ms | tok/sec: 27.8468\n",
      "step 1857 | train loss: 3.85 | val loss: 3.68 | perplexity: 39.77 | lr: 3.12e-04 | norm: 0.3382 | dt: 14106.7541ms | tok/sec: 27.8743\n",
      "step 1858 | train loss: 3.80 | val loss: 3.68 | perplexity: 39.63 | lr: 3.11e-04 | norm: 0.3541 | dt: 14124.7416ms | tok/sec: 27.8388\n",
      "step 1859 | train loss: 3.95 | val loss: 3.68 | perplexity: 39.53 | lr: 3.10e-04 | norm: 0.3245 | dt: 14147.7864ms | tok/sec: 27.7935\n",
      "step 1860 | train loss: 3.93 | val loss: 3.68 | perplexity: 39.63 | lr: 3.09e-04 | norm: 0.3293 | dt: 14135.3993ms | tok/sec: 27.8178\n",
      "step 1861 | train loss: 3.91 | val loss: 3.69 | perplexity: 39.88 | lr: 3.09e-04 | norm: 0.3708 | dt: 14139.7557ms | tok/sec: 27.8092\n",
      "step 1862 | train loss: 3.68 | val loss: 3.69 | perplexity: 39.99 | lr: 3.08e-04 | norm: 0.3645 | dt: 14127.6498ms | tok/sec: 27.8331\n",
      "step 1863 | train loss: 3.94 | val loss: 3.69 | perplexity: 39.92 | lr: 3.07e-04 | norm: 0.3312 | dt: 14115.5677ms | tok/sec: 27.8569\n",
      "step 1864 | train loss: 3.89 | val loss: 3.68 | perplexity: 39.84 | lr: 3.07e-04 | norm: 0.3095 | dt: 14119.2989ms | tok/sec: 27.8495\n",
      "step 1865 | train loss: 4.07 | val loss: 3.68 | perplexity: 39.78 | lr: 3.06e-04 | norm: 0.3492 | dt: 14123.7862ms | tok/sec: 27.8407\n",
      "step 1866 | train loss: 3.74 | val loss: 3.68 | perplexity: 39.68 | lr: 3.05e-04 | norm: 0.3666 | dt: 14149.8351ms | tok/sec: 27.7894\n",
      "step 1867 | train loss: 3.81 | val loss: 3.68 | perplexity: 39.75 | lr: 3.05e-04 | norm: 0.3485 | dt: 14111.1801ms | tok/sec: 27.8656\n",
      "step 1868 | train loss: 3.64 | val loss: 3.68 | perplexity: 39.74 | lr: 3.04e-04 | norm: 0.3490 | dt: 14114.4123ms | tok/sec: 27.8592\n",
      "step 1869 | train loss: 3.77 | val loss: 3.68 | perplexity: 39.74 | lr: 3.03e-04 | norm: 0.3130 | dt: 14139.7274ms | tok/sec: 27.8093\n",
      "step 1870 | train loss: 3.66 | val loss: 3.68 | perplexity: 39.55 | lr: 3.02e-04 | norm: 0.3212 | dt: 14132.6580ms | tok/sec: 27.8232\n",
      "step 1871 | train loss: 3.61 | val loss: 3.67 | perplexity: 39.40 | lr: 3.02e-04 | norm: 0.2956 | dt: 14126.7760ms | tok/sec: 27.8348\n",
      "step 1872 | train loss: 3.87 | val loss: 3.68 | perplexity: 39.51 | lr: 3.01e-04 | norm: 0.3914 | dt: 14118.3565ms | tok/sec: 27.8514\n",
      "step 1873 | train loss: 3.91 | val loss: 3.68 | perplexity: 39.58 | lr: 3.00e-04 | norm: 0.2996 | dt: 14106.7796ms | tok/sec: 27.8743\n",
      "step 1874 | train loss: 3.58 | val loss: 3.68 | perplexity: 39.57 | lr: 3.00e-04 | norm: 0.3443 | dt: 14109.1914ms | tok/sec: 27.8695\n",
      "step 1875 | train loss: 3.81 | val loss: 3.68 | perplexity: 39.57 | lr: 2.99e-04 | norm: 0.3096 | dt: 14128.1826ms | tok/sec: 27.8320\n",
      "step 1876 | train loss: 3.98 | val loss: 3.68 | perplexity: 39.50 | lr: 2.98e-04 | norm: 0.3558 | dt: 14089.8449ms | tok/sec: 27.9078\n",
      "step 1877 | train loss: 3.72 | val loss: 3.67 | perplexity: 39.45 | lr: 2.98e-04 | norm: 0.3393 | dt: 14097.4212ms | tok/sec: 27.8928\n",
      "step 1878 | train loss: 3.88 | val loss: 3.68 | perplexity: 39.47 | lr: 2.97e-04 | norm: 0.3306 | dt: 14110.2653ms | tok/sec: 27.8674\n",
      "step 1879 | train loss: 3.77 | val loss: 3.67 | perplexity: 39.40 | lr: 2.96e-04 | norm: 0.3574 | dt: 14125.0091ms | tok/sec: 27.8383\n",
      "step 1880 | train loss: 3.65 | val loss: 3.67 | perplexity: 39.32 | lr: 2.95e-04 | norm: 0.3313 | dt: 14113.9243ms | tok/sec: 27.8601\n",
      "step 1881 | train loss: 4.01 | val loss: 3.67 | perplexity: 39.30 | lr: 2.95e-04 | norm: 0.2959 | dt: 14104.3341ms | tok/sec: 27.8791\n",
      "step 1882 | train loss: 3.81 | val loss: 3.67 | perplexity: 39.35 | lr: 2.94e-04 | norm: 0.2989 | dt: 14126.0788ms | tok/sec: 27.8362\n",
      "step 1883 | train loss: 3.59 | val loss: 3.67 | perplexity: 39.27 | lr: 2.93e-04 | norm: 0.3204 | dt: 14125.4833ms | tok/sec: 27.8373\n",
      "step 1884 | train loss: 3.69 | val loss: 3.67 | perplexity: 39.16 | lr: 2.93e-04 | norm: 0.3423 | dt: 14121.7818ms | tok/sec: 27.8446\n",
      "step 1885 | train loss: 3.89 | val loss: 3.67 | perplexity: 39.10 | lr: 2.92e-04 | norm: 0.3383 | dt: 14132.1290ms | tok/sec: 27.8243\n",
      "step 1886 | train loss: 3.72 | val loss: 3.67 | perplexity: 39.08 | lr: 2.91e-04 | norm: 0.2873 | dt: 14452.7118ms | tok/sec: 27.2071\n",
      "step 1887 | train loss: 3.84 | val loss: 3.67 | perplexity: 39.08 | lr: 2.91e-04 | norm: 0.3012 | dt: 14045.6641ms | tok/sec: 27.9955\n",
      "step 1888 | train loss: 3.67 | val loss: 3.67 | perplexity: 39.16 | lr: 2.90e-04 | norm: 0.2804 | dt: 14056.3838ms | tok/sec: 27.9742\n",
      "step 1889 | train loss: 3.68 | val loss: 3.67 | perplexity: 39.17 | lr: 2.89e-04 | norm: 0.3513 | dt: 14046.1504ms | tok/sec: 27.9946\n",
      "step 1890 | train loss: 3.61 | val loss: 3.67 | perplexity: 39.13 | lr: 2.89e-04 | norm: 0.2899 | dt: 14051.4078ms | tok/sec: 27.9841\n",
      "step 1891 | train loss: 3.75 | val loss: 3.67 | perplexity: 39.06 | lr: 2.88e-04 | norm: 0.2856 | dt: 14050.4208ms | tok/sec: 27.9861\n",
      "step 1892 | train loss: 3.85 | val loss: 3.66 | perplexity: 39.01 | lr: 2.87e-04 | norm: 0.2756 | dt: 14038.0201ms | tok/sec: 28.0108\n",
      "step 1893 | train loss: 3.91 | val loss: 3.66 | perplexity: 38.98 | lr: 2.87e-04 | norm: 0.2772 | dt: 14026.9291ms | tok/sec: 28.0329\n",
      "step 1894 | train loss: 3.65 | val loss: 3.66 | perplexity: 38.97 | lr: 2.86e-04 | norm: 0.3106 | dt: 14034.3702ms | tok/sec: 28.0181\n",
      "step 1895 | train loss: 4.04 | val loss: 3.67 | perplexity: 39.09 | lr: 2.85e-04 | norm: 0.3332 | dt: 14049.0797ms | tok/sec: 27.9887\n",
      "step 1896 | train loss: 3.76 | val loss: 3.67 | perplexity: 39.19 | lr: 2.84e-04 | norm: 0.3350 | dt: 14026.0031ms | tok/sec: 28.0348\n",
      "step 1897 | train loss: 3.92 | val loss: 3.67 | perplexity: 39.10 | lr: 2.84e-04 | norm: 0.3488 | dt: 14032.2828ms | tok/sec: 28.0222\n",
      "step 1898 | train loss: 3.43 | val loss: 3.67 | perplexity: 39.09 | lr: 2.83e-04 | norm: 0.2992 | dt: 14032.6717ms | tok/sec: 28.0215\n",
      "step 1899 | train loss: 3.96 | val loss: 3.67 | perplexity: 39.18 | lr: 2.82e-04 | norm: 0.3364 | dt: 14165.6179ms | tok/sec: 27.7585\n",
      "step 1900 | train loss: 3.80 | val loss: 3.67 | perplexity: 39.20 | lr: 2.82e-04 | norm: 0.3201 | dt: 14065.0318ms | tok/sec: 27.9570\n",
      "step 1901 | train loss: 3.87 | val loss: 3.67 | perplexity: 39.15 | lr: 2.81e-04 | norm: 0.2699 | dt: 14094.7716ms | tok/sec: 27.8980\n",
      "step 1902 | train loss: 3.83 | val loss: 3.66 | perplexity: 39.05 | lr: 2.80e-04 | norm: 0.3402 | dt: 14066.9465ms | tok/sec: 27.9532\n",
      "step 1903 | train loss: 3.90 | val loss: 3.66 | perplexity: 39.02 | lr: 2.80e-04 | norm: 0.2921 | dt: 14037.5464ms | tok/sec: 28.0117\n",
      "step 1904 | train loss: 3.65 | val loss: 3.66 | perplexity: 39.05 | lr: 2.79e-04 | norm: 0.2720 | dt: 14045.0943ms | tok/sec: 27.9967\n",
      "step 1905 | train loss: 3.99 | val loss: 3.67 | perplexity: 39.07 | lr: 2.78e-04 | norm: 0.3185 | dt: 14044.6205ms | tok/sec: 27.9976\n",
      "step 1906 | train loss: 3.78 | val loss: 3.67 | perplexity: 39.08 | lr: 2.78e-04 | norm: 0.2966 | dt: 14104.0502ms | tok/sec: 27.8797\n",
      "step 1907 | train loss: 3.80 | val loss: 3.66 | perplexity: 38.95 | lr: 2.77e-04 | norm: 0.3286 | dt: 14056.5622ms | tok/sec: 27.9738\n",
      "step 1908 | train loss: 3.78 | val loss: 3.66 | perplexity: 38.93 | lr: 2.76e-04 | norm: 0.4109 | dt: 14060.7572ms | tok/sec: 27.9655\n",
      "step 1909 | train loss: 3.89 | val loss: 3.66 | perplexity: 39.03 | lr: 2.76e-04 | norm: 0.3279 | dt: 14058.6078ms | tok/sec: 27.9698\n",
      "step 1910 | train loss: 3.68 | val loss: 3.67 | perplexity: 39.16 | lr: 2.75e-04 | norm: 0.3175 | dt: 14069.1493ms | tok/sec: 27.9488\n",
      "step 1911 | train loss: 3.64 | val loss: 3.66 | perplexity: 38.89 | lr: 2.74e-04 | norm: 0.4066 | dt: 14048.5325ms | tok/sec: 27.9898\n",
      "step 1912 | train loss: 3.94 | val loss: 3.66 | perplexity: 38.73 | lr: 2.74e-04 | norm: 0.3559 | dt: 14054.6174ms | tok/sec: 27.9777\n",
      "step 1913 | train loss: 3.50 | val loss: 3.65 | perplexity: 38.61 | lr: 2.73e-04 | norm: 0.3110 | dt: 14179.9340ms | tok/sec: 27.7305\n",
      "step 1914 | train loss: 3.74 | val loss: 3.65 | perplexity: 38.52 | lr: 2.72e-04 | norm: 0.2878 | dt: 14073.0956ms | tok/sec: 27.9410\n",
      "step 1915 | train loss: 3.78 | val loss: 3.65 | perplexity: 38.49 | lr: 2.72e-04 | norm: 0.2957 | dt: 14057.0312ms | tok/sec: 27.9729\n",
      "step 1916 | train loss: 3.75 | val loss: 3.65 | perplexity: 38.42 | lr: 2.71e-04 | norm: 0.3220 | dt: 14044.5662ms | tok/sec: 27.9977\n",
      "step 1917 | train loss: 3.70 | val loss: 3.65 | perplexity: 38.46 | lr: 2.70e-04 | norm: 0.3037 | dt: 14072.4685ms | tok/sec: 27.9422\n",
      "step 1918 | train loss: 3.65 | val loss: 3.65 | perplexity: 38.58 | lr: 2.70e-04 | norm: 0.2756 | dt: 14072.0086ms | tok/sec: 27.9431\n",
      "step 1919 | train loss: 3.67 | val loss: 3.65 | perplexity: 38.63 | lr: 2.69e-04 | norm: 0.2949 | dt: 14067.7168ms | tok/sec: 27.9517\n",
      "step 1920 | train loss: 3.79 | val loss: 3.65 | perplexity: 38.48 | lr: 2.68e-04 | norm: 0.3252 | dt: 14068.4998ms | tok/sec: 27.9501\n",
      "step 1921 | train loss: 3.72 | val loss: 3.65 | perplexity: 38.38 | lr: 2.68e-04 | norm: 0.2783 | dt: 14050.4065ms | tok/sec: 27.9861\n",
      "step 1922 | train loss: 3.51 | val loss: 3.65 | perplexity: 38.40 | lr: 2.67e-04 | norm: 0.2871 | dt: 14070.0154ms | tok/sec: 27.9471\n",
      "step 1923 | train loss: 3.56 | val loss: 3.65 | perplexity: 38.34 | lr: 2.67e-04 | norm: 0.3137 | dt: 14067.9491ms | tok/sec: 27.9512\n",
      "step 1924 | train loss: 3.78 | val loss: 3.65 | perplexity: 38.37 | lr: 2.66e-04 | norm: 0.2917 | dt: 14072.6125ms | tok/sec: 27.9419\n",
      "step 1925 | train loss: 3.89 | val loss: 3.65 | perplexity: 38.35 | lr: 2.65e-04 | norm: 0.2876 | dt: 14058.4607ms | tok/sec: 27.9701\n",
      "step 1926 | train loss: 4.17 | val loss: 3.65 | perplexity: 38.44 | lr: 2.65e-04 | norm: 0.3526 | dt: 14034.7993ms | tok/sec: 28.0172\n",
      "step 1927 | train loss: 3.71 | val loss: 3.65 | perplexity: 38.56 | lr: 2.64e-04 | norm: 0.2711 | dt: 14051.1231ms | tok/sec: 27.9847\n",
      "step 1928 | train loss: 3.39 | val loss: 3.65 | perplexity: 38.57 | lr: 2.63e-04 | norm: 0.4074 | dt: 14060.5917ms | tok/sec: 27.9658\n",
      "step 1929 | train loss: 3.85 | val loss: 3.66 | perplexity: 38.68 | lr: 2.63e-04 | norm: 0.2778 | dt: 14039.0189ms | tok/sec: 28.0088\n",
      "step 1930 | train loss: 3.50 | val loss: 3.66 | perplexity: 38.67 | lr: 2.62e-04 | norm: 0.3227 | dt: 14067.1551ms | tok/sec: 27.9528\n",
      "step 1931 | train loss: 3.58 | val loss: 3.65 | perplexity: 38.48 | lr: 2.61e-04 | norm: 0.3230 | dt: 14048.3801ms | tok/sec: 27.9901\n",
      "step 1932 | train loss: 3.84 | val loss: 3.65 | perplexity: 38.38 | lr: 2.61e-04 | norm: 0.2905 | dt: 14054.9300ms | tok/sec: 27.9771\n",
      "step 1933 | train loss: 3.66 | val loss: 3.65 | perplexity: 38.46 | lr: 2.60e-04 | norm: 0.3074 | dt: 14048.0242ms | tok/sec: 27.9908\n",
      "step 1934 | train loss: 3.78 | val loss: 3.65 | perplexity: 38.64 | lr: 2.59e-04 | norm: 0.3113 | dt: 14066.8747ms | tok/sec: 27.9533\n",
      "step 1935 | train loss: 3.77 | val loss: 3.66 | perplexity: 38.74 | lr: 2.59e-04 | norm: 0.3109 | dt: 14066.6125ms | tok/sec: 27.9539\n",
      "step 1936 | train loss: 3.97 | val loss: 3.66 | perplexity: 38.80 | lr: 2.58e-04 | norm: 0.3102 | dt: 14085.3980ms | tok/sec: 27.9166\n",
      "step 1937 | train loss: 3.69 | val loss: 3.66 | perplexity: 38.80 | lr: 2.58e-04 | norm: 0.2900 | dt: 14073.5960ms | tok/sec: 27.9400\n",
      "step 1938 | train loss: 3.63 | val loss: 3.66 | perplexity: 38.72 | lr: 2.57e-04 | norm: 0.3426 | dt: 14062.2122ms | tok/sec: 27.9626\n",
      "step 1939 | train loss: 3.43 | val loss: 3.65 | perplexity: 38.61 | lr: 2.56e-04 | norm: 0.2932 | dt: 14083.2627ms | tok/sec: 27.9208\n",
      "step 1940 | train loss: 3.92 | val loss: 3.65 | perplexity: 38.57 | lr: 2.56e-04 | norm: 0.3357 | dt: 14061.8243ms | tok/sec: 27.9634\n",
      "step 1941 | train loss: 3.73 | val loss: 3.65 | perplexity: 38.51 | lr: 2.55e-04 | norm: 0.3031 | dt: 14039.4893ms | tok/sec: 28.0079\n",
      "step 1942 | train loss: 3.55 | val loss: 3.65 | perplexity: 38.39 | lr: 2.54e-04 | norm: 0.3234 | dt: 14075.5947ms | tok/sec: 27.9360\n",
      "step 1943 | train loss: 3.46 | val loss: 3.65 | perplexity: 38.39 | lr: 2.54e-04 | norm: 0.3425 | dt: 14043.0534ms | tok/sec: 28.0007\n",
      "step 1944 | train loss: 3.63 | val loss: 3.65 | perplexity: 38.51 | lr: 2.53e-04 | norm: 0.2952 | dt: 14042.2893ms | tok/sec: 28.0023\n",
      "step 1945 | train loss: 3.49 | val loss: 3.65 | perplexity: 38.66 | lr: 2.52e-04 | norm: 0.3207 | dt: 14056.3800ms | tok/sec: 27.9742\n",
      "step 1946 | train loss: 3.70 | val loss: 3.65 | perplexity: 38.64 | lr: 2.52e-04 | norm: 0.3320 | dt: 14065.1033ms | tok/sec: 27.9569\n",
      "step 1947 | train loss: 3.86 | val loss: 3.65 | perplexity: 38.58 | lr: 2.51e-04 | norm: 0.3183 | dt: 14073.0908ms | tok/sec: 27.9410\n",
      "step 1948 | train loss: 3.98 | val loss: 3.65 | perplexity: 38.60 | lr: 2.51e-04 | norm: 0.3733 | dt: 14086.7302ms | tok/sec: 27.9139\n",
      "step 1949 | train loss: 3.51 | val loss: 3.66 | perplexity: 38.67 | lr: 2.50e-04 | norm: 0.3440 | dt: 14034.1094ms | tok/sec: 28.0186\n",
      "step 1950 | train loss: 3.87 | val loss: 3.66 | perplexity: 38.72 | lr: 2.49e-04 | norm: 0.3140 | dt: 14006.7070ms | tok/sec: 28.0734\n",
      "step 1951 | train loss: 3.89 | val loss: 3.66 | perplexity: 38.72 | lr: 2.49e-04 | norm: 0.3201 | dt: 13994.4308ms | tok/sec: 28.0980\n",
      "step 1952 | train loss: 3.75 | val loss: 3.66 | perplexity: 38.76 | lr: 2.48e-04 | norm: 0.2971 | dt: 14021.6572ms | tok/sec: 28.0435\n",
      "step 1953 | train loss: 4.02 | val loss: 3.66 | perplexity: 38.86 | lr: 2.47e-04 | norm: 0.3384 | dt: 14001.0469ms | tok/sec: 28.0848\n",
      "step 1954 | train loss: 3.81 | val loss: 3.66 | perplexity: 38.90 | lr: 2.47e-04 | norm: 0.2811 | dt: 14005.3568ms | tok/sec: 28.0761\n",
      "step 1955 | train loss: 3.87 | val loss: 3.66 | perplexity: 38.85 | lr: 2.46e-04 | norm: 0.2935 | dt: 13986.5208ms | tok/sec: 28.1139\n",
      "step 1956 | train loss: 3.85 | val loss: 3.66 | perplexity: 38.70 | lr: 2.46e-04 | norm: 0.2980 | dt: 14013.6032ms | tok/sec: 28.0596\n",
      "step 1957 | train loss: 3.84 | val loss: 3.65 | perplexity: 38.63 | lr: 2.45e-04 | norm: 0.2619 | dt: 13997.3853ms | tok/sec: 28.0921\n",
      "step 1958 | train loss: 4.05 | val loss: 3.65 | perplexity: 38.57 | lr: 2.44e-04 | norm: 0.3443 | dt: 13996.4693ms | tok/sec: 28.0939\n",
      "step 1959 | train loss: 3.83 | val loss: 3.65 | perplexity: 38.62 | lr: 2.44e-04 | norm: 0.3746 | dt: 14010.2491ms | tok/sec: 28.0663\n",
      "step 1960 | train loss: 3.90 | val loss: 3.66 | perplexity: 38.74 | lr: 2.43e-04 | norm: 0.3316 | dt: 14038.6202ms | tok/sec: 28.0096\n",
      "step 1961 | train loss: 3.88 | val loss: 3.66 | perplexity: 38.79 | lr: 2.43e-04 | norm: 0.3270 | dt: 13993.0823ms | tok/sec: 28.1007\n",
      "step 1962 | train loss: 3.91 | val loss: 3.66 | perplexity: 38.83 | lr: 2.42e-04 | norm: 0.3577 | dt: 14020.7255ms | tok/sec: 28.0453\n",
      "step 1963 | train loss: 3.70 | val loss: 3.66 | perplexity: 38.90 | lr: 2.41e-04 | norm: 0.3256 | dt: 13999.9721ms | tok/sec: 28.0869\n",
      "step 1964 | train loss: 3.68 | val loss: 3.66 | perplexity: 38.97 | lr: 2.41e-04 | norm: 0.3167 | dt: 13992.1546ms | tok/sec: 28.1026\n",
      "step 1965 | train loss: 4.07 | val loss: 3.66 | perplexity: 38.87 | lr: 2.40e-04 | norm: 0.4272 | dt: 14011.4632ms | tok/sec: 28.0639\n",
      "step 1966 | train loss: 3.96 | val loss: 3.66 | perplexity: 38.77 | lr: 2.39e-04 | norm: 0.3293 | dt: 14052.2430ms | tok/sec: 27.9824\n",
      "step 1967 | train loss: 3.97 | val loss: 3.66 | perplexity: 38.69 | lr: 2.39e-04 | norm: 0.3441 | dt: 14015.7490ms | tok/sec: 28.0553\n",
      "step 1968 | train loss: 3.57 | val loss: 3.65 | perplexity: 38.65 | lr: 2.38e-04 | norm: 0.4304 | dt: 14010.8759ms | tok/sec: 28.0651\n",
      "step 1969 | train loss: 3.77 | val loss: 3.66 | perplexity: 38.74 | lr: 2.38e-04 | norm: 0.3157 | dt: 14040.3032ms | tok/sec: 28.0062\n",
      "step 1970 | train loss: 3.58 | val loss: 3.66 | perplexity: 38.85 | lr: 2.37e-04 | norm: 0.3220 | dt: 14019.7723ms | tok/sec: 28.0472\n",
      "step 1971 | train loss: 3.54 | val loss: 3.66 | perplexity: 38.75 | lr: 2.36e-04 | norm: 0.3577 | dt: 14041.7418ms | tok/sec: 28.0034\n",
      "step 1972 | train loss: 3.61 | val loss: 3.66 | perplexity: 38.67 | lr: 2.36e-04 | norm: 0.3355 | dt: 13992.4965ms | tok/sec: 28.1019\n",
      "step 1973 | train loss: 3.61 | val loss: 3.66 | perplexity: 38.67 | lr: 2.35e-04 | norm: 0.3442 | dt: 14008.7864ms | tok/sec: 28.0692\n",
      "step 1974 | train loss: 3.83 | val loss: 3.65 | perplexity: 38.66 | lr: 2.35e-04 | norm: 0.3804 | dt: 14024.0943ms | tok/sec: 28.0386\n",
      "step 1975 | train loss: 3.84 | val loss: 3.65 | perplexity: 38.66 | lr: 2.34e-04 | norm: 0.3293 | dt: 14022.5141ms | tok/sec: 28.0418\n",
      "step 1976 | train loss: 3.81 | val loss: 3.66 | perplexity: 38.69 | lr: 2.33e-04 | norm: 0.3034 | dt: 14026.7322ms | tok/sec: 28.0333\n",
      "step 1977 | train loss: 4.11 | val loss: 3.66 | perplexity: 38.72 | lr: 2.33e-04 | norm: 0.3711 | dt: 14036.1202ms | tok/sec: 28.0146\n",
      "step 1978 | train loss: 3.60 | val loss: 3.65 | perplexity: 38.66 | lr: 2.32e-04 | norm: 0.3310 | dt: 14043.4434ms | tok/sec: 28.0000\n",
      "step 1979 | train loss: 3.55 | val loss: 3.65 | perplexity: 38.57 | lr: 2.32e-04 | norm: 0.3502 | dt: 14035.2275ms | tok/sec: 28.0164\n",
      "step 1980 | train loss: 3.69 | val loss: 3.65 | perplexity: 38.56 | lr: 2.31e-04 | norm: 0.3020 | dt: 14026.2387ms | tok/sec: 28.0343\n",
      "step 1981 | train loss: 3.82 | val loss: 3.65 | perplexity: 38.58 | lr: 2.30e-04 | norm: 0.3265 | dt: 14424.5923ms | tok/sec: 27.2601\n",
      "step 1982 | train loss: 3.58 | val loss: 3.66 | perplexity: 38.73 | lr: 2.30e-04 | norm: 0.3010 | dt: 13992.4834ms | tok/sec: 28.1019\n",
      "step 1983 | train loss: 3.85 | val loss: 3.66 | perplexity: 38.75 | lr: 2.29e-04 | norm: 0.3509 | dt: 13997.9162ms | tok/sec: 28.0910\n",
      "step 1984 | train loss: 4.07 | val loss: 3.66 | perplexity: 38.67 | lr: 2.29e-04 | norm: 0.3250 | dt: 13998.4653ms | tok/sec: 28.0899\n",
      "step 1985 | train loss: 3.77 | val loss: 3.65 | perplexity: 38.61 | lr: 2.28e-04 | norm: 0.3170 | dt: 13997.3080ms | tok/sec: 28.0923\n",
      "step 1986 | train loss: 3.74 | val loss: 3.65 | perplexity: 38.60 | lr: 2.28e-04 | norm: 0.3019 | dt: 14031.0562ms | tok/sec: 28.0247\n",
      "step 1987 | train loss: 3.53 | val loss: 3.65 | perplexity: 38.48 | lr: 2.27e-04 | norm: 0.3086 | dt: 13999.8982ms | tok/sec: 28.0871\n",
      "step 1988 | train loss: 3.78 | val loss: 3.65 | perplexity: 38.33 | lr: 2.26e-04 | norm: 0.3296 | dt: 14011.6081ms | tok/sec: 28.0636\n",
      "step 1989 | train loss: 3.70 | val loss: 3.65 | perplexity: 38.35 | lr: 2.26e-04 | norm: 0.3013 | dt: 14001.3807ms | tok/sec: 28.0841\n",
      "step 1990 | train loss: 3.67 | val loss: 3.65 | perplexity: 38.42 | lr: 2.25e-04 | norm: 0.2828 | dt: 14025.9480ms | tok/sec: 28.0349\n",
      "step 1991 | train loss: 3.68 | val loss: 3.65 | perplexity: 38.45 | lr: 2.25e-04 | norm: 0.2738 | dt: 14005.4410ms | tok/sec: 28.0759\n",
      "step 1992 | train loss: 3.81 | val loss: 3.65 | perplexity: 38.42 | lr: 2.24e-04 | norm: 0.3150 | dt: 14033.3986ms | tok/sec: 28.0200\n",
      "step 1993 | train loss: 3.71 | val loss: 3.65 | perplexity: 38.36 | lr: 2.23e-04 | norm: 0.3041 | dt: 14031.8551ms | tok/sec: 28.0231\n",
      "step 1994 | train loss: 3.91 | val loss: 3.65 | perplexity: 38.35 | lr: 2.23e-04 | norm: 0.2682 | dt: 14017.8480ms | tok/sec: 28.0511\n",
      "step 1995 | train loss: 3.85 | val loss: 3.65 | perplexity: 38.29 | lr: 2.22e-04 | norm: 0.3217 | dt: 14011.8930ms | tok/sec: 28.0630\n",
      "step 1996 | train loss: 3.72 | val loss: 3.64 | perplexity: 38.22 | lr: 2.22e-04 | norm: 0.3309 | dt: 14004.3526ms | tok/sec: 28.0781\n",
      "step 1997 | train loss: 3.79 | val loss: 3.64 | perplexity: 38.24 | lr: 2.21e-04 | norm: 0.2828 | dt: 14025.5749ms | tok/sec: 28.0356\n",
      "step 1998 | train loss: 3.72 | val loss: 3.64 | perplexity: 38.23 | lr: 2.21e-04 | norm: 0.2781 | dt: 13993.0046ms | tok/sec: 28.1009\n",
      "step 1999 | train loss: 3.54 | val loss: 3.64 | perplexity: 38.18 | lr: 2.20e-04 | norm: 0.2758 | dt: 14018.5885ms | tok/sec: 28.0496\n",
      "step 2000 | train loss: 3.67 | val loss: 3.64 | perplexity: 38.08 | lr: 2.19e-04 | norm: 0.3263 | dt: 14016.9802ms | tok/sec: 28.0528\n",
      "step 2001 | train loss: 3.67 | val loss: 3.64 | perplexity: 38.15 | lr: 2.19e-04 | norm: 0.2952 | dt: 14015.6562ms | tok/sec: 28.0555\n",
      "step 2002 | train loss: 3.89 | val loss: 3.65 | perplexity: 38.32 | lr: 2.18e-04 | norm: 0.2825 | dt: 14011.9104ms | tok/sec: 28.0630\n",
      "step 2003 | train loss: 3.57 | val loss: 3.65 | perplexity: 38.32 | lr: 2.18e-04 | norm: 0.3841 | dt: 14019.5596ms | tok/sec: 28.0477\n",
      "step 2004 | train loss: 3.89 | val loss: 3.65 | perplexity: 38.33 | lr: 2.17e-04 | norm: 0.2840 | dt: 14009.2595ms | tok/sec: 28.0683\n",
      "step 2005 | train loss: 3.60 | val loss: 3.65 | perplexity: 38.35 | lr: 2.17e-04 | norm: 0.3128 | dt: 14007.2048ms | tok/sec: 28.0724\n",
      "step 2006 | train loss: 3.91 | val loss: 3.65 | perplexity: 38.34 | lr: 2.16e-04 | norm: 0.3366 | dt: 14008.1973ms | tok/sec: 28.0704\n",
      "step 2007 | train loss: 3.77 | val loss: 3.65 | perplexity: 38.31 | lr: 2.15e-04 | norm: 0.2952 | dt: 14033.3574ms | tok/sec: 28.0201\n",
      "step 2008 | train loss: 3.69 | val loss: 3.65 | perplexity: 38.29 | lr: 2.15e-04 | norm: 0.2892 | dt: 14008.9169ms | tok/sec: 28.0690\n",
      "step 2009 | train loss: 3.68 | val loss: 3.64 | perplexity: 38.23 | lr: 2.14e-04 | norm: 0.3383 | dt: 14047.9956ms | tok/sec: 27.9909\n",
      "step 2010 | train loss: 3.64 | val loss: 3.64 | perplexity: 38.24 | lr: 2.14e-04 | norm: 0.3411 | dt: 14002.2411ms | tok/sec: 28.0824\n",
      "step 2011 | train loss: 3.69 | val loss: 3.65 | perplexity: 38.29 | lr: 2.13e-04 | norm: 0.2830 | dt: 14021.3866ms | tok/sec: 28.0440\n",
      "step 2012 | train loss: 3.62 | val loss: 3.64 | perplexity: 38.24 | lr: 2.13e-04 | norm: 0.3244 | dt: 13986.7163ms | tok/sec: 28.1135\n",
      "step 2013 | train loss: 3.74 | val loss: 3.64 | perplexity: 38.18 | lr: 2.12e-04 | norm: 0.3390 | dt: 14011.1761ms | tok/sec: 28.0645\n",
      "step 2014 | train loss: 4.41 | val loss: 3.64 | perplexity: 38.16 | lr: 2.12e-04 | norm: 0.4981 | dt: 14000.1025ms | tok/sec: 28.0867\n",
      "step 2015 | train loss: 4.24 | val loss: 3.64 | perplexity: 38.27 | lr: 2.11e-04 | norm: 0.3808 | dt: 13981.3793ms | tok/sec: 28.1243\n",
      "step 2016 | train loss: 3.86 | val loss: 3.64 | perplexity: 38.28 | lr: 2.10e-04 | norm: 0.3588 | dt: 14000.0565ms | tok/sec: 28.0867\n",
      "step 2017 | train loss: 3.72 | val loss: 3.64 | perplexity: 38.25 | lr: 2.10e-04 | norm: 0.3833 | dt: 13981.7691ms | tok/sec: 28.1235\n",
      "step 2018 | train loss: 3.85 | val loss: 3.65 | perplexity: 38.30 | lr: 2.09e-04 | norm: 0.3441 | dt: 13996.0778ms | tok/sec: 28.0947\n",
      "step 2019 | train loss: 3.96 | val loss: 3.65 | perplexity: 38.33 | lr: 2.09e-04 | norm: 0.3348 | dt: 13998.0538ms | tok/sec: 28.0908\n",
      "step 2020 | train loss: 3.65 | val loss: 3.65 | perplexity: 38.28 | lr: 2.08e-04 | norm: 0.3396 | dt: 14012.4841ms | tok/sec: 28.0618\n",
      "step 2021 | train loss: 3.82 | val loss: 3.64 | perplexity: 38.16 | lr: 2.08e-04 | norm: 0.3651 | dt: 14017.6589ms | tok/sec: 28.0515\n",
      "step 2022 | train loss: 3.69 | val loss: 3.64 | perplexity: 38.10 | lr: 2.07e-04 | norm: 0.3512 | dt: 14009.8584ms | tok/sec: 28.0671\n",
      "step 2023 | train loss: 3.65 | val loss: 3.64 | perplexity: 38.08 | lr: 2.07e-04 | norm: 0.2856 | dt: 13990.0143ms | tok/sec: 28.1069\n",
      "step 2024 | train loss: 3.56 | val loss: 3.64 | perplexity: 38.06 | lr: 2.06e-04 | norm: 0.3186 | dt: 14008.3542ms | tok/sec: 28.0701\n",
      "step 2025 | train loss: 3.74 | val loss: 3.64 | perplexity: 38.06 | lr: 2.06e-04 | norm: 0.3244 | dt: 14011.4043ms | tok/sec: 28.0640\n",
      "step 2026 | train loss: 3.74 | val loss: 3.64 | perplexity: 38.11 | lr: 2.05e-04 | norm: 0.3637 | dt: 13986.0287ms | tok/sec: 28.1149\n",
      "step 2027 | train loss: 3.68 | val loss: 3.64 | perplexity: 38.17 | lr: 2.05e-04 | norm: 0.3047 | dt: 14041.4927ms | tok/sec: 28.0039\n",
      "step 2028 | train loss: 4.00 | val loss: 3.64 | perplexity: 38.24 | lr: 2.04e-04 | norm: 0.3369 | dt: 13991.9686ms | tok/sec: 28.1030\n",
      "step 2029 | train loss: 3.46 | val loss: 3.64 | perplexity: 38.11 | lr: 2.03e-04 | norm: 0.3666 | dt: 14011.5836ms | tok/sec: 28.0636\n",
      "step 2030 | train loss: 3.55 | val loss: 3.64 | perplexity: 38.01 | lr: 2.03e-04 | norm: 0.3402 | dt: 14008.8351ms | tok/sec: 28.0691\n",
      "step 2031 | train loss: 3.65 | val loss: 3.64 | perplexity: 38.06 | lr: 2.02e-04 | norm: 0.3358 | dt: 14011.5089ms | tok/sec: 28.0638\n",
      "step 2032 | train loss: 3.64 | val loss: 3.64 | perplexity: 38.14 | lr: 2.02e-04 | norm: 0.3105 | dt: 13988.4963ms | tok/sec: 28.1100\n",
      "step 2033 | train loss: 3.66 | val loss: 3.64 | perplexity: 37.90 | lr: 2.01e-04 | norm: 0.5060 | dt: 13983.5489ms | tok/sec: 28.1199\n",
      "step 2034 | train loss: 3.51 | val loss: 3.63 | perplexity: 37.64 | lr: 2.01e-04 | norm: 0.3259 | dt: 14008.5728ms | tok/sec: 28.0697\n",
      "step 2035 | train loss: 3.42 | val loss: 3.63 | perplexity: 37.62 | lr: 2.00e-04 | norm: 0.3589 | dt: 13997.8259ms | tok/sec: 28.0912\n",
      "step 2036 | train loss: 3.36 | val loss: 3.63 | perplexity: 37.75 | lr: 2.00e-04 | norm: 0.3058 | dt: 13996.8319ms | tok/sec: 28.0932\n",
      "step 2037 | train loss: 3.62 | val loss: 3.63 | perplexity: 37.63 | lr: 1.99e-04 | norm: 0.3891 | dt: 14005.8796ms | tok/sec: 28.0751\n",
      "step 2038 | train loss: 3.25 | val loss: 3.63 | perplexity: 37.53 | lr: 1.99e-04 | norm: 0.2952 | dt: 13968.8380ms | tok/sec: 28.1495\n",
      "step 2039 | train loss: 4.86 | val loss: 3.62 | perplexity: 37.52 | lr: 1.98e-04 | norm: 0.4983 | dt: 13980.3240ms | tok/sec: 28.1264\n",
      "step 2040 | train loss: 3.45 | val loss: 3.62 | perplexity: 37.48 | lr: 1.98e-04 | norm: 0.3312 | dt: 14013.0956ms | tok/sec: 28.0606\n",
      "step 2041 | train loss: 3.58 | val loss: 3.62 | perplexity: 37.38 | lr: 1.97e-04 | norm: 0.3264 | dt: 14012.3715ms | tok/sec: 28.0621\n",
      "step 2042 | train loss: 3.64 | val loss: 3.62 | perplexity: 37.40 | lr: 1.97e-04 | norm: 0.3932 | dt: 14008.5621ms | tok/sec: 28.0697\n",
      "step 2043 | train loss: 3.57 | val loss: 3.63 | perplexity: 37.55 | lr: 1.96e-04 | norm: 0.2796 | dt: 13981.4837ms | tok/sec: 28.1241\n",
      "step 2044 | train loss: 3.30 | val loss: 3.63 | perplexity: 37.66 | lr: 1.96e-04 | norm: 0.3870 | dt: 13990.5446ms | tok/sec: 28.1058\n",
      "step 2045 | train loss: 3.43 | val loss: 3.63 | perplexity: 37.76 | lr: 1.95e-04 | norm: 0.5016 | dt: 13994.7307ms | tok/sec: 28.0974\n",
      "step 2046 | train loss: 3.71 | val loss: 3.63 | perplexity: 37.78 | lr: 1.95e-04 | norm: 0.3462 | dt: 13987.8812ms | tok/sec: 28.1112\n",
      "step 2047 | train loss: 3.55 | val loss: 3.63 | perplexity: 37.75 | lr: 1.94e-04 | norm: 0.2805 | dt: 13984.0806ms | tok/sec: 28.1188\n",
      "step 2048 | train loss: 3.63 | val loss: 3.63 | perplexity: 37.69 | lr: 1.93e-04 | norm: 0.3311 | dt: 13996.7842ms | tok/sec: 28.0933\n",
      "step 2049 | train loss: 3.65 | val loss: 3.63 | perplexity: 37.62 | lr: 1.93e-04 | norm: 0.3129 | dt: 13971.2057ms | tok/sec: 28.1447\n",
      "step 2050 | train loss: 3.41 | val loss: 3.62 | perplexity: 37.52 | lr: 1.92e-04 | norm: 0.3254 | dt: 13983.2311ms | tok/sec: 28.1205\n",
      "step 2051 | train loss: 3.10 | val loss: 3.62 | perplexity: 37.45 | lr: 1.92e-04 | norm: 0.3603 | dt: 13973.0988ms | tok/sec: 28.1409\n",
      "step 2052 | train loss: 3.47 | val loss: 3.62 | perplexity: 37.40 | lr: 1.91e-04 | norm: 0.3012 | dt: 13967.1795ms | tok/sec: 28.1529\n",
      "step 2053 | train loss: 3.94 | val loss: 3.62 | perplexity: 37.32 | lr: 1.91e-04 | norm: 0.3825 | dt: 13987.9839ms | tok/sec: 28.1110\n",
      "step 2054 | train loss: 3.42 | val loss: 3.62 | perplexity: 37.29 | lr: 1.90e-04 | norm: 0.4009 | dt: 14009.0907ms | tok/sec: 28.0686\n",
      "step 2055 | train loss: 3.34 | val loss: 3.62 | perplexity: 37.39 | lr: 1.90e-04 | norm: 0.3101 | dt: 13986.7837ms | tok/sec: 28.1134\n",
      "step 2056 | train loss: 3.94 | val loss: 3.62 | perplexity: 37.45 | lr: 1.89e-04 | norm: 0.3863 | dt: 13998.9731ms | tok/sec: 28.0889\n",
      "step 2057 | train loss: 3.50 | val loss: 3.62 | perplexity: 37.47 | lr: 1.89e-04 | norm: 0.3857 | dt: 14016.3074ms | tok/sec: 28.0542\n",
      "step 2058 | train loss: 3.31 | val loss: 3.62 | perplexity: 37.49 | lr: 1.88e-04 | norm: 0.2806 | dt: 14002.4891ms | tok/sec: 28.0819\n",
      "step 2059 | train loss: 3.57 | val loss: 3.62 | perplexity: 37.45 | lr: 1.88e-04 | norm: 0.2940 | dt: 13990.7353ms | tok/sec: 28.1055\n",
      "step 2060 | train loss: 3.39 | val loss: 3.62 | perplexity: 37.42 | lr: 1.87e-04 | norm: 0.3004 | dt: 13991.8122ms | tok/sec: 28.1033\n",
      "step 2061 | train loss: 3.42 | val loss: 3.62 | perplexity: 37.41 | lr: 1.87e-04 | norm: 0.2869 | dt: 13992.7793ms | tok/sec: 28.1014\n",
      "step 2062 | train loss: 3.37 | val loss: 3.62 | perplexity: 37.34 | lr: 1.86e-04 | norm: 0.3933 | dt: 13991.4727ms | tok/sec: 28.1040\n",
      "step 2063 | train loss: 3.46 | val loss: 3.62 | perplexity: 37.37 | lr: 1.86e-04 | norm: 0.3125 | dt: 14017.7066ms | tok/sec: 28.0514\n",
      "step 2064 | train loss: 3.51 | val loss: 3.62 | perplexity: 37.43 | lr: 1.85e-04 | norm: 0.3291 | dt: 13995.2269ms | tok/sec: 28.0964\n",
      "step 2065 | train loss: 3.30 | val loss: 3.62 | perplexity: 37.50 | lr: 1.85e-04 | norm: 0.3258 | dt: 14026.0861ms | tok/sec: 28.0346\n",
      "step 2066 | train loss: 4.00 | val loss: 3.62 | perplexity: 37.51 | lr: 1.84e-04 | norm: 0.3836 | dt: 13990.5260ms | tok/sec: 28.1059\n",
      "step 2067 | train loss: 3.48 | val loss: 3.62 | perplexity: 37.51 | lr: 1.84e-04 | norm: 0.3411 | dt: 14024.3032ms | tok/sec: 28.0382\n",
      "step 2068 | train loss: 3.59 | val loss: 3.62 | perplexity: 37.48 | lr: 1.83e-04 | norm: 0.3010 | dt: 14010.7329ms | tok/sec: 28.0653\n",
      "step 2069 | train loss: 3.68 | val loss: 3.62 | perplexity: 37.46 | lr: 1.83e-04 | norm: 0.3246 | dt: 13996.5236ms | tok/sec: 28.0938\n",
      "step 2070 | train loss: 3.67 | val loss: 3.62 | perplexity: 37.48 | lr: 1.83e-04 | norm: 0.3554 | dt: 14008.5588ms | tok/sec: 28.0697\n",
      "step 2071 | train loss: 3.75 | val loss: 3.63 | perplexity: 37.56 | lr: 1.82e-04 | norm: 0.3287 | dt: 13992.7151ms | tok/sec: 28.1015\n",
      "step 2072 | train loss: 3.32 | val loss: 3.63 | perplexity: 37.58 | lr: 1.82e-04 | norm: 0.2933 | dt: 13995.0824ms | tok/sec: 28.0967\n",
      "step 2073 | train loss: 3.64 | val loss: 3.62 | perplexity: 37.37 | lr: 1.81e-04 | norm: 0.4684 | dt: 14015.8529ms | tok/sec: 28.0551\n",
      "step 2074 | train loss: 3.45 | val loss: 3.62 | perplexity: 37.27 | lr: 1.81e-04 | norm: 0.3936 | dt: 13980.9809ms | tok/sec: 28.1251\n",
      "step 2075 | train loss: 3.60 | val loss: 3.62 | perplexity: 37.36 | lr: 1.80e-04 | norm: 0.2973 | dt: 13991.9116ms | tok/sec: 28.1031\n",
      "step 2076 | train loss: 3.53 | val loss: 3.62 | perplexity: 37.37 | lr: 1.80e-04 | norm: 0.3568 | dt: 13984.7803ms | tok/sec: 28.1174\n",
      "step 2077 | train loss: 3.64 | val loss: 3.62 | perplexity: 37.29 | lr: 1.79e-04 | norm: 0.3446 | dt: 14000.4029ms | tok/sec: 28.0860\n",
      "step 2078 | train loss: 3.69 | val loss: 3.62 | perplexity: 37.25 | lr: 1.79e-04 | norm: 0.3169 | dt: 13996.4297ms | tok/sec: 28.0940\n",
      "step 2079 | train loss: 3.39 | val loss: 3.62 | perplexity: 37.21 | lr: 1.78e-04 | norm: 0.2849 | dt: 13993.2897ms | tok/sec: 28.1003\n",
      "step 2080 | train loss: 3.68 | val loss: 3.62 | perplexity: 37.16 | lr: 1.78e-04 | norm: 0.3140 | dt: 14010.1473ms | tok/sec: 28.0665\n",
      "step 2081 | train loss: 3.62 | val loss: 3.61 | perplexity: 37.08 | lr: 1.77e-04 | norm: 0.3559 | dt: 13991.3232ms | tok/sec: 28.1043\n",
      "step 2082 | train loss: 3.42 | val loss: 3.61 | perplexity: 37.08 | lr: 1.77e-04 | norm: 0.3099 | dt: 13989.5909ms | tok/sec: 28.1078\n",
      "step 2083 | train loss: 3.94 | val loss: 3.62 | perplexity: 37.15 | lr: 1.76e-04 | norm: 0.3484 | dt: 14000.1843ms | tok/sec: 28.0865\n",
      "step 2084 | train loss: 3.87 | val loss: 3.62 | perplexity: 37.18 | lr: 1.76e-04 | norm: 0.4031 | dt: 14029.9325ms | tok/sec: 28.0269\n",
      "step 2085 | train loss: 4.31 | val loss: 3.61 | perplexity: 37.14 | lr: 1.75e-04 | norm: 0.4392 | dt: 14002.7568ms | tok/sec: 28.0813\n",
      "step 2086 | train loss: 3.83 | val loss: 3.61 | perplexity: 37.13 | lr: 1.75e-04 | norm: 0.3161 | dt: 14013.5911ms | tok/sec: 28.0596\n",
      "step 2087 | train loss: 3.49 | val loss: 3.61 | perplexity: 37.11 | lr: 1.74e-04 | norm: 0.3489 | dt: 14009.7077ms | tok/sec: 28.0674\n",
      "step 2088 | train loss: 3.54 | val loss: 3.61 | perplexity: 37.02 | lr: 1.74e-04 | norm: 0.4013 | dt: 13984.4713ms | tok/sec: 28.1180\n",
      "step 2089 | train loss: 3.38 | val loss: 3.61 | perplexity: 37.00 | lr: 1.74e-04 | norm: 0.2646 | dt: 14014.4615ms | tok/sec: 28.0579\n",
      "step 2090 | train loss: 3.49 | val loss: 3.61 | perplexity: 36.98 | lr: 1.73e-04 | norm: 0.2827 | dt: 14025.9390ms | tok/sec: 28.0349\n",
      "step 2091 | train loss: 3.59 | val loss: 3.61 | perplexity: 36.91 | lr: 1.73e-04 | norm: 0.3367 | dt: 14025.0483ms | tok/sec: 28.0367\n",
      "step 2092 | train loss: 3.60 | val loss: 3.61 | perplexity: 36.86 | lr: 1.72e-04 | norm: 0.2967 | dt: 14001.1990ms | tok/sec: 28.0845\n",
      "step 2093 | train loss: 3.67 | val loss: 3.61 | perplexity: 36.85 | lr: 1.72e-04 | norm: 0.2925 | dt: 14004.1571ms | tok/sec: 28.0785\n",
      "step 2094 | train loss: 3.87 | val loss: 3.61 | perplexity: 36.86 | lr: 1.71e-04 | norm: 0.3547 | dt: 14004.5328ms | tok/sec: 28.0778\n",
      "step 2095 | train loss: 3.80 | val loss: 3.61 | perplexity: 36.91 | lr: 1.71e-04 | norm: 0.5998 | dt: 14016.1624ms | tok/sec: 28.0545\n",
      "step 2096 | train loss: 3.60 | val loss: 3.61 | perplexity: 37.00 | lr: 1.70e-04 | norm: 0.2971 | dt: 14010.0672ms | tok/sec: 28.0667\n",
      "step 2097 | train loss: 3.46 | val loss: 3.61 | perplexity: 37.15 | lr: 1.70e-04 | norm: 0.3763 | dt: 14026.9284ms | tok/sec: 28.0329\n",
      "step 2098 | train loss: 3.55 | val loss: 3.62 | perplexity: 37.21 | lr: 1.69e-04 | norm: 0.3132 | dt: 13988.9638ms | tok/sec: 28.1090\n",
      "step 2099 | train loss: 3.61 | val loss: 3.62 | perplexity: 37.18 | lr: 1.69e-04 | norm: 0.3107 | dt: 14006.2361ms | tok/sec: 28.0744\n",
      "step 2100 | train loss: 3.36 | val loss: 3.61 | perplexity: 37.06 | lr: 1.69e-04 | norm: 0.3141 | dt: 14032.3598ms | tok/sec: 28.0221\n",
      "step 2101 | train loss: 3.67 | val loss: 3.61 | perplexity: 36.95 | lr: 1.68e-04 | norm: 0.3126 | dt: 14018.8842ms | tok/sec: 28.0490\n",
      "step 2102 | train loss: 3.46 | val loss: 3.61 | perplexity: 36.88 | lr: 1.68e-04 | norm: 0.3149 | dt: 14003.2649ms | tok/sec: 28.0803\n",
      "step 2103 | train loss: 3.49 | val loss: 3.61 | perplexity: 36.83 | lr: 1.67e-04 | norm: 0.2683 | dt: 14026.3369ms | tok/sec: 28.0341\n",
      "step 2104 | train loss: 3.62 | val loss: 3.61 | perplexity: 36.80 | lr: 1.67e-04 | norm: 0.3500 | dt: 13984.8847ms | tok/sec: 28.1172\n",
      "step 2105 | train loss: 3.81 | val loss: 3.61 | perplexity: 36.83 | lr: 1.66e-04 | norm: 0.3516 | dt: 13992.6744ms | tok/sec: 28.1016\n",
      "step 2106 | train loss: 3.73 | val loss: 3.61 | perplexity: 36.85 | lr: 1.66e-04 | norm: 0.2875 | dt: 14000.9952ms | tok/sec: 28.0849\n",
      "step 2107 | train loss: 3.35 | val loss: 3.61 | perplexity: 36.86 | lr: 1.65e-04 | norm: 0.2900 | dt: 14035.6824ms | tok/sec: 28.0155\n",
      "step 2108 | train loss: 3.47 | val loss: 3.61 | perplexity: 36.83 | lr: 1.65e-04 | norm: 0.3852 | dt: 14010.4229ms | tok/sec: 28.0660\n",
      "step 2109 | train loss: 3.52 | val loss: 3.61 | perplexity: 36.85 | lr: 1.65e-04 | norm: 0.3487 | dt: 14006.6941ms | tok/sec: 28.0734\n",
      "step 2110 | train loss: 3.46 | val loss: 3.61 | perplexity: 36.86 | lr: 1.64e-04 | norm: 0.2738 | dt: 13998.3253ms | tok/sec: 28.0902\n",
      "step 2111 | train loss: 3.61 | val loss: 3.61 | perplexity: 36.85 | lr: 1.64e-04 | norm: 0.2752 | dt: 14004.2646ms | tok/sec: 28.0783\n",
      "step 2112 | train loss: 4.06 | val loss: 3.61 | perplexity: 36.81 | lr: 1.63e-04 | norm: 0.3101 | dt: 14019.6640ms | tok/sec: 28.0475\n",
      "step 2113 | train loss: 3.84 | val loss: 3.61 | perplexity: 36.83 | lr: 1.63e-04 | norm: 0.2522 | dt: 13996.0632ms | tok/sec: 28.0948\n",
      "step 2114 | train loss: 3.85 | val loss: 3.61 | perplexity: 36.82 | lr: 1.62e-04 | norm: 0.3108 | dt: 14009.9018ms | tok/sec: 28.0670\n",
      "step 2115 | train loss: 3.89 | val loss: 3.60 | perplexity: 36.78 | lr: 1.62e-04 | norm: 0.2920 | dt: 14000.3867ms | tok/sec: 28.0861\n",
      "step 2116 | train loss: 3.72 | val loss: 3.60 | perplexity: 36.69 | lr: 1.62e-04 | norm: 0.3518 | dt: 14007.9803ms | tok/sec: 28.0709\n",
      "step 2117 | train loss: 3.58 | val loss: 3.60 | perplexity: 36.65 | lr: 1.61e-04 | norm: 0.2905 | dt: 14027.4673ms | tok/sec: 28.0319\n",
      "step 2118 | train loss: 3.43 | val loss: 3.60 | perplexity: 36.68 | lr: 1.61e-04 | norm: 0.2927 | dt: 14008.4138ms | tok/sec: 28.0700\n",
      "step 2119 | train loss: 3.93 | val loss: 3.60 | perplexity: 36.74 | lr: 1.60e-04 | norm: 0.5092 | dt: 14016.2122ms | tok/sec: 28.0544\n",
      "step 2120 | train loss: 3.99 | val loss: 3.61 | perplexity: 36.82 | lr: 1.60e-04 | norm: 0.4235 | dt: 14001.5075ms | tok/sec: 28.0838\n",
      "step 2121 | train loss: 3.41 | val loss: 3.61 | perplexity: 36.85 | lr: 1.60e-04 | norm: 0.3277 | dt: 14021.8961ms | tok/sec: 28.0430\n",
      "step 2122 | train loss: 3.89 | val loss: 3.60 | perplexity: 36.76 | lr: 1.59e-04 | norm: 0.4048 | dt: 14027.7524ms | tok/sec: 28.0313\n",
      "step 2123 | train loss: 3.41 | val loss: 3.60 | perplexity: 36.66 | lr: 1.59e-04 | norm: 0.4745 | dt: 13999.8076ms | tok/sec: 28.0872\n",
      "step 2124 | train loss: 3.75 | val loss: 3.60 | perplexity: 36.68 | lr: 1.58e-04 | norm: 0.2829 | dt: 14029.7894ms | tok/sec: 28.0272\n",
      "step 2125 | train loss: 3.44 | val loss: 3.60 | perplexity: 36.76 | lr: 1.58e-04 | norm: 0.3415 | dt: 14001.7898ms | tok/sec: 28.0833\n",
      "step 2126 | train loss: 3.29 | val loss: 3.61 | perplexity: 36.87 | lr: 1.57e-04 | norm: 0.3005 | dt: 14008.4565ms | tok/sec: 28.0699\n",
      "step 2127 | train loss: 3.44 | val loss: 3.61 | perplexity: 36.89 | lr: 1.57e-04 | norm: 0.3350 | dt: 14026.0940ms | tok/sec: 28.0346\n",
      "step 2128 | train loss: 3.57 | val loss: 3.61 | perplexity: 36.85 | lr: 1.57e-04 | norm: 0.3075 | dt: 14019.2347ms | tok/sec: 28.0483\n",
      "step 2129 | train loss: 3.55 | val loss: 3.61 | perplexity: 36.79 | lr: 1.56e-04 | norm: 0.3023 | dt: 14001.0672ms | tok/sec: 28.0847\n",
      "step 2130 | train loss: 3.65 | val loss: 3.60 | perplexity: 36.72 | lr: 1.56e-04 | norm: 0.2936 | dt: 14020.8850ms | tok/sec: 28.0450\n",
      "step 2131 | train loss: 3.48 | val loss: 3.60 | perplexity: 36.67 | lr: 1.55e-04 | norm: 0.3108 | dt: 14958.3795ms | tok/sec: 26.2873\n",
      "step 2132 | train loss: 3.77 | val loss: 3.60 | perplexity: 36.67 | lr: 1.55e-04 | norm: 0.3083 | dt: 13906.8696ms | tok/sec: 28.2749\n",
      "step 2133 | train loss: 3.69 | val loss: 3.60 | perplexity: 36.68 | lr: 1.55e-04 | norm: 0.2954 | dt: 14129.7317ms | tok/sec: 27.8290\n",
      "step 2134 | train loss: 3.41 | val loss: 3.60 | perplexity: 36.68 | lr: 1.54e-04 | norm: 0.3354 | dt: 13902.9860ms | tok/sec: 28.2828\n",
      "step 2135 | train loss: 3.63 | val loss: 3.60 | perplexity: 36.65 | lr: 1.54e-04 | norm: 0.3094 | dt: 14366.1432ms | tok/sec: 27.3710\n",
      "step 2136 | train loss: 3.56 | val loss: 3.60 | perplexity: 36.60 | lr: 1.53e-04 | norm: 0.2965 | dt: 14538.6584ms | tok/sec: 27.0462\n",
      "step 2137 | train loss: 3.62 | val loss: 3.60 | perplexity: 36.60 | lr: 1.53e-04 | norm: 0.2675 | dt: 14549.6354ms | tok/sec: 27.0258\n",
      "step 2138 | train loss: 3.42 | val loss: 3.60 | perplexity: 36.66 | lr: 1.53e-04 | norm: 0.2719 | dt: 13898.2306ms | tok/sec: 28.2925\n",
      "step 2139 | train loss: 3.46 | val loss: 3.60 | perplexity: 36.69 | lr: 1.52e-04 | norm: 0.2755 | dt: 13800.0548ms | tok/sec: 28.4938\n",
      "step 2140 | train loss: 3.61 | val loss: 3.60 | perplexity: 36.69 | lr: 1.52e-04 | norm: 0.3102 | dt: 13897.2123ms | tok/sec: 28.2946\n",
      "step 2141 | train loss: 3.71 | val loss: 3.60 | perplexity: 36.67 | lr: 1.51e-04 | norm: 0.3515 | dt: 13845.1591ms | tok/sec: 28.4010\n",
      "step 2142 | train loss: 3.41 | val loss: 3.60 | perplexity: 36.66 | lr: 1.51e-04 | norm: 0.3124 | dt: 13969.4691ms | tok/sec: 28.1482\n",
      "step 2143 | train loss: 3.78 | val loss: 3.60 | perplexity: 36.62 | lr: 1.51e-04 | norm: 0.3815 | dt: 13799.3553ms | tok/sec: 28.4952\n",
      "step 2144 | train loss: 3.60 | val loss: 3.60 | perplexity: 36.58 | lr: 1.50e-04 | norm: 0.3085 | dt: 13882.2055ms | tok/sec: 28.3252\n",
      "step 2145 | train loss: 3.61 | val loss: 3.60 | perplexity: 36.60 | lr: 1.50e-04 | norm: 0.2514 | dt: 13933.3031ms | tok/sec: 28.2213\n",
      "step 2146 | train loss: 3.16 | val loss: 3.60 | perplexity: 36.66 | lr: 1.50e-04 | norm: 0.3279 | dt: 13925.7581ms | tok/sec: 28.2366\n",
      "step 2147 | train loss: 3.87 | val loss: 3.60 | perplexity: 36.66 | lr: 1.49e-04 | norm: 0.3285 | dt: 13983.1676ms | tok/sec: 28.1207\n",
      "step 2148 | train loss: 3.58 | val loss: 3.60 | perplexity: 36.67 | lr: 1.49e-04 | norm: 0.2957 | dt: 14023.2346ms | tok/sec: 28.0403\n",
      "step 2149 | train loss: 4.03 | val loss: 3.60 | perplexity: 36.72 | lr: 1.48e-04 | norm: 0.2794 | dt: 13885.1042ms | tok/sec: 28.3193\n",
      "step 2150 | train loss: 3.52 | val loss: 3.60 | perplexity: 36.75 | lr: 1.48e-04 | norm: 0.2625 | dt: 13734.0727ms | tok/sec: 28.6307\n",
      "step 2151 | train loss: 3.53 | val loss: 3.60 | perplexity: 36.68 | lr: 1.48e-04 | norm: 0.3044 | dt: 13741.1485ms | tok/sec: 28.6159\n",
      "step 2152 | train loss: 3.63 | val loss: 3.60 | perplexity: 36.61 | lr: 1.47e-04 | norm: 0.2954 | dt: 13710.1946ms | tok/sec: 28.6806\n",
      "step 2153 | train loss: 3.64 | val loss: 3.60 | perplexity: 36.55 | lr: 1.47e-04 | norm: 0.2844 | dt: 13956.0742ms | tok/sec: 28.1753\n",
      "step 2154 | train loss: 3.53 | val loss: 3.60 | perplexity: 36.54 | lr: 1.47e-04 | norm: 0.3076 | dt: 13788.8949ms | tok/sec: 28.5169\n",
      "step 2155 | train loss: 3.74 | val loss: 3.60 | perplexity: 36.59 | lr: 1.46e-04 | norm: 0.4040 | dt: 13796.4137ms | tok/sec: 28.5013\n",
      "step 2156 | train loss: 3.75 | val loss: 3.60 | perplexity: 36.65 | lr: 1.46e-04 | norm: 0.2565 | dt: 13725.0242ms | tok/sec: 28.6496\n",
      "step 2157 | train loss: 3.72 | val loss: 3.60 | perplexity: 36.66 | lr: 1.45e-04 | norm: 0.3140 | dt: 13702.6601ms | tok/sec: 28.6963\n",
      "step 2158 | train loss: 4.23 | val loss: 3.60 | perplexity: 36.68 | lr: 1.45e-04 | norm: 0.3286 | dt: 13797.4126ms | tok/sec: 28.4993\n",
      "step 2159 | train loss: 3.55 | val loss: 3.60 | perplexity: 36.70 | lr: 1.45e-04 | norm: 0.2358 | dt: 13887.0728ms | tok/sec: 28.3153\n",
      "step 2160 | train loss: 3.60 | val loss: 3.60 | perplexity: 36.69 | lr: 1.44e-04 | norm: 0.2504 | dt: 13729.3022ms | tok/sec: 28.6406\n",
      "step 2161 | train loss: 3.69 | val loss: 3.60 | perplexity: 36.65 | lr: 1.44e-04 | norm: 0.2796 | dt: 13774.2610ms | tok/sec: 28.5472\n",
      "step 2162 | train loss: 3.74 | val loss: 3.60 | perplexity: 36.62 | lr: 1.44e-04 | norm: 0.2686 | dt: 13704.6850ms | tok/sec: 28.6921\n",
      "step 2163 | train loss: 3.69 | val loss: 3.60 | perplexity: 36.64 | lr: 1.43e-04 | norm: 0.2911 | dt: 13811.9411ms | tok/sec: 28.4693\n",
      "step 2164 | train loss: 3.69 | val loss: 3.60 | perplexity: 36.65 | lr: 1.43e-04 | norm: 0.3080 | dt: 13747.8390ms | tok/sec: 28.6020\n",
      "step 2165 | train loss: 3.76 | val loss: 3.60 | perplexity: 36.65 | lr: 1.43e-04 | norm: 0.2910 | dt: 13715.6374ms | tok/sec: 28.6692\n",
      "step 2166 | train loss: 3.29 | val loss: 3.60 | perplexity: 36.58 | lr: 1.42e-04 | norm: 0.3777 | dt: 13848.9084ms | tok/sec: 28.3933\n",
      "step 2167 | train loss: 3.49 | val loss: 3.60 | perplexity: 36.48 | lr: 1.42e-04 | norm: 0.3083 | dt: 13726.4700ms | tok/sec: 28.6465\n",
      "step 2168 | train loss: 3.67 | val loss: 3.60 | perplexity: 36.44 | lr: 1.41e-04 | norm: 0.3784 | dt: 13744.9961ms | tok/sec: 28.6079\n",
      "step 2169 | train loss: 3.69 | val loss: 3.60 | perplexity: 36.45 | lr: 1.41e-04 | norm: 0.2834 | dt: 13753.2597ms | tok/sec: 28.5907\n",
      "step 2170 | train loss: 3.46 | val loss: 3.60 | perplexity: 36.49 | lr: 1.41e-04 | norm: 0.3448 | dt: 13873.9085ms | tok/sec: 28.3421\n",
      "step 2171 | train loss: 3.76 | val loss: 3.60 | perplexity: 36.53 | lr: 1.40e-04 | norm: 0.3236 | dt: 13751.1315ms | tok/sec: 28.5952\n",
      "step 2172 | train loss: 3.73 | val loss: 3.60 | perplexity: 36.59 | lr: 1.40e-04 | norm: 0.2961 | dt: 13830.0552ms | tok/sec: 28.4320\n",
      "step 2173 | train loss: 3.87 | val loss: 3.60 | perplexity: 36.61 | lr: 1.40e-04 | norm: 0.2878 | dt: 13699.6627ms | tok/sec: 28.7026\n",
      "step 2174 | train loss: 3.72 | val loss: 3.60 | perplexity: 36.55 | lr: 1.39e-04 | norm: 0.3340 | dt: 13714.4883ms | tok/sec: 28.6716\n",
      "step 2175 | train loss: 3.94 | val loss: 3.60 | perplexity: 36.50 | lr: 1.39e-04 | norm: 0.3043 | dt: 13801.8386ms | tok/sec: 28.4901\n",
      "step 2176 | train loss: 3.47 | val loss: 3.60 | perplexity: 36.43 | lr: 1.39e-04 | norm: 0.2711 | dt: 13917.0520ms | tok/sec: 28.2543\n",
      "step 2177 | train loss: 3.61 | val loss: 3.59 | perplexity: 36.34 | lr: 1.38e-04 | norm: 0.2713 | dt: 13741.9932ms | tok/sec: 28.6142\n",
      "step 2178 | train loss: 3.81 | val loss: 3.59 | perplexity: 36.30 | lr: 1.38e-04 | norm: 0.2972 | dt: 13508.9874ms | tok/sec: 29.1077\n",
      "step 2179 | train loss: 3.58 | val loss: 3.59 | perplexity: 36.27 | lr: 1.38e-04 | norm: 0.3120 | dt: 13818.6250ms | tok/sec: 28.4555\n",
      "step 2180 | train loss: 3.86 | val loss: 3.59 | perplexity: 36.24 | lr: 1.37e-04 | norm: 0.2912 | dt: 13527.1661ms | tok/sec: 29.0686\n",
      "step 2181 | train loss: 3.59 | val loss: 3.59 | perplexity: 36.24 | lr: 1.37e-04 | norm: 0.3269 | dt: 13563.0133ms | tok/sec: 28.9918\n",
      "step 2182 | train loss: 3.91 | val loss: 3.59 | perplexity: 36.27 | lr: 1.37e-04 | norm: 0.2840 | dt: 13715.3671ms | tok/sec: 28.6697\n",
      "step 2183 | train loss: 3.69 | val loss: 3.59 | perplexity: 36.31 | lr: 1.36e-04 | norm: 0.3020 | dt: 13564.3072ms | tok/sec: 28.9890\n",
      "step 2184 | train loss: 3.60 | val loss: 3.59 | perplexity: 36.35 | lr: 1.36e-04 | norm: 0.2929 | dt: 13639.9674ms | tok/sec: 28.8282\n",
      "step 2185 | train loss: 3.96 | val loss: 3.59 | perplexity: 36.33 | lr: 1.36e-04 | norm: 0.3264 | dt: 13949.8305ms | tok/sec: 28.1879\n",
      "step 2186 | train loss: 3.22 | val loss: 3.59 | perplexity: 36.29 | lr: 1.35e-04 | norm: 0.3162 | dt: 14001.0397ms | tok/sec: 28.0848\n",
      "step 2187 | train loss: 3.90 | val loss: 3.59 | perplexity: 36.27 | lr: 1.35e-04 | norm: 0.3152 | dt: 13637.1088ms | tok/sec: 28.8343\n",
      "step 2188 | train loss: 3.80 | val loss: 3.59 | perplexity: 36.30 | lr: 1.35e-04 | norm: 0.2894 | dt: 13696.3172ms | tok/sec: 28.7096\n",
      "step 2189 | train loss: 3.94 | val loss: 3.59 | perplexity: 36.34 | lr: 1.34e-04 | norm: 0.5687 | dt: 13722.6403ms | tok/sec: 28.6545\n",
      "step 2190 | train loss: 3.87 | val loss: 3.59 | perplexity: 36.39 | lr: 1.34e-04 | norm: 0.2993 | dt: 13663.5699ms | tok/sec: 28.7784\n",
      "step 2191 | train loss: 3.71 | val loss: 3.60 | perplexity: 36.44 | lr: 1.34e-04 | norm: 0.2977 | dt: 13859.9141ms | tok/sec: 28.3707\n",
      "step 2192 | train loss: 4.14 | val loss: 3.60 | perplexity: 36.43 | lr: 1.33e-04 | norm: 0.3026 | dt: 13731.9541ms | tok/sec: 28.6351\n",
      "step 2193 | train loss: 3.89 | val loss: 3.59 | perplexity: 36.39 | lr: 1.33e-04 | norm: 0.3356 | dt: 13840.5409ms | tok/sec: 28.4105\n",
      "step 2194 | train loss: 3.57 | val loss: 3.59 | perplexity: 36.33 | lr: 1.33e-04 | norm: 0.2888 | dt: 13852.7877ms | tok/sec: 28.3853\n",
      "step 2195 | train loss: 3.89 | val loss: 3.59 | perplexity: 36.29 | lr: 1.32e-04 | norm: 0.2857 | dt: 13800.8265ms | tok/sec: 28.4922\n",
      "step 2196 | train loss: 3.73 | val loss: 3.59 | perplexity: 36.29 | lr: 1.32e-04 | norm: 0.2513 | dt: 14021.3816ms | tok/sec: 28.0440\n",
      "step 2197 | train loss: 3.82 | val loss: 3.59 | perplexity: 36.27 | lr: 1.32e-04 | norm: 0.3242 | dt: 13766.1932ms | tok/sec: 28.5639\n",
      "step 2198 | train loss: 3.62 | val loss: 3.59 | perplexity: 36.30 | lr: 1.32e-04 | norm: 0.2963 | dt: 13817.2934ms | tok/sec: 28.4583\n",
      "step 2199 | train loss: 3.86 | val loss: 3.59 | perplexity: 36.31 | lr: 1.31e-04 | norm: 0.2634 | dt: 13795.7294ms | tok/sec: 28.5027\n",
      "step 2200 | train loss: 4.48 | val loss: 3.59 | perplexity: 36.33 | lr: 1.31e-04 | norm: 0.6126 | dt: 13691.0808ms | tok/sec: 28.7206\n",
      "step 2201 | train loss: 3.56 | val loss: 3.59 | perplexity: 36.34 | lr: 1.31e-04 | norm: 0.2858 | dt: 13839.7715ms | tok/sec: 28.4120\n",
      "step 2202 | train loss: 3.90 | val loss: 3.59 | perplexity: 36.34 | lr: 1.30e-04 | norm: 0.3181 | dt: 14373.7435ms | tok/sec: 27.3565\n",
      "step 2203 | train loss: 3.77 | val loss: 3.59 | perplexity: 36.36 | lr: 1.30e-04 | norm: 0.2584 | dt: 14201.2920ms | tok/sec: 27.6887\n",
      "step 2204 | train loss: 3.60 | val loss: 3.59 | perplexity: 36.38 | lr: 1.30e-04 | norm: 0.2411 | dt: 14139.4022ms | tok/sec: 27.8099\n",
      "step 2205 | train loss: 3.80 | val loss: 3.59 | perplexity: 36.37 | lr: 1.29e-04 | norm: 0.3635 | dt: 13987.5555ms | tok/sec: 28.1118\n",
      "step 2206 | train loss: 3.72 | val loss: 3.59 | perplexity: 36.34 | lr: 1.29e-04 | norm: 0.2912 | dt: 14212.4095ms | tok/sec: 27.6671\n",
      "step 2207 | train loss: 3.36 | val loss: 3.59 | perplexity: 36.30 | lr: 1.29e-04 | norm: 0.2987 | dt: 14102.4430ms | tok/sec: 27.8828\n",
      "step 2208 | train loss: 3.45 | val loss: 3.59 | perplexity: 36.26 | lr: 1.29e-04 | norm: 0.2883 | dt: 14229.3911ms | tok/sec: 27.6341\n",
      "step 2209 | train loss: 3.66 | val loss: 3.59 | perplexity: 36.22 | lr: 1.28e-04 | norm: 0.3021 | dt: 14284.8368ms | tok/sec: 27.5268\n",
      "step 2210 | train loss: 3.50 | val loss: 3.59 | perplexity: 36.18 | lr: 1.28e-04 | norm: 0.2778 | dt: 14324.8916ms | tok/sec: 27.4498\n",
      "step 2211 | train loss: 4.18 | val loss: 3.59 | perplexity: 36.28 | lr: 1.28e-04 | norm: 0.5844 | dt: 14331.8558ms | tok/sec: 27.4365\n",
      "step 2212 | train loss: 3.68 | val loss: 3.60 | perplexity: 36.51 | lr: 1.27e-04 | norm: 0.2437 | dt: 14355.9251ms | tok/sec: 27.3905\n",
      "step 2213 | train loss: 3.60 | val loss: 3.60 | perplexity: 36.59 | lr: 1.27e-04 | norm: 0.3627 | dt: 14063.8270ms | tok/sec: 27.9594\n",
      "step 2214 | train loss: 3.68 | val loss: 3.60 | perplexity: 36.62 | lr: 1.27e-04 | norm: 0.3539 | dt: 13719.8210ms | tok/sec: 28.6604\n",
      "step 2215 | train loss: 3.97 | val loss: 3.60 | perplexity: 36.62 | lr: 1.27e-04 | norm: 0.4168 | dt: 13741.5733ms | tok/sec: 28.6151\n",
      "step 2216 | train loss: 3.50 | val loss: 3.60 | perplexity: 36.60 | lr: 1.26e-04 | norm: 0.3732 | dt: 13748.1227ms | tok/sec: 28.6014\n",
      "step 2217 | train loss: 3.83 | val loss: 3.60 | perplexity: 36.62 | lr: 1.26e-04 | norm: 0.3071 | dt: 13689.5621ms | tok/sec: 28.7238\n",
      "step 2218 | train loss: 3.67 | val loss: 3.60 | perplexity: 36.56 | lr: 1.26e-04 | norm: 0.3442 | dt: 13800.2317ms | tok/sec: 28.4934\n",
      "step 2219 | train loss: 3.99 | val loss: 3.60 | perplexity: 36.51 | lr: 1.25e-04 | norm: 0.3932 | dt: 13797.0695ms | tok/sec: 28.5000\n",
      "step 2220 | train loss: 3.44 | val loss: 3.60 | perplexity: 36.46 | lr: 1.25e-04 | norm: 0.3130 | dt: 13792.2795ms | tok/sec: 28.5099\n",
      "step 2221 | train loss: 3.72 | val loss: 3.59 | perplexity: 36.41 | lr: 1.25e-04 | norm: 0.2798 | dt: 13803.5071ms | tok/sec: 28.4867\n",
      "step 2222 | train loss: 3.70 | val loss: 3.59 | perplexity: 36.38 | lr: 1.25e-04 | norm: 0.3118 | dt: 13844.6193ms | tok/sec: 28.4021\n",
      "step 2223 | train loss: 3.49 | val loss: 3.59 | perplexity: 36.36 | lr: 1.24e-04 | norm: 0.2768 | dt: 13757.8821ms | tok/sec: 28.5811\n",
      "step 2224 | train loss: 3.54 | val loss: 3.59 | perplexity: 36.35 | lr: 1.24e-04 | norm: 0.2550 | dt: 13678.2103ms | tok/sec: 28.7476\n",
      "step 2225 | train loss: 3.40 | val loss: 3.59 | perplexity: 36.36 | lr: 1.24e-04 | norm: 0.3196 | dt: 13888.2661ms | tok/sec: 28.3128\n",
      "step 2226 | train loss: 3.84 | val loss: 3.59 | perplexity: 36.32 | lr: 1.23e-04 | norm: 0.2776 | dt: 13665.2775ms | tok/sec: 28.7748\n",
      "step 2227 | train loss: 3.76 | val loss: 3.59 | perplexity: 36.23 | lr: 1.23e-04 | norm: 0.3202 | dt: 13846.5729ms | tok/sec: 28.3981\n",
      "step 2228 | train loss: 3.71 | val loss: 3.58 | perplexity: 36.02 | lr: 1.23e-04 | norm: 0.3276 | dt: 13814.7888ms | tok/sec: 28.4634\n",
      "step 2229 | train loss: 3.68 | val loss: 3.58 | perplexity: 35.83 | lr: 1.23e-04 | norm: 0.2929 | dt: 13875.6995ms | tok/sec: 28.3385\n",
      "step 2230 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.69 | lr: 1.22e-04 | norm: 0.2801 | dt: 13613.0552ms | tok/sec: 28.8852\n",
      "step 2231 | train loss: 3.74 | val loss: 3.57 | perplexity: 35.54 | lr: 1.22e-04 | norm: 0.3518 | dt: 13786.8192ms | tok/sec: 28.5212\n",
      "step 2232 | train loss: 3.55 | val loss: 3.57 | perplexity: 35.41 | lr: 1.22e-04 | norm: 0.3263 | dt: 13816.3016ms | tok/sec: 28.4603\n",
      "step 2233 | train loss: 3.57 | val loss: 3.56 | perplexity: 35.33 | lr: 1.22e-04 | norm: 0.2969 | dt: 13774.8210ms | tok/sec: 28.5460\n",
      "step 2234 | train loss: 3.64 | val loss: 3.56 | perplexity: 35.32 | lr: 1.21e-04 | norm: 0.2945 | dt: 13699.2042ms | tok/sec: 28.7036\n",
      "step 2235 | train loss: 3.98 | val loss: 3.57 | perplexity: 35.35 | lr: 1.21e-04 | norm: 0.3386 | dt: 13632.9703ms | tok/sec: 28.8430\n",
      "step 2236 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.37 | lr: 1.21e-04 | norm: 0.2919 | dt: 13789.5648ms | tok/sec: 28.5155\n",
      "step 2237 | train loss: 3.73 | val loss: 3.57 | perplexity: 35.38 | lr: 1.21e-04 | norm: 0.3376 | dt: 13759.6588ms | tok/sec: 28.5775\n",
      "step 2238 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.39 | lr: 1.20e-04 | norm: 0.3092 | dt: 13759.8801ms | tok/sec: 28.5770\n",
      "step 2239 | train loss: 3.60 | val loss: 3.57 | perplexity: 35.41 | lr: 1.20e-04 | norm: 0.2787 | dt: 13947.1829ms | tok/sec: 28.1932\n",
      "step 2240 | train loss: 3.60 | val loss: 3.57 | perplexity: 35.42 | lr: 1.20e-04 | norm: 0.3131 | dt: 13713.6929ms | tok/sec: 28.6732\n",
      "step 2241 | train loss: 3.89 | val loss: 3.57 | perplexity: 35.42 | lr: 1.20e-04 | norm: 0.2609 | dt: 13790.9584ms | tok/sec: 28.5126\n",
      "step 2242 | train loss: 3.59 | val loss: 3.57 | perplexity: 35.38 | lr: 1.19e-04 | norm: 0.2551 | dt: 13721.5927ms | tok/sec: 28.6567\n",
      "step 2243 | train loss: 3.65 | val loss: 3.56 | perplexity: 35.32 | lr: 1.19e-04 | norm: 0.2758 | dt: 13770.5195ms | tok/sec: 28.5549\n",
      "step 2244 | train loss: 3.99 | val loss: 3.56 | perplexity: 35.27 | lr: 1.19e-04 | norm: 0.2741 | dt: 13820.3385ms | tok/sec: 28.4520\n",
      "step 2245 | train loss: 3.82 | val loss: 3.56 | perplexity: 35.23 | lr: 1.19e-04 | norm: 0.2967 | dt: 13761.6043ms | tok/sec: 28.5734\n",
      "step 2246 | train loss: 3.58 | val loss: 3.56 | perplexity: 35.22 | lr: 1.18e-04 | norm: 0.2720 | dt: 13719.7773ms | tok/sec: 28.6605\n",
      "step 2247 | train loss: 3.58 | val loss: 3.56 | perplexity: 35.23 | lr: 1.18e-04 | norm: 0.3350 | dt: 13808.0094ms | tok/sec: 28.4774\n",
      "step 2248 | train loss: 4.28 | val loss: 3.56 | perplexity: 35.27 | lr: 1.18e-04 | norm: 0.8556 | dt: 13744.2231ms | tok/sec: 28.6095\n",
      "step 2249 | train loss: 3.69 | val loss: 3.56 | perplexity: 35.29 | lr: 1.18e-04 | norm: 0.2933 | dt: 13722.5609ms | tok/sec: 28.6547\n",
      "step 2250 | train loss: 3.61 | val loss: 3.56 | perplexity: 35.31 | lr: 1.17e-04 | norm: 0.2732 | dt: 13823.4258ms | tok/sec: 28.4456\n",
      "step 2251 | train loss: 3.58 | val loss: 3.56 | perplexity: 35.31 | lr: 1.17e-04 | norm: 0.3229 | dt: 13774.3945ms | tok/sec: 28.5469\n",
      "step 2252 | train loss: 3.69 | val loss: 3.56 | perplexity: 35.34 | lr: 1.17e-04 | norm: 0.2816 | dt: 13818.9785ms | tok/sec: 28.4548\n",
      "step 2253 | train loss: 3.70 | val loss: 3.57 | perplexity: 35.38 | lr: 1.17e-04 | norm: 0.2853 | dt: 13768.0507ms | tok/sec: 28.5600\n",
      "step 2254 | train loss: 3.85 | val loss: 3.57 | perplexity: 35.38 | lr: 1.17e-04 | norm: 0.3135 | dt: 13663.8887ms | tok/sec: 28.7778\n",
      "step 2255 | train loss: 3.88 | val loss: 3.56 | perplexity: 35.33 | lr: 1.16e-04 | norm: 0.3179 | dt: 14001.3676ms | tok/sec: 28.0841\n",
      "step 2256 | train loss: 3.64 | val loss: 3.56 | perplexity: 35.25 | lr: 1.16e-04 | norm: 0.2932 | dt: 14208.9949ms | tok/sec: 27.6737\n",
      "step 2257 | train loss: 3.69 | val loss: 3.56 | perplexity: 35.21 | lr: 1.16e-04 | norm: 0.2600 | dt: 13750.2894ms | tok/sec: 28.5969\n",
      "step 2258 | train loss: 3.96 | val loss: 3.56 | perplexity: 35.20 | lr: 1.16e-04 | norm: 0.2783 | dt: 13677.7484ms | tok/sec: 28.7486\n",
      "step 2259 | train loss: 3.88 | val loss: 3.56 | perplexity: 35.21 | lr: 1.15e-04 | norm: 0.4026 | dt: 13636.6060ms | tok/sec: 28.8353\n",
      "step 2260 | train loss: 3.96 | val loss: 3.56 | perplexity: 35.28 | lr: 1.15e-04 | norm: 0.3479 | dt: 13742.9469ms | tok/sec: 28.6122\n",
      "step 2261 | train loss: 3.86 | val loss: 3.57 | perplexity: 35.39 | lr: 1.15e-04 | norm: 0.3421 | dt: 13678.8201ms | tok/sec: 28.7463\n",
      "step 2262 | train loss: 3.68 | val loss: 3.57 | perplexity: 35.42 | lr: 1.15e-04 | norm: 0.3549 | dt: 13765.9540ms | tok/sec: 28.5644\n",
      "step 2263 | train loss: 3.96 | val loss: 3.57 | perplexity: 35.44 | lr: 1.15e-04 | norm: 0.3699 | dt: 13644.9380ms | tok/sec: 28.8177\n",
      "step 2264 | train loss: 3.48 | val loss: 3.57 | perplexity: 35.42 | lr: 1.14e-04 | norm: 0.3211 | dt: 13610.2784ms | tok/sec: 28.8911\n",
      "step 2265 | train loss: 3.93 | val loss: 3.57 | perplexity: 35.38 | lr: 1.14e-04 | norm: 0.3428 | dt: 13824.8858ms | tok/sec: 28.4426\n",
      "step 2266 | train loss: 4.02 | val loss: 3.57 | perplexity: 35.36 | lr: 1.14e-04 | norm: 0.3588 | dt: 13776.1106ms | tok/sec: 28.5433\n",
      "step 2267 | train loss: 3.92 | val loss: 3.57 | perplexity: 35.36 | lr: 1.14e-04 | norm: 0.3576 | dt: 13702.4109ms | tok/sec: 28.6968\n",
      "step 2268 | train loss: 3.63 | val loss: 3.57 | perplexity: 35.36 | lr: 1.14e-04 | norm: 0.3046 | dt: 13510.6435ms | tok/sec: 29.1042\n",
      "step 2269 | train loss: 4.07 | val loss: 3.57 | perplexity: 35.34 | lr: 1.13e-04 | norm: 0.3733 | dt: 13443.9485ms | tok/sec: 29.2485\n",
      "step 2270 | train loss: 3.81 | val loss: 3.56 | perplexity: 35.32 | lr: 1.13e-04 | norm: 0.2927 | dt: 13511.1003ms | tok/sec: 29.1032\n",
      "step 2271 | train loss: 3.80 | val loss: 3.56 | perplexity: 35.33 | lr: 1.13e-04 | norm: 0.3176 | dt: 13737.6161ms | tok/sec: 28.6233\n",
      "step 2272 | train loss: 3.48 | val loss: 3.56 | perplexity: 35.32 | lr: 1.13e-04 | norm: 0.3098 | dt: 13608.6538ms | tok/sec: 28.8946\n",
      "step 2273 | train loss: 3.86 | val loss: 3.56 | perplexity: 35.29 | lr: 1.13e-04 | norm: 0.3519 | dt: 13802.2618ms | tok/sec: 28.4892\n",
      "step 2274 | train loss: 3.63 | val loss: 3.56 | perplexity: 35.31 | lr: 1.12e-04 | norm: 0.2925 | dt: 13822.5787ms | tok/sec: 28.4474\n",
      "step 2275 | train loss: 3.55 | val loss: 3.57 | perplexity: 35.35 | lr: 1.12e-04 | norm: 0.2902 | dt: 13603.9593ms | tok/sec: 28.9045\n",
      "step 2276 | train loss: 3.73 | val loss: 3.57 | perplexity: 35.37 | lr: 1.12e-04 | norm: 0.3200 | dt: 13626.6370ms | tok/sec: 28.8564\n",
      "step 2277 | train loss: 3.73 | val loss: 3.57 | perplexity: 35.40 | lr: 1.12e-04 | norm: 0.3455 | dt: 13454.3831ms | tok/sec: 29.2259\n",
      "step 2278 | train loss: 3.27 | val loss: 3.57 | perplexity: 35.42 | lr: 1.12e-04 | norm: 0.2701 | dt: 13653.0991ms | tok/sec: 28.8005\n",
      "step 2279 | train loss: 3.61 | val loss: 3.57 | perplexity: 35.40 | lr: 1.11e-04 | norm: 0.3047 | dt: 13692.8513ms | tok/sec: 28.7169\n",
      "step 2280 | train loss: 3.76 | val loss: 3.57 | perplexity: 35.38 | lr: 1.11e-04 | norm: 0.2665 | dt: 13612.8447ms | tok/sec: 28.8857\n",
      "step 2281 | train loss: 3.65 | val loss: 3.57 | perplexity: 35.35 | lr: 1.11e-04 | norm: 0.3356 | dt: 14332.8753ms | tok/sec: 27.4346\n",
      "step 2282 | train loss: 3.75 | val loss: 3.57 | perplexity: 35.34 | lr: 1.11e-04 | norm: 0.3000 | dt: 14246.8808ms | tok/sec: 27.6001\n",
      "step 2283 | train loss: 3.78 | val loss: 3.57 | perplexity: 35.36 | lr: 1.11e-04 | norm: 0.2930 | dt: 14066.5362ms | tok/sec: 27.9540\n",
      "step 2284 | train loss: 3.66 | val loss: 3.57 | perplexity: 35.38 | lr: 1.10e-04 | norm: 0.2844 | dt: 13899.7407ms | tok/sec: 28.2894\n",
      "step 2285 | train loss: 3.69 | val loss: 3.57 | perplexity: 35.41 | lr: 1.10e-04 | norm: 0.2862 | dt: 14279.8407ms | tok/sec: 27.5364\n",
      "step 2286 | train loss: 3.58 | val loss: 3.57 | perplexity: 35.42 | lr: 1.10e-04 | norm: 0.2546 | dt: 14080.2195ms | tok/sec: 27.9268\n",
      "step 2287 | train loss: 3.52 | val loss: 3.57 | perplexity: 35.40 | lr: 1.10e-04 | norm: 0.2598 | dt: 14069.9260ms | tok/sec: 27.9473\n",
      "step 2288 | train loss: 3.70 | val loss: 3.57 | perplexity: 35.37 | lr: 1.10e-04 | norm: 0.3104 | dt: 14150.4996ms | tok/sec: 27.7881\n",
      "step 2289 | train loss: 3.75 | val loss: 3.56 | perplexity: 35.33 | lr: 1.10e-04 | norm: 0.3153 | dt: 14121.5379ms | tok/sec: 27.8451\n",
      "step 2290 | train loss: 3.48 | val loss: 3.56 | perplexity: 35.32 | lr: 1.09e-04 | norm: 0.2991 | dt: 14224.1786ms | tok/sec: 27.6442\n",
      "step 2291 | train loss: 3.68 | val loss: 3.56 | perplexity: 35.31 | lr: 1.09e-04 | norm: 0.2809 | dt: 13963.3212ms | tok/sec: 28.1606\n",
      "step 2292 | train loss: 3.71 | val loss: 3.56 | perplexity: 35.32 | lr: 1.09e-04 | norm: 0.2891 | dt: 14011.0533ms | tok/sec: 28.0647\n",
      "step 2293 | train loss: 3.68 | val loss: 3.56 | perplexity: 35.32 | lr: 1.09e-04 | norm: 0.2555 | dt: 14081.3878ms | tok/sec: 27.9245\n",
      "step 2294 | train loss: 3.60 | val loss: 3.57 | perplexity: 35.37 | lr: 1.09e-04 | norm: 0.3671 | dt: 14189.8148ms | tok/sec: 27.7111\n",
      "step 2295 | train loss: 3.61 | val loss: 3.57 | perplexity: 35.38 | lr: 1.09e-04 | norm: 0.2891 | dt: 13957.1595ms | tok/sec: 28.1731\n",
      "step 2296 | train loss: 3.79 | val loss: 3.57 | perplexity: 35.37 | lr: 1.08e-04 | norm: 0.3292 | dt: 13951.6847ms | tok/sec: 28.1841\n",
      "step 2297 | train loss: 3.89 | val loss: 3.57 | perplexity: 35.36 | lr: 1.08e-04 | norm: 0.3453 | dt: 14056.5386ms | tok/sec: 27.9739\n",
      "step 2298 | train loss: 3.60 | val loss: 3.57 | perplexity: 35.37 | lr: 1.08e-04 | norm: 0.3877 | dt: 14085.8130ms | tok/sec: 27.9157\n",
      "step 2299 | train loss: 3.41 | val loss: 3.57 | perplexity: 35.37 | lr: 1.08e-04 | norm: 0.3215 | dt: 13970.0904ms | tok/sec: 28.1470\n",
      "step 2300 | train loss: 3.44 | val loss: 3.57 | perplexity: 35.34 | lr: 1.08e-04 | norm: 0.2857 | dt: 14048.0630ms | tok/sec: 27.9908\n",
      "step 2301 | train loss: 3.48 | val loss: 3.57 | perplexity: 35.35 | lr: 1.08e-04 | norm: 0.2912 | dt: 14363.0998ms | tok/sec: 27.3768\n",
      "step 2302 | train loss: 3.47 | val loss: 3.57 | perplexity: 35.38 | lr: 1.07e-04 | norm: 0.2803 | dt: 14234.4103ms | tok/sec: 27.6243\n",
      "step 2303 | train loss: 3.48 | val loss: 3.57 | perplexity: 35.40 | lr: 1.07e-04 | norm: 0.3125 | dt: 14066.6475ms | tok/sec: 27.9538\n",
      "step 2304 | train loss: 4.05 | val loss: 3.57 | perplexity: 35.42 | lr: 1.07e-04 | norm: 0.3875 | dt: 14142.9324ms | tok/sec: 27.8030\n",
      "step 2305 | train loss: 3.82 | val loss: 3.57 | perplexity: 35.46 | lr: 1.07e-04 | norm: 0.2720 | dt: 14080.9758ms | tok/sec: 27.9253\n",
      "step 2306 | train loss: 3.53 | val loss: 3.57 | perplexity: 35.50 | lr: 1.07e-04 | norm: 0.2660 | dt: 14107.9361ms | tok/sec: 27.8720\n",
      "step 2307 | train loss: 3.79 | val loss: 3.57 | perplexity: 35.52 | lr: 1.07e-04 | norm: 0.2515 | dt: 14097.9714ms | tok/sec: 27.8917\n",
      "step 2308 | train loss: 3.80 | val loss: 3.57 | perplexity: 35.52 | lr: 1.07e-04 | norm: 0.7521 | dt: 14216.7652ms | tok/sec: 27.6586\n",
      "step 2309 | train loss: 3.67 | val loss: 3.57 | perplexity: 35.48 | lr: 1.06e-04 | norm: 0.2902 | dt: 14235.2011ms | tok/sec: 27.6228\n",
      "step 2310 | train loss: 3.26 | val loss: 3.57 | perplexity: 35.44 | lr: 1.06e-04 | norm: 0.3917 | dt: 14231.9996ms | tok/sec: 27.6290\n",
      "step 2311 | train loss: 3.81 | val loss: 3.57 | perplexity: 35.45 | lr: 1.06e-04 | norm: 0.3643 | dt: 14121.9769ms | tok/sec: 27.8443\n",
      "step 2312 | train loss: 3.70 | val loss: 3.57 | perplexity: 35.45 | lr: 1.06e-04 | norm: 0.2844 | dt: 14095.8383ms | tok/sec: 27.8959\n",
      "step 2313 | train loss: 3.72 | val loss: 3.57 | perplexity: 35.46 | lr: 1.06e-04 | norm: 0.3123 | dt: 14318.3103ms | tok/sec: 27.4625\n",
      "step 2314 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.46 | lr: 1.06e-04 | norm: 0.2978 | dt: 14056.8957ms | tok/sec: 27.9732\n",
      "step 2315 | train loss: 3.50 | val loss: 3.57 | perplexity: 35.45 | lr: 1.06e-04 | norm: 0.2711 | dt: 14167.9323ms | tok/sec: 27.7539\n",
      "step 2316 | train loss: 3.40 | val loss: 3.57 | perplexity: 35.45 | lr: 1.06e-04 | norm: 0.2552 | dt: 13797.1423ms | tok/sec: 28.4998\n",
      "step 2317 | train loss: 3.77 | val loss: 3.57 | perplexity: 35.44 | lr: 1.05e-04 | norm: 0.2791 | dt: 14307.9512ms | tok/sec: 27.4823\n",
      "step 2318 | train loss: 3.50 | val loss: 3.57 | perplexity: 35.44 | lr: 1.05e-04 | norm: 0.3456 | dt: 14043.8657ms | tok/sec: 27.9991\n",
      "step 2319 | train loss: 3.67 | val loss: 3.57 | perplexity: 35.45 | lr: 1.05e-04 | norm: 0.2943 | dt: 14022.2790ms | tok/sec: 28.0422\n",
      "step 2320 | train loss: 3.80 | val loss: 3.57 | perplexity: 35.44 | lr: 1.05e-04 | norm: 0.3086 | dt: 14029.6090ms | tok/sec: 28.0276\n",
      "step 2321 | train loss: 3.55 | val loss: 3.57 | perplexity: 35.43 | lr: 1.05e-04 | norm: 0.2697 | dt: 14026.8424ms | tok/sec: 28.0331\n",
      "step 2322 | train loss: 3.49 | val loss: 3.57 | perplexity: 35.43 | lr: 1.05e-04 | norm: 0.2712 | dt: 14010.2682ms | tok/sec: 28.0663\n",
      "step 2323 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.40 | lr: 1.05e-04 | norm: 0.2965 | dt: 14009.5980ms | tok/sec: 28.0676\n",
      "step 2324 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.40 | lr: 1.05e-04 | norm: 0.2364 | dt: 13986.2356ms | tok/sec: 28.1145\n",
      "step 2325 | train loss: 4.04 | val loss: 3.57 | perplexity: 35.39 | lr: 1.04e-04 | norm: 0.3979 | dt: 13980.4096ms | tok/sec: 28.1262\n",
      "step 2326 | train loss: 3.83 | val loss: 3.57 | perplexity: 35.38 | lr: 1.04e-04 | norm: 0.3132 | dt: 13981.0531ms | tok/sec: 28.1249\n",
      "step 2327 | train loss: 3.76 | val loss: 3.57 | perplexity: 35.37 | lr: 1.04e-04 | norm: 0.2849 | dt: 14219.7895ms | tok/sec: 27.6527\n",
      "step 2328 | train loss: 3.75 | val loss: 3.57 | perplexity: 35.37 | lr: 1.04e-04 | norm: 0.2986 | dt: 14190.0816ms | tok/sec: 27.7106\n",
      "step 2329 | train loss: 3.41 | val loss: 3.57 | perplexity: 35.37 | lr: 1.04e-04 | norm: 0.2801 | dt: 14139.4987ms | tok/sec: 27.8098\n",
      "step 2330 | train loss: 3.71 | val loss: 3.57 | perplexity: 35.35 | lr: 1.04e-04 | norm: 0.3104 | dt: 14202.4286ms | tok/sec: 27.6865\n",
      "step 2331 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.36 | lr: 1.04e-04 | norm: 0.3977 | dt: 14353.1351ms | tok/sec: 27.3958\n",
      "step 2332 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.38 | lr: 1.04e-04 | norm: 0.2740 | dt: 14324.0681ms | tok/sec: 27.4514\n",
      "step 2333 | train loss: 3.54 | val loss: 3.57 | perplexity: 35.40 | lr: 1.04e-04 | norm: 0.2930 | dt: 14435.0500ms | tok/sec: 27.2404\n",
      "step 2334 | train loss: 3.58 | val loss: 3.57 | perplexity: 35.39 | lr: 1.03e-04 | norm: 0.2599 | dt: 14331.2125ms | tok/sec: 27.4377\n",
      "step 2335 | train loss: 3.46 | val loss: 3.57 | perplexity: 35.38 | lr: 1.03e-04 | norm: 0.2843 | dt: 14265.8625ms | tok/sec: 27.5634\n",
      "step 2336 | train loss: 3.66 | val loss: 3.57 | perplexity: 35.37 | lr: 1.03e-04 | norm: 0.2451 | dt: 14332.3760ms | tok/sec: 27.4355\n",
      "step 2337 | train loss: 3.81 | val loss: 3.57 | perplexity: 35.34 | lr: 1.03e-04 | norm: 0.2924 | dt: 14332.2396ms | tok/sec: 27.4358\n",
      "step 2338 | train loss: 3.25 | val loss: 3.56 | perplexity: 35.32 | lr: 1.03e-04 | norm: 0.3385 | dt: 14200.6764ms | tok/sec: 27.6899\n",
      "step 2339 | train loss: 3.61 | val loss: 3.56 | perplexity: 35.30 | lr: 1.03e-04 | norm: 0.3291 | dt: 14167.6786ms | tok/sec: 27.7544\n",
      "step 2340 | train loss: 3.71 | val loss: 3.56 | perplexity: 35.31 | lr: 1.03e-04 | norm: 0.2980 | dt: 14226.7146ms | tok/sec: 27.6393\n",
      "step 2341 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.36 | lr: 1.03e-04 | norm: 0.2711 | dt: 14505.1289ms | tok/sec: 27.1088\n",
      "step 2342 | train loss: 3.84 | val loss: 3.57 | perplexity: 35.42 | lr: 1.03e-04 | norm: 0.3152 | dt: 14416.2638ms | tok/sec: 27.2759\n",
      "step 2343 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.46 | lr: 1.03e-04 | norm: 0.3046 | dt: 14541.4693ms | tok/sec: 27.0410\n",
      "step 2344 | train loss: 3.65 | val loss: 3.57 | perplexity: 35.50 | lr: 1.02e-04 | norm: 0.2777 | dt: 14565.6555ms | tok/sec: 26.9961\n",
      "step 2345 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.52 | lr: 1.02e-04 | norm: 0.2885 | dt: 14687.8979ms | tok/sec: 26.7714\n",
      "step 2346 | train loss: 3.74 | val loss: 3.57 | perplexity: 35.50 | lr: 1.02e-04 | norm: 0.2947 | dt: 14654.0828ms | tok/sec: 26.8332\n",
      "step 2347 | train loss: 3.97 | val loss: 3.57 | perplexity: 35.47 | lr: 1.02e-04 | norm: 0.4316 | dt: 14539.0103ms | tok/sec: 27.0456\n",
      "step 2348 | train loss: 3.53 | val loss: 3.57 | perplexity: 35.46 | lr: 1.02e-04 | norm: 0.2853 | dt: 14521.6401ms | tok/sec: 27.0779\n",
      "step 2349 | train loss: 3.54 | val loss: 3.57 | perplexity: 35.45 | lr: 1.02e-04 | norm: 0.2918 | dt: 14576.0019ms | tok/sec: 26.9769\n",
      "step 2350 | train loss: 3.50 | val loss: 3.57 | perplexity: 35.43 | lr: 1.02e-04 | norm: 0.3440 | dt: 14355.0761ms | tok/sec: 27.3921\n",
      "step 2351 | train loss: 3.53 | val loss: 3.57 | perplexity: 35.39 | lr: 1.02e-04 | norm: 0.3709 | dt: 14425.9300ms | tok/sec: 27.2576\n",
      "step 2352 | train loss: 3.52 | val loss: 3.57 | perplexity: 35.41 | lr: 1.02e-04 | norm: 0.2905 | dt: 14450.1143ms | tok/sec: 27.2120\n",
      "step 2353 | train loss: 3.85 | val loss: 3.57 | perplexity: 35.42 | lr: 1.02e-04 | norm: 0.3294 | dt: 14443.6362ms | tok/sec: 27.2242\n",
      "step 2354 | train loss: 3.71 | val loss: 3.57 | perplexity: 35.41 | lr: 1.02e-04 | norm: 0.3713 | dt: 14571.8386ms | tok/sec: 26.9847\n",
      "step 2355 | train loss: 3.87 | val loss: 3.57 | perplexity: 35.40 | lr: 1.02e-04 | norm: 0.3165 | dt: 14599.3958ms | tok/sec: 26.9337\n",
      "step 2356 | train loss: 3.96 | val loss: 3.57 | perplexity: 35.42 | lr: 1.02e-04 | norm: 0.3608 | dt: 14530.6709ms | tok/sec: 27.0611\n",
      "step 2357 | train loss: 3.56 | val loss: 3.57 | perplexity: 35.46 | lr: 1.01e-04 | norm: 0.2837 | dt: 14615.6173ms | tok/sec: 26.9038\n",
      "step 2358 | train loss: 3.32 | val loss: 3.57 | perplexity: 35.52 | lr: 1.01e-04 | norm: 0.2507 | dt: 14641.8393ms | tok/sec: 26.8556\n",
      "step 2359 | train loss: 3.36 | val loss: 3.57 | perplexity: 35.53 | lr: 1.01e-04 | norm: 0.2765 | dt: 14405.7069ms | tok/sec: 27.2958\n",
      "step 2360 | train loss: 3.83 | val loss: 3.57 | perplexity: 35.52 | lr: 1.01e-04 | norm: 0.3004 | dt: 14570.0057ms | tok/sec: 26.9880\n",
      "step 2361 | train loss: 3.71 | val loss: 3.57 | perplexity: 35.48 | lr: 1.01e-04 | norm: 0.3018 | dt: 14470.5529ms | tok/sec: 27.1735\n",
      "step 2362 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.43 | lr: 1.01e-04 | norm: 0.2943 | dt: 14498.4667ms | tok/sec: 27.1212\n",
      "step 2363 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.42 | lr: 1.01e-04 | norm: 0.2738 | dt: 14550.4642ms | tok/sec: 27.0243\n",
      "step 2364 | train loss: 3.60 | val loss: 3.57 | perplexity: 35.43 | lr: 1.01e-04 | norm: 0.3013 | dt: 14657.5422ms | tok/sec: 26.8269\n",
      "step 2365 | train loss: 3.37 | val loss: 3.57 | perplexity: 35.42 | lr: 1.01e-04 | norm: 0.3224 | dt: 14588.2709ms | tok/sec: 26.9543\n",
      "step 2366 | train loss: 3.47 | val loss: 3.57 | perplexity: 35.43 | lr: 1.01e-04 | norm: 0.3117 | dt: 14586.2255ms | tok/sec: 26.9580\n",
      "step 2367 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.46 | lr: 1.01e-04 | norm: 0.2994 | dt: 14706.4056ms | tok/sec: 26.7377\n",
      "step 2368 | train loss: 3.73 | val loss: 3.57 | perplexity: 35.48 | lr: 1.01e-04 | norm: 0.2441 | dt: 15099.2203ms | tok/sec: 26.0421\n",
      "step 2369 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.48 | lr: 1.01e-04 | norm: 0.2729 | dt: 14684.7169ms | tok/sec: 26.7772\n",
      "step 2370 | train loss: 3.61 | val loss: 3.57 | perplexity: 35.47 | lr: 1.01e-04 | norm: 0.2482 | dt: 15165.3602ms | tok/sec: 25.9286\n",
      "step 2371 | train loss: 3.70 | val loss: 3.57 | perplexity: 35.47 | lr: 1.01e-04 | norm: 0.4961 | dt: 15018.2950ms | tok/sec: 26.1825\n",
      "step 2372 | train loss: 3.59 | val loss: 3.57 | perplexity: 35.48 | lr: 1.01e-04 | norm: 0.3647 | dt: 15182.8964ms | tok/sec: 25.8986\n",
      "step 2373 | train loss: 3.70 | val loss: 3.57 | perplexity: 35.48 | lr: 1.01e-04 | norm: 0.3227 | dt: 15103.2288ms | tok/sec: 26.0352\n",
      "step 2374 | train loss: 3.82 | val loss: 3.57 | perplexity: 35.50 | lr: 1.01e-04 | norm: 0.2933 | dt: 15146.8463ms | tok/sec: 25.9603\n",
      "step 2375 | train loss: 3.90 | val loss: 3.57 | perplexity: 35.50 | lr: 1.00e-04 | norm: 0.2690 | dt: 15012.0404ms | tok/sec: 26.1934\n",
      "step 2376 | train loss: 3.65 | val loss: 3.57 | perplexity: 35.51 | lr: 1.00e-04 | norm: 0.3268 | dt: 14934.4985ms | tok/sec: 26.3294\n",
      "step 2377 | train loss: 3.68 | val loss: 3.57 | perplexity: 35.51 | lr: 1.00e-04 | norm: 0.3023 | dt: 15104.4252ms | tok/sec: 26.0332\n",
      "step 2378 | train loss: 3.50 | val loss: 3.57 | perplexity: 35.51 | lr: 1.00e-04 | norm: 0.3055 | dt: 14990.1912ms | tok/sec: 26.2316\n",
      "step 2379 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.50 | lr: 1.00e-04 | norm: 0.3315 | dt: 14933.3737ms | tok/sec: 26.3314\n",
      "step 2380 | train loss: 3.67 | val loss: 3.57 | perplexity: 35.48 | lr: 1.00e-04 | norm: 0.3142 | dt: 14926.3897ms | tok/sec: 26.3437\n",
      "step 2381 | train loss: 3.99 | val loss: 3.57 | perplexity: 35.48 | lr: 1.00e-04 | norm: 0.3581 | dt: 14461.2660ms | tok/sec: 27.1910\n",
      "step 2382 | train loss: 3.81 | val loss: 3.57 | perplexity: 35.49 | lr: 1.00e-04 | norm: 0.2990 | dt: 14417.8028ms | tok/sec: 27.2729\n",
      "step 2383 | train loss: 3.61 | val loss: 3.57 | perplexity: 35.49 | lr: 1.00e-04 | norm: 0.3222 | dt: 14515.8737ms | tok/sec: 27.0887\n",
      "step 2384 | train loss: 3.83 | val loss: 3.57 | perplexity: 35.47 | lr: 1.00e-04 | norm: 0.3256 | dt: 14350.4972ms | tok/sec: 27.4009\n",
      "step 2385 | train loss: 3.52 | val loss: 3.57 | perplexity: 35.44 | lr: 1.00e-04 | norm: 0.3431 | dt: 14442.7464ms | tok/sec: 27.2258\n",
      "step 2386 | train loss: 3.88 | val loss: 3.57 | perplexity: 35.41 | lr: 1.00e-04 | norm: 0.3126 | dt: 14809.6781ms | tok/sec: 26.5513\n",
      "step 2387 | train loss: 3.85 | val loss: 3.57 | perplexity: 35.38 | lr: 1.00e-04 | norm: 0.2728 | dt: 15171.8230ms | tok/sec: 25.9175\n",
      "step 2388 | train loss: 3.56 | val loss: 3.57 | perplexity: 35.36 | lr: 1.00e-04 | norm: 0.3002 | dt: 14909.8308ms | tok/sec: 26.3729\n",
      "step 2389 | train loss: 3.94 | val loss: 3.57 | perplexity: 35.36 | lr: 1.00e-04 | norm: 0.2940 | dt: 15118.7708ms | tok/sec: 26.0085\n",
      "step 2390 | train loss: 3.83 | val loss: 3.57 | perplexity: 35.39 | lr: 1.00e-04 | norm: 0.3010 | dt: 15212.7557ms | tok/sec: 25.8478\n",
      "step 2391 | train loss: 3.71 | val loss: 3.57 | perplexity: 35.42 | lr: 1.00e-04 | norm: 0.3252 | dt: 15144.6121ms | tok/sec: 25.9641\n",
      "step 2392 | train loss: 3.67 | val loss: 3.57 | perplexity: 35.44 | lr: 1.00e-04 | norm: 0.2571 | dt: 14891.7079ms | tok/sec: 26.4050\n",
      "step 2393 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.41 | lr: 1.00e-04 | norm: 0.2924 | dt: 15035.4908ms | tok/sec: 26.1525\n",
      "step 2394 | train loss: 3.63 | val loss: 3.57 | perplexity: 35.40 | lr: 1.00e-04 | norm: 0.2590 | dt: 14992.7604ms | tok/sec: 26.2271\n",
      "step 2395 | train loss: 3.65 | val loss: 3.57 | perplexity: 35.38 | lr: 1.00e-04 | norm: 0.3215 | dt: 15200.2547ms | tok/sec: 25.8690\n",
      "step 2396 | train loss: 3.78 | val loss: 3.57 | perplexity: 35.37 | lr: 1.00e-04 | norm: 0.3159 | dt: 14945.0550ms | tok/sec: 26.3108\n",
      "step 2397 | train loss: 4.05 | val loss: 3.57 | perplexity: 35.37 | lr: 1.00e-04 | norm: 0.4206 | dt: 15056.1495ms | tok/sec: 26.1166\n",
      "step 2398 | train loss: 3.57 | val loss: 3.57 | perplexity: 35.41 | lr: 1.00e-04 | norm: 0.2663 | dt: 15118.8023ms | tok/sec: 26.0084\n",
      "step 2399 | train loss: 3.87 | val loss: 3.57 | perplexity: 35.46 | lr: 1.00e-04 | norm: 0.3067 | dt: 14792.8705ms | tok/sec: 26.5815\n",
      "[Text Eval] samples=60 BLEU=0.00 ROUGE-L=0.0570 SELF-BLEU=6.60 REP=0.7257 D1=0.2038 D2=0.2465\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, token_encoder, EVAL_FREQ, grad_accum_steps, device,master_process, logpath)\n",
    "history,evaluation = trainer.train(MAX_STEPS, WARMUP_STEPS, MAX_LR, MIN_LR)\n",
    "dt = (time.time() - start_time) / (60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b4f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 10.8067hr\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training time: {dt:.4f}hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b89f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4965cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(evaluation, f, indent=4)\n",
    "\n",
    "with open('training_history.json','w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
