{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3058e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from src.model import GPT,Config\n",
    "from src.trainer import Trainer\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b993b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = './log'\n",
    "DATASET_PATH = './data/tinystories'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes, split='train'):\n",
    "        super().__init__()\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "        \n",
    "        # get the shard filenames\n",
    "        data_root = \"./data/tinystories\"\n",
    "        shard_filenames = os.listdir(data_root)\n",
    "        shard_filenames = sorted([filename for filename in shard_filenames if split in filename])\n",
    "        self.shard_filepaths = [os.path.join(data_root, filename) for filename in shard_filenames]\n",
    "        assert len(self.shard_filepaths) > 0, f'no shards found for split {split}'\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f'found {len(self.shard_filepaths)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def load_tokens(self, filepath):\n",
    "        tokens = torch.tensor(np.load(filepath).astype(np.int32), dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard 0\n",
    "        self.curr_shard = 0\n",
    "        self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "        self.curr_pos = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        batch = self.tokens[self.curr_pos : self.curr_pos + B*T + 1]\n",
    "        x_batch = batch[:-1].view(B, T)\n",
    "        y_batch = batch[1:].view(B, T)\n",
    "        self.curr_pos += B * T * self.num_processes\n",
    "        if self.curr_pos + (B * T + 1) > len(self.tokens):\n",
    "            self.curr_shard = (self.curr_shard + 1) % len(self.shard_filepaths)\n",
    "            self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "            self.curr_pos = self.B * self.T * self.process_rank\n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a831a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add76cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 4\n",
    "CTX_LENGTH = 2048\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 10\n",
    "EMBED_DIM = 768\n",
    "WEIGHT_DECAY =0.1\n",
    "MAX_LR = 1e-3\n",
    "MIN_LR = 1e-3*0.1\n",
    "EVAL_FREQ = 250\n",
    "MAX_STEPS = 1000\n",
    "WARMUP_STEPS = 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbf5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf24d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 shards for split train\n",
      "found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='train')\n",
    "val_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7461c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 111,086,592\n",
      "num decay parameter tensors: 42 with 110,985,216 parameters\n",
      "num nodecay parameter tensors: 82 with 101,376 parameters\n",
      "using fused AdamW optimizer: True\n"
     ]
    }
   ],
   "source": [
    "gpt_config = Config(vocab_size=50304,  # number of tokens: 50000 BPE merges + 256 bytes tokens + 1 <endoftext> token = 50257, \n",
    "                    # 50304 (nice number, lots of power of 2s) used instead of 50257 (bad, odd number)\n",
    "                           context_length=CTX_LENGTH, \n",
    "                           num_layers=NUM_LAYERS, \n",
    "                           num_heads=NUM_HEADS, \n",
    "                           embedding_dim=EMBED_DIM\n",
    "                           )\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of trainable parameters: {total_params:,}')\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "optimizer = model.configure_optimizer(weight_decay=WEIGHT_DECAY,lr=MAX_LR,device_type=device_type,master_process=master_process)\n",
    "token_encoder = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f51484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.9669\n",
      "step    0 | loss: 10.965103 | lr: 1.40e-06 | norm: 21.0113 | dt: 9939.6815ms | tok/sec: 26.3735\n",
      "step    1 | loss: 10.846848 | lr: 2.80e-06 | norm: 21.3282 | dt: 7599.0839ms | tok/sec: 34.4968\n",
      "step    2 | loss: 10.654166 | lr: 4.20e-06 | norm: 19.0643 | dt: 7562.1984ms | tok/sec: 34.6651\n",
      "step    3 | loss: 10.432758 | lr: 5.59e-06 | norm: 14.6875 | dt: 7711.6253ms | tok/sec: 33.9934\n",
      "step    4 | loss: 10.152704 | lr: 6.99e-06 | norm: 11.8371 | dt: 7781.1313ms | tok/sec: 33.6897\n",
      "step    5 | loss: 9.975197 | lr: 8.39e-06 | norm: 9.1772 | dt: 7794.3084ms | tok/sec: 33.6327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m start_time = time.time()\n\u001b[32m      2\u001b[39m trainer = Trainer(model, optimizer, train_loader, val_loader, token_encoder, EVAL_FREQ, grad_accum_steps, device,master_process, logpath)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIN_LR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m dt = (time.time() - start_time) / (\u001b[32m60\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mhr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backups\\Code\\Python\\Deep_learning\\Project\\src\\trainer.py:42\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, max_steps, warmup_steps, max_lr, min_lr)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mini_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.grad_accum_steps):\n\u001b[32m     41\u001b[39m     inp, target = \u001b[38;5;28mself\u001b[39m.train_loader.next_batch()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     inp, target = \u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,target.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[38;5;28mself\u001b[39m.device_type,dtype=torch.bfloat16):\n\u001b[32m     45\u001b[39m         logits,loss = \u001b[38;5;28mself\u001b[39m.model(inp,target)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, token_encoder, EVAL_FREQ, grad_accum_steps, device,master_process, logpath)\n",
    "trainer.train(MAX_STEPS, WARMUP_STEPS, MAX_LR, MIN_LR)\n",
    "dt = (time.time() - start_time) / (60*60)\n",
    "print(f\"Total training time: {dt:.4f}hr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
