{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3058e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from src.model import GPT,Config\n",
    "from src.trainer import Trainer\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b993b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = './log'\n",
    "DATASET_PATH = './data/gutenberg'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes, split='train'):\n",
    "        super().__init__()\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "        \n",
    "        # get the shard filenames\n",
    "        data_root = DATASET_PATH\n",
    "        shard_filenames = os.listdir(data_root)\n",
    "        shard_filenames = sorted([filename for filename in shard_filenames if split in filename])\n",
    "        self.shard_filepaths = [os.path.join(data_root, filename) for filename in shard_filenames]\n",
    "        assert len(self.shard_filepaths) > 0, f'no shards found for split {split}'\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f'found {len(self.shard_filepaths)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def load_tokens(self, filepath):\n",
    "        tokens = torch.tensor(np.load(filepath).astype(np.int32), dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard 0\n",
    "        self.curr_shard = 0\n",
    "        self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "        self.curr_pos = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        batch = self.tokens[self.curr_pos : self.curr_pos + B*T + 1]\n",
    "        x_batch = batch[:-1].view(B, T)\n",
    "        y_batch = batch[1:].view(B, T)\n",
    "        self.curr_pos += B * T * self.num_processes\n",
    "        if self.curr_pos + (B * T + 1) > len(self.tokens):\n",
    "            self.curr_shard = (self.curr_shard + 1) % len(self.shard_filepaths)\n",
    "            self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "            self.curr_pos = self.B * self.T * self.process_rank\n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a831a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add76cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 6\n",
    "CTX_LENGTH = 2048\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 10\n",
    "EMBED_DIM = 768\n",
    "WEIGHT_DECAY =0.1\n",
    "MAX_LR = 1e-3\n",
    "MIN_LR = 1e-4\n",
    "EVAL_FREQ = 1\n",
    "MAX_STEPS = 2400\n",
    "WARMUP_STEPS = 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbf5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf24d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='train')\n",
    "val_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7461c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 111,086,592\n",
      "num decay parameter tensors: 42 with 110,985,216 parameters\n",
      "num nodecay parameter tensors: 82 with 101,376 parameters\n",
      "using fused AdamW optimizer: True\n"
     ]
    }
   ],
   "source": [
    "gpt_config = Config(vocab_size=50304,  # number of tokens: 50000 BPE merges + 256 bytes tokens + 1 <endoftext> token = 50257, \n",
    "                    # 50304 (nice number, lots of power of 2s) used instead of 50257 (bad, odd number)\n",
    "                           context_length=CTX_LENGTH, \n",
    "                           num_layers=NUM_LAYERS, \n",
    "                           num_heads=NUM_HEADS, \n",
    "                           embedding_dim=EMBED_DIM\n",
    "                           )\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of trainable parameters: {total_params:,}')\n",
    "model.to(device)\n",
    "optimizer = model.configure_optimizer(weight_decay=WEIGHT_DECAY,lr=MAX_LR,device_type=device_type,master_process=master_process)\n",
    "token_encoder = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f51484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | train loss: 11.01 | val loss: 10.91 | perplexity: 54936.00 | lr: 1.40e-06 | norm: 15.3370 | dt: 12569.0479ms | tok/sec: 31.2845\n",
      "step    1 | train loss: 10.92 | val loss: 10.77 | perplexity: 47357.07 | lr: 2.80e-06 | norm: 16.1844 | dt: 22720.5837ms | tok/sec: 17.3066\n",
      "step    2 | train loss: 10.78 | val loss: 10.57 | perplexity: 39088.95 | lr: 4.20e-06 | norm: 14.8037 | dt: 20535.1980ms | tok/sec: 19.1484\n",
      "step    3 | train loss: 10.57 | val loss: 10.38 | perplexity: 32116.49 | lr: 5.59e-06 | norm: 13.0473 | dt: 20557.3471ms | tok/sec: 19.1278\n",
      "step    4 | train loss: 10.40 | val loss: 10.20 | perplexity: 26985.90 | lr: 6.99e-06 | norm: 10.3302 | dt: 20668.8342ms | tok/sec: 19.0246\n",
      "step    5 | train loss: 10.22 | val loss: 10.06 | perplexity: 23293.58 | lr: 8.39e-06 | norm: 8.6743 | dt: 20480.0291ms | tok/sec: 19.2000\n",
      "step    6 | train loss: 10.03 | val loss: 9.92 | perplexity: 20386.35 | lr: 9.79e-06 | norm: 7.5107 | dt: 20522.9800ms | tok/sec: 19.1598\n",
      "step    7 | train loss: 9.92 | val loss: 9.80 | perplexity: 18066.35 | lr: 1.12e-05 | norm: 6.6492 | dt: 20693.4512ms | tok/sec: 19.0020\n",
      "step    8 | train loss: 9.76 | val loss: 9.69 | perplexity: 16236.16 | lr: 1.26e-05 | norm: 5.6863 | dt: 20393.6055ms | tok/sec: 19.2813\n",
      "step    9 | train loss: 9.61 | val loss: 9.60 | perplexity: 14809.35 | lr: 1.40e-05 | norm: 4.8430 | dt: 20355.5212ms | tok/sec: 19.3174\n",
      "step   10 | train loss: 9.84 | val loss: 9.53 | perplexity: 13833.06 | lr: 1.54e-05 | norm: 5.3306 | dt: 19878.3915ms | tok/sec: 19.7811\n",
      "step   11 | train loss: 9.59 | val loss: 9.48 | perplexity: 13114.08 | lr: 1.68e-05 | norm: 4.0949 | dt: 20240.1278ms | tok/sec: 19.4275\n",
      "step   12 | train loss: 9.40 | val loss: 9.43 | perplexity: 12414.03 | lr: 1.82e-05 | norm: 4.0796 | dt: 19933.3298ms | tok/sec: 19.7266\n",
      "step   13 | train loss: 9.36 | val loss: 9.37 | perplexity: 11744.51 | lr: 1.96e-05 | norm: 3.5270 | dt: 19820.9002ms | tok/sec: 19.8385\n",
      "step   14 | train loss: 9.28 | val loss: 9.32 | perplexity: 11116.94 | lr: 2.10e-05 | norm: 3.6609 | dt: 19804.3745ms | tok/sec: 19.8550\n",
      "step   15 | train loss: 9.35 | val loss: 9.26 | perplexity: 10534.69 | lr: 2.24e-05 | norm: 3.5176 | dt: 19821.9104ms | tok/sec: 19.8374\n",
      "step   16 | train loss: 9.27 | val loss: 9.22 | perplexity: 10080.81 | lr: 2.38e-05 | norm: 3.3868 | dt: 19783.7851ms | tok/sec: 19.8757\n",
      "step   17 | train loss: 9.34 | val loss: 9.17 | perplexity: 9613.39 | lr: 2.52e-05 | norm: 3.5796 | dt: 19697.4888ms | tok/sec: 19.9627\n",
      "step   18 | train loss: 9.13 | val loss: 9.11 | perplexity: 9031.38 | lr: 2.66e-05 | norm: 3.7804 | dt: 19010.3376ms | tok/sec: 20.6843\n",
      "step   19 | train loss: 9.17 | val loss: 9.05 | perplexity: 8478.06 | lr: 2.80e-05 | norm: 3.4413 | dt: 19015.9345ms | tok/sec: 20.6782\n",
      "step   20 | train loss: 8.96 | val loss: 8.97 | perplexity: 7902.33 | lr: 2.94e-05 | norm: 3.6410 | dt: 19039.0778ms | tok/sec: 20.6531\n",
      "step   21 | train loss: 9.12 | val loss: 8.91 | perplexity: 7429.90 | lr: 3.08e-05 | norm: 3.2072 | dt: 18872.2665ms | tok/sec: 20.8357\n",
      "step   22 | train loss: 9.09 | val loss: 8.84 | perplexity: 6906.99 | lr: 3.22e-05 | norm: 4.0029 | dt: 18907.6531ms | tok/sec: 20.7967\n",
      "step   23 | train loss: 9.02 | val loss: 8.79 | perplexity: 6575.32 | lr: 3.36e-05 | norm: 3.2930 | dt: 18906.8654ms | tok/sec: 20.7975\n",
      "step   24 | train loss: 8.88 | val loss: 8.74 | perplexity: 6249.35 | lr: 3.50e-05 | norm: 2.9992 | dt: 18902.2238ms | tok/sec: 20.8026\n",
      "step   25 | train loss: 8.87 | val loss: 8.68 | perplexity: 5894.41 | lr: 3.64e-05 | norm: 2.8136 | dt: 18682.1110ms | tok/sec: 21.0477\n",
      "step   26 | train loss: 8.79 | val loss: 8.63 | perplexity: 5575.17 | lr: 3.78e-05 | norm: 2.5132 | dt: 18659.0626ms | tok/sec: 21.0737\n",
      "step   27 | train loss: 8.90 | val loss: 8.57 | perplexity: 5286.98 | lr: 3.92e-05 | norm: 2.6107 | dt: 18606.6661ms | tok/sec: 21.1331\n",
      "step   28 | train loss: 8.86 | val loss: 8.53 | perplexity: 5045.19 | lr: 4.06e-05 | norm: 2.7595 | dt: 18628.5629ms | tok/sec: 21.1082\n",
      "step   29 | train loss: 8.88 | val loss: 8.49 | perplexity: 4866.04 | lr: 4.20e-05 | norm: 2.7772 | dt: 18729.2540ms | tok/sec: 20.9947\n",
      "step   30 | train loss: 8.60 | val loss: 8.45 | perplexity: 4664.13 | lr: 4.34e-05 | norm: 2.6426 | dt: 18627.1284ms | tok/sec: 21.1099\n",
      "step   31 | train loss: 8.56 | val loss: 8.40 | perplexity: 4437.88 | lr: 4.48e-05 | norm: 2.3744 | dt: 18681.2730ms | tok/sec: 21.0487\n",
      "step   32 | train loss: 8.36 | val loss: 8.34 | perplexity: 4172.56 | lr: 4.62e-05 | norm: 2.6656 | dt: 18655.8032ms | tok/sec: 21.0774\n",
      "step   33 | train loss: 8.28 | val loss: 8.28 | perplexity: 3934.50 | lr: 4.76e-05 | norm: 2.3546 | dt: 18719.5151ms | tok/sec: 21.0057\n",
      "step   34 | train loss: 8.40 | val loss: 8.22 | perplexity: 3718.81 | lr: 4.90e-05 | norm: 2.2641 | dt: 18667.6531ms | tok/sec: 21.0640\n",
      "step   35 | train loss: 8.41 | val loss: 8.17 | perplexity: 3517.19 | lr: 5.03e-05 | norm: 2.7604 | dt: 18660.2042ms | tok/sec: 21.0724\n",
      "step   36 | train loss: 8.09 | val loss: 8.11 | perplexity: 3333.83 | lr: 5.17e-05 | norm: 2.3509 | dt: 18724.6535ms | tok/sec: 20.9999\n",
      "step   37 | train loss: 8.10 | val loss: 8.06 | perplexity: 3161.90 | lr: 5.31e-05 | norm: 2.2688 | dt: 18574.0330ms | tok/sec: 21.1702\n",
      "step   38 | train loss: 8.03 | val loss: 8.02 | perplexity: 3052.48 | lr: 5.45e-05 | norm: 3.3823 | dt: 18692.3370ms | tok/sec: 21.0362\n",
      "step   39 | train loss: 8.04 | val loss: 7.98 | perplexity: 2918.79 | lr: 5.59e-05 | norm: 2.3432 | dt: 18715.6134ms | tok/sec: 21.0101\n",
      "step   40 | train loss: 7.87 | val loss: 7.93 | perplexity: 2780.18 | lr: 5.73e-05 | norm: 2.0840 | dt: 18788.8713ms | tok/sec: 20.9281\n",
      "step   41 | train loss: 8.06 | val loss: 7.87 | perplexity: 2623.35 | lr: 5.87e-05 | norm: 2.2205 | dt: 18666.6105ms | tok/sec: 21.0652\n",
      "step   42 | train loss: 8.30 | val loss: 7.83 | perplexity: 2518.48 | lr: 6.01e-05 | norm: 2.0731 | dt: 18667.9730ms | tok/sec: 21.0637\n",
      "step   43 | train loss: 8.14 | val loss: 7.78 | perplexity: 2386.45 | lr: 6.15e-05 | norm: 2.2984 | dt: 18640.9264ms | tok/sec: 21.0942\n",
      "step   44 | train loss: 8.18 | val loss: 7.74 | perplexity: 2295.32 | lr: 6.29e-05 | norm: 2.1104 | dt: 18635.9379ms | tok/sec: 21.0999\n",
      "step   45 | train loss: 7.72 | val loss: 7.68 | perplexity: 2159.95 | lr: 6.43e-05 | norm: 2.7209 | dt: 18729.2573ms | tok/sec: 20.9947\n",
      "step   46 | train loss: 7.75 | val loss: 7.62 | perplexity: 2030.85 | lr: 6.57e-05 | norm: 2.1109 | dt: 18611.9072ms | tok/sec: 21.1271\n",
      "step   47 | train loss: 7.51 | val loss: 7.56 | perplexity: 1920.62 | lr: 6.71e-05 | norm: 2.5621 | dt: 18759.7923ms | tok/sec: 20.9606\n",
      "step   48 | train loss: 7.44 | val loss: 7.50 | perplexity: 1808.22 | lr: 6.85e-05 | norm: 1.9284 | dt: 18727.7691ms | tok/sec: 20.9964\n",
      "step   49 | train loss: 7.46 | val loss: 7.44 | perplexity: 1696.90 | lr: 6.99e-05 | norm: 1.7845 | dt: 18539.4232ms | tok/sec: 21.2097\n",
      "step   50 | train loss: 7.92 | val loss: 7.40 | perplexity: 1628.38 | lr: 7.13e-05 | norm: 2.2522 | dt: 18749.4118ms | tok/sec: 20.9722\n",
      "step   51 | train loss: 7.55 | val loss: 7.34 | perplexity: 1538.78 | lr: 7.27e-05 | norm: 2.0686 | dt: 18635.4287ms | tok/sec: 21.1005\n",
      "step   52 | train loss: 7.30 | val loss: 7.28 | perplexity: 1449.21 | lr: 7.41e-05 | norm: 2.1719 | dt: 18867.6665ms | tok/sec: 20.8407\n",
      "step   53 | train loss: 7.11 | val loss: 7.23 | perplexity: 1374.48 | lr: 7.55e-05 | norm: 2.2845 | dt: 18751.8835ms | tok/sec: 20.9694\n",
      "step   54 | train loss: 7.29 | val loss: 7.17 | perplexity: 1303.45 | lr: 7.69e-05 | norm: 1.7080 | dt: 18670.7833ms | tok/sec: 21.0605\n",
      "step   55 | train loss: 7.46 | val loss: 7.11 | perplexity: 1228.74 | lr: 7.83e-05 | norm: 1.6934 | dt: 18593.4551ms | tok/sec: 21.1481\n",
      "step   56 | train loss: 7.14 | val loss: 7.07 | perplexity: 1175.03 | lr: 7.97e-05 | norm: 1.5134 | dt: 18641.2747ms | tok/sec: 21.0938\n",
      "step   57 | train loss: 7.42 | val loss: 7.03 | perplexity: 1130.62 | lr: 8.11e-05 | norm: 2.0294 | dt: 19032.0036ms | tok/sec: 20.6608\n",
      "step   58 | train loss: 7.65 | val loss: 7.03 | perplexity: 1125.44 | lr: 8.25e-05 | norm: 2.1157 | dt: 18698.6175ms | tok/sec: 21.0291\n",
      "step   59 | train loss: 7.27 | val loss: 6.97 | perplexity: 1061.82 | lr: 8.39e-05 | norm: 2.5220 | dt: 18812.3229ms | tok/sec: 20.9020\n",
      "step   60 | train loss: 7.28 | val loss: 6.91 | perplexity: 997.34 | lr: 8.53e-05 | norm: 2.0218 | dt: 18701.0200ms | tok/sec: 21.0264\n",
      "step   61 | train loss: 7.22 | val loss: 6.86 | perplexity: 957.70 | lr: 8.67e-05 | norm: 2.1777 | dt: 18593.6818ms | tok/sec: 21.1478\n",
      "step   62 | train loss: 6.84 | val loss: 6.80 | perplexity: 901.71 | lr: 8.81e-05 | norm: 1.9691 | dt: 18618.4015ms | tok/sec: 21.1198\n",
      "step   63 | train loss: 6.80 | val loss: 6.78 | perplexity: 875.71 | lr: 8.95e-05 | norm: 1.6179 | dt: 18510.4616ms | tok/sec: 21.2429\n",
      "step   64 | train loss: 6.83 | val loss: 6.75 | perplexity: 855.31 | lr: 9.09e-05 | norm: 1.8450 | dt: 18731.2403ms | tok/sec: 20.9925\n",
      "step   65 | train loss: 6.78 | val loss: 6.71 | perplexity: 817.13 | lr: 9.23e-05 | norm: 1.5932 | dt: 18702.6956ms | tok/sec: 21.0246\n",
      "step   66 | train loss: 6.90 | val loss: 6.67 | perplexity: 785.20 | lr: 9.37e-05 | norm: 1.2570 | dt: 18628.5412ms | tok/sec: 21.1083\n",
      "step   67 | train loss: 6.63 | val loss: 6.62 | perplexity: 746.46 | lr: 9.51e-05 | norm: 1.8602 | dt: 18663.0015ms | tok/sec: 21.0693\n",
      "step   68 | train loss: 6.71 | val loss: 6.57 | perplexity: 710.44 | lr: 9.65e-05 | norm: 1.3488 | dt: 18777.9741ms | tok/sec: 20.9403\n",
      "step   69 | train loss: 6.84 | val loss: 6.53 | perplexity: 682.78 | lr: 9.79e-05 | norm: 1.1193 | dt: 18630.6343ms | tok/sec: 21.1059\n",
      "step   70 | train loss: 6.49 | val loss: 6.49 | perplexity: 658.32 | lr: 9.93e-05 | norm: 1.2921 | dt: 18728.4184ms | tok/sec: 20.9957\n",
      "step   71 | train loss: 6.57 | val loss: 6.45 | perplexity: 633.16 | lr: 1.01e-04 | norm: 1.3055 | dt: 18592.8311ms | tok/sec: 21.1488\n",
      "step   72 | train loss: 6.70 | val loss: 6.43 | perplexity: 617.91 | lr: 1.02e-04 | norm: 0.9851 | dt: 18738.3339ms | tok/sec: 20.9846\n",
      "step   73 | train loss: 7.16 | val loss: 6.39 | perplexity: 595.92 | lr: 1.03e-04 | norm: 1.7163 | dt: 18628.0186ms | tok/sec: 21.1088\n",
      "step   74 | train loss: 6.38 | val loss: 6.37 | perplexity: 583.09 | lr: 1.05e-04 | norm: 1.2437 | dt: 18622.6957ms | tok/sec: 21.1149\n",
      "step   75 | train loss: 6.57 | val loss: 6.34 | perplexity: 566.63 | lr: 1.06e-04 | norm: 1.8138 | dt: 18622.2134ms | tok/sec: 21.1154\n",
      "step   76 | train loss: 6.70 | val loss: 6.32 | perplexity: 553.09 | lr: 1.08e-04 | norm: 1.4779 | dt: 18704.9932ms | tok/sec: 21.0220\n",
      "step   77 | train loss: 6.22 | val loss: 6.28 | perplexity: 535.46 | lr: 1.09e-04 | norm: 1.7311 | dt: 18602.1266ms | tok/sec: 21.1382\n",
      "step   78 | train loss: 6.37 | val loss: 6.26 | perplexity: 521.34 | lr: 1.10e-04 | norm: 1.2192 | dt: 18778.9378ms | tok/sec: 20.9392\n",
      "step   79 | train loss: 6.47 | val loss: 6.24 | perplexity: 511.02 | lr: 1.12e-04 | norm: 1.4787 | dt: 18726.9182ms | tok/sec: 20.9974\n",
      "step   80 | train loss: 6.34 | val loss: 6.22 | perplexity: 502.50 | lr: 1.13e-04 | norm: 1.3455 | dt: 18621.1991ms | tok/sec: 21.1166\n",
      "step   81 | train loss: 6.17 | val loss: 6.20 | perplexity: 490.39 | lr: 1.15e-04 | norm: 1.0740 | dt: 19000.5884ms | tok/sec: 20.6949\n",
      "step   82 | train loss: 6.36 | val loss: 6.18 | perplexity: 483.61 | lr: 1.16e-04 | norm: 1.1292 | dt: 18739.2752ms | tok/sec: 20.9835\n",
      "step   83 | train loss: 6.28 | val loss: 6.16 | perplexity: 472.44 | lr: 1.17e-04 | norm: 1.1426 | dt: 18805.6724ms | tok/sec: 20.9094\n",
      "step   84 | train loss: 6.30 | val loss: 6.14 | perplexity: 462.40 | lr: 1.19e-04 | norm: 0.8917 | dt: 18716.5749ms | tok/sec: 21.0090\n",
      "step   85 | train loss: 5.79 | val loss: 6.12 | perplexity: 456.77 | lr: 1.20e-04 | norm: 1.4793 | dt: 18655.9451ms | tok/sec: 21.0772\n",
      "step   86 | train loss: 6.51 | val loss: 6.11 | perplexity: 449.50 | lr: 1.22e-04 | norm: 1.1209 | dt: 19019.2423ms | tok/sec: 20.6746\n",
      "step   87 | train loss: 5.96 | val loss: 6.10 | perplexity: 444.09 | lr: 1.23e-04 | norm: 0.8694 | dt: 18742.1269ms | tok/sec: 20.9803\n",
      "step   88 | train loss: 5.87 | val loss: 6.09 | perplexity: 441.96 | lr: 1.24e-04 | norm: 1.2630 | dt: 18646.9285ms | tok/sec: 21.0874\n",
      "step   89 | train loss: 5.97 | val loss: 6.07 | perplexity: 432.56 | lr: 1.26e-04 | norm: 1.4192 | dt: 18670.5811ms | tok/sec: 21.0607\n",
      "step   90 | train loss: 6.14 | val loss: 6.05 | perplexity: 424.46 | lr: 1.27e-04 | norm: 1.0042 | dt: 18767.5061ms | tok/sec: 20.9520\n",
      "step   91 | train loss: 6.48 | val loss: 6.04 | perplexity: 421.66 | lr: 1.29e-04 | norm: 1.0621 | dt: 18751.2202ms | tok/sec: 20.9702\n",
      "step   92 | train loss: 6.57 | val loss: 6.03 | perplexity: 417.49 | lr: 1.30e-04 | norm: 0.8462 | dt: 18678.9582ms | tok/sec: 21.0513\n",
      "step   93 | train loss: 6.13 | val loss: 6.01 | perplexity: 406.45 | lr: 1.31e-04 | norm: 1.0459 | dt: 18847.3489ms | tok/sec: 20.8632\n",
      "step   94 | train loss: 6.17 | val loss: 6.00 | perplexity: 404.86 | lr: 1.33e-04 | norm: 0.7921 | dt: 18727.7184ms | tok/sec: 20.9965\n",
      "step   95 | train loss: 5.86 | val loss: 5.99 | perplexity: 398.89 | lr: 1.34e-04 | norm: 1.1139 | dt: 18725.7943ms | tok/sec: 20.9986\n",
      "step   96 | train loss: 6.37 | val loss: 5.98 | perplexity: 395.66 | lr: 1.36e-04 | norm: 1.0741 | dt: 18739.1293ms | tok/sec: 20.9837\n",
      "step   97 | train loss: 5.98 | val loss: 5.99 | perplexity: 399.71 | lr: 1.37e-04 | norm: 1.7037 | dt: 18649.6065ms | tok/sec: 21.0844\n",
      "step   98 | train loss: 5.96 | val loss: 5.99 | perplexity: 398.69 | lr: 1.38e-04 | norm: 1.2248 | dt: 18779.0215ms | tok/sec: 20.9391\n",
      "step   99 | train loss: 5.93 | val loss: 5.97 | perplexity: 390.90 | lr: 1.40e-04 | norm: 1.4934 | dt: 18808.8436ms | tok/sec: 20.9059\n",
      "step  100 | train loss: 6.20 | val loss: 5.94 | perplexity: 379.67 | lr: 1.41e-04 | norm: 1.0685 | dt: 18773.8481ms | tok/sec: 20.9449\n",
      "step  101 | train loss: 6.19 | val loss: 5.92 | perplexity: 372.33 | lr: 1.43e-04 | norm: 1.2299 | dt: 18642.3280ms | tok/sec: 21.0926\n",
      "step  102 | train loss: 6.13 | val loss: 5.91 | perplexity: 367.25 | lr: 1.44e-04 | norm: 0.8104 | dt: 18750.0906ms | tok/sec: 20.9714\n",
      "step  103 | train loss: 5.90 | val loss: 5.90 | perplexity: 363.92 | lr: 1.45e-04 | norm: 1.1472 | dt: 18662.7119ms | tok/sec: 21.0696\n",
      "step  104 | train loss: 6.03 | val loss: 5.88 | perplexity: 357.21 | lr: 1.47e-04 | norm: 1.2474 | dt: 18701.2162ms | tok/sec: 21.0262\n",
      "step  105 | train loss: 5.86 | val loss: 5.87 | perplexity: 354.36 | lr: 1.48e-04 | norm: 0.7703 | dt: 18666.7480ms | tok/sec: 21.0651\n",
      "step  106 | train loss: 6.00 | val loss: 5.86 | perplexity: 349.28 | lr: 1.50e-04 | norm: 1.2239 | dt: 18659.4484ms | tok/sec: 21.0733\n",
      "step  107 | train loss: 5.90 | val loss: 5.84 | perplexity: 342.68 | lr: 1.51e-04 | norm: 1.0036 | dt: 18613.5592ms | tok/sec: 21.1252\n",
      "step  108 | train loss: 6.33 | val loss: 5.86 | perplexity: 350.66 | lr: 1.52e-04 | norm: 1.3194 | dt: 18655.4077ms | tok/sec: 21.0779\n",
      "step  109 | train loss: 6.16 | val loss: 5.83 | perplexity: 340.68 | lr: 1.54e-04 | norm: 2.6030 | dt: 18783.1066ms | tok/sec: 20.9346\n",
      "step  110 | train loss: 5.78 | val loss: 5.82 | perplexity: 336.20 | lr: 1.55e-04 | norm: 0.8929 | dt: 18823.8006ms | tok/sec: 20.8893\n",
      "step  111 | train loss: 5.74 | val loss: 5.80 | perplexity: 331.70 | lr: 1.57e-04 | norm: 1.2614 | dt: 18724.6563ms | tok/sec: 20.9999\n",
      "step  112 | train loss: 5.81 | val loss: 5.79 | perplexity: 327.65 | lr: 1.58e-04 | norm: 0.9497 | dt: 18671.3326ms | tok/sec: 21.0599\n",
      "step  113 | train loss: 6.07 | val loss: 5.77 | perplexity: 321.29 | lr: 1.59e-04 | norm: 1.0313 | dt: 18720.2380ms | tok/sec: 21.0049\n",
      "step  114 | train loss: 5.83 | val loss: 5.76 | perplexity: 318.27 | lr: 1.61e-04 | norm: 0.9247 | dt: 18743.7253ms | tok/sec: 20.9785\n",
      "step  115 | train loss: 5.58 | val loss: 5.76 | perplexity: 317.43 | lr: 1.62e-04 | norm: 1.1714 | dt: 18702.1451ms | tok/sec: 21.0252\n",
      "step  116 | train loss: 5.90 | val loss: 5.75 | perplexity: 315.28 | lr: 1.64e-04 | norm: 1.0761 | dt: 18748.7667ms | tok/sec: 20.9729\n",
      "step  117 | train loss: 5.77 | val loss: 5.74 | perplexity: 311.12 | lr: 1.65e-04 | norm: 1.2425 | dt: 18814.5742ms | tok/sec: 20.8995\n",
      "step  118 | train loss: 5.68 | val loss: 5.73 | perplexity: 307.49 | lr: 1.66e-04 | norm: 0.8332 | dt: 18622.3884ms | tok/sec: 21.1152\n",
      "step  119 | train loss: 5.57 | val loss: 5.73 | perplexity: 306.95 | lr: 1.68e-04 | norm: 0.9405 | dt: 18798.5060ms | tok/sec: 20.9174\n",
      "step  120 | train loss: 5.94 | val loss: 5.72 | perplexity: 305.05 | lr: 1.69e-04 | norm: 1.0280 | dt: 18676.1239ms | tok/sec: 21.0545\n",
      "step  121 | train loss: 5.75 | val loss: 5.71 | perplexity: 301.59 | lr: 1.71e-04 | norm: 1.4048 | dt: 18758.8949ms | tok/sec: 20.9616\n",
      "step  122 | train loss: 5.79 | val loss: 5.71 | perplexity: 301.74 | lr: 1.72e-04 | norm: 0.7969 | dt: 18693.4719ms | tok/sec: 21.0349\n",
      "step  123 | train loss: 5.61 | val loss: 5.68 | perplexity: 292.97 | lr: 1.73e-04 | norm: 1.0040 | dt: 18692.1923ms | tok/sec: 21.0364\n",
      "step  124 | train loss: 5.78 | val loss: 5.68 | perplexity: 291.64 | lr: 1.75e-04 | norm: 0.9952 | dt: 18676.4278ms | tok/sec: 21.0541\n",
      "step  125 | train loss: 5.89 | val loss: 5.67 | perplexity: 291.42 | lr: 1.76e-04 | norm: 1.2363 | dt: 18712.6880ms | tok/sec: 21.0133\n",
      "step  126 | train loss: 5.72 | val loss: 5.66 | perplexity: 285.88 | lr: 1.78e-04 | norm: 1.0667 | dt: 18704.7968ms | tok/sec: 21.0222\n",
      "step  127 | train loss: 5.48 | val loss: 5.65 | perplexity: 283.75 | lr: 1.79e-04 | norm: 0.8093 | dt: 18711.0531ms | tok/sec: 21.0152\n",
      "step  128 | train loss: 5.75 | val loss: 5.64 | perplexity: 280.68 | lr: 1.80e-04 | norm: 1.1663 | dt: 18747.2198ms | tok/sec: 20.9746\n",
      "step  129 | train loss: 5.72 | val loss: 5.63 | perplexity: 277.88 | lr: 1.82e-04 | norm: 1.0277 | dt: 18595.0210ms | tok/sec: 21.1463\n",
      "step  130 | train loss: 5.72 | val loss: 5.64 | perplexity: 280.32 | lr: 1.83e-04 | norm: 0.9072 | dt: 18684.9396ms | tok/sec: 21.0445\n",
      "step  131 | train loss: 5.57 | val loss: 5.62 | perplexity: 277.03 | lr: 1.85e-04 | norm: 1.1825 | dt: 18717.2897ms | tok/sec: 21.0082\n",
      "step  132 | train loss: 5.76 | val loss: 5.62 | perplexity: 274.62 | lr: 1.86e-04 | norm: 0.7570 | dt: 18600.9209ms | tok/sec: 21.1396\n",
      "step  133 | train loss: 5.55 | val loss: 5.61 | perplexity: 272.48 | lr: 1.87e-04 | norm: 1.0311 | dt: 18634.0921ms | tok/sec: 21.1020\n",
      "step  134 | train loss: 5.93 | val loss: 5.60 | perplexity: 271.60 | lr: 1.89e-04 | norm: 0.9842 | dt: 18672.1940ms | tok/sec: 21.0589\n",
      "step  135 | train loss: 5.71 | val loss: 5.59 | perplexity: 268.57 | lr: 1.90e-04 | norm: 1.1583 | dt: 18822.6001ms | tok/sec: 20.8906\n",
      "step  136 | train loss: 5.65 | val loss: 5.58 | perplexity: 265.77 | lr: 1.92e-04 | norm: 0.8389 | dt: 18724.4182ms | tok/sec: 21.0002\n",
      "step  137 | train loss: 5.43 | val loss: 5.57 | perplexity: 263.41 | lr: 1.93e-04 | norm: 1.0504 | dt: 18738.9677ms | tok/sec: 20.9839\n",
      "step  138 | train loss: 5.56 | val loss: 5.58 | perplexity: 266.18 | lr: 1.94e-04 | norm: 1.0782 | dt: 18706.5153ms | tok/sec: 21.0203\n",
      "step  139 | train loss: 5.57 | val loss: 5.61 | perplexity: 272.31 | lr: 1.96e-04 | norm: 1.0251 | dt: 18623.1484ms | tok/sec: 21.1144\n",
      "step  140 | train loss: 5.89 | val loss: 5.60 | perplexity: 271.69 | lr: 1.97e-04 | norm: 1.2310 | dt: 18666.3070ms | tok/sec: 21.0655\n",
      "step  141 | train loss: 5.71 | val loss: 5.59 | perplexity: 267.95 | lr: 1.99e-04 | norm: 0.9688 | dt: 18640.5463ms | tok/sec: 21.0947\n",
      "step  142 | train loss: 5.74 | val loss: 5.56 | perplexity: 258.91 | lr: 2.00e-04 | norm: 1.2348 | dt: 18689.3561ms | tok/sec: 21.0396\n",
      "step  143 | train loss: 5.78 | val loss: 5.53 | perplexity: 253.03 | lr: 2.01e-04 | norm: 0.9195 | dt: 18671.9799ms | tok/sec: 21.0591\n",
      "step  144 | train loss: 5.81 | val loss: 5.54 | perplexity: 253.73 | lr: 2.03e-04 | norm: 0.8017 | dt: 18625.4926ms | tok/sec: 21.1117\n",
      "step  145 | train loss: 5.95 | val loss: 5.53 | perplexity: 251.87 | lr: 2.04e-04 | norm: 0.9226 | dt: 18682.0092ms | tok/sec: 21.0478\n",
      "step  146 | train loss: 5.85 | val loss: 5.54 | perplexity: 253.61 | lr: 2.06e-04 | norm: 0.8421 | dt: 18801.2514ms | tok/sec: 20.9144\n",
      "step  147 | train loss: 5.35 | val loss: 5.52 | perplexity: 250.58 | lr: 2.07e-04 | norm: 1.3313 | dt: 18699.4371ms | tok/sec: 21.0282\n",
      "step  148 | train loss: 5.77 | val loss: 5.52 | perplexity: 249.49 | lr: 2.08e-04 | norm: 1.1553 | dt: 18718.0455ms | tok/sec: 21.0073\n",
      "step  149 | train loss: 5.77 | val loss: 5.52 | perplexity: 248.63 | lr: 2.10e-04 | norm: 0.8701 | dt: 18598.4554ms | tok/sec: 21.1424\n",
      "step  150 | train loss: 5.55 | val loss: 5.51 | perplexity: 247.86 | lr: 2.11e-04 | norm: 0.9622 | dt: 18683.5279ms | tok/sec: 21.0461\n",
      "step  151 | train loss: 5.78 | val loss: 5.51 | perplexity: 246.69 | lr: 2.13e-04 | norm: 0.8906 | dt: 18675.6251ms | tok/sec: 21.0550\n",
      "step  152 | train loss: 6.03 | val loss: 5.50 | perplexity: 244.24 | lr: 2.14e-04 | norm: 0.7913 | dt: 18631.3348ms | tok/sec: 21.1051\n",
      "step  153 | train loss: 5.60 | val loss: 5.49 | perplexity: 241.49 | lr: 2.15e-04 | norm: 0.7681 | dt: 18777.0727ms | tok/sec: 20.9413\n",
      "step  154 | train loss: 5.67 | val loss: 5.48 | perplexity: 240.99 | lr: 2.17e-04 | norm: 0.8044 | dt: 18696.6181ms | tok/sec: 21.0314\n",
      "step  155 | train loss: 5.71 | val loss: 5.49 | perplexity: 243.16 | lr: 2.18e-04 | norm: 0.7056 | dt: 18786.6666ms | tok/sec: 20.9306\n",
      "step  156 | train loss: 5.64 | val loss: 5.49 | perplexity: 243.35 | lr: 2.20e-04 | norm: 1.0258 | dt: 18702.6358ms | tok/sec: 21.0246\n",
      "step  157 | train loss: 5.74 | val loss: 5.48 | perplexity: 240.55 | lr: 2.21e-04 | norm: 1.0153 | dt: 18655.6194ms | tok/sec: 21.0776\n",
      "step  158 | train loss: 5.67 | val loss: 5.48 | perplexity: 239.73 | lr: 2.22e-04 | norm: 1.6296 | dt: 18748.8399ms | tok/sec: 20.9728\n",
      "step  159 | train loss: 5.61 | val loss: 5.47 | perplexity: 237.55 | lr: 2.24e-04 | norm: 1.1407 | dt: 18689.2657ms | tok/sec: 21.0397\n",
      "step  160 | train loss: 5.63 | val loss: 5.47 | perplexity: 236.55 | lr: 2.25e-04 | norm: 0.6887 | dt: 18744.8015ms | tok/sec: 20.9773\n",
      "step  161 | train loss: 5.62 | val loss: 5.45 | perplexity: 233.65 | lr: 2.27e-04 | norm: 0.8944 | dt: 18725.9851ms | tok/sec: 20.9984\n",
      "step  162 | train loss: 5.53 | val loss: 5.45 | perplexity: 232.68 | lr: 2.28e-04 | norm: 0.8314 | dt: 18590.5371ms | tok/sec: 21.1514\n",
      "step  163 | train loss: 5.44 | val loss: 5.44 | perplexity: 230.36 | lr: 2.29e-04 | norm: 0.7509 | dt: 18728.4629ms | tok/sec: 20.9956\n",
      "step  164 | train loss: 5.83 | val loss: 5.44 | perplexity: 229.84 | lr: 2.31e-04 | norm: 0.8543 | dt: 18710.9423ms | tok/sec: 21.0153\n",
      "step  165 | train loss: 5.67 | val loss: 5.45 | perplexity: 232.16 | lr: 2.32e-04 | norm: 0.8272 | dt: 18671.2377ms | tok/sec: 21.0600\n",
      "step  166 | train loss: 5.53 | val loss: 5.44 | perplexity: 230.39 | lr: 2.34e-04 | norm: 0.8479 | dt: 18650.5759ms | tok/sec: 21.0833\n",
      "step  167 | train loss: 5.65 | val loss: 5.44 | perplexity: 231.41 | lr: 2.35e-04 | norm: 1.0928 | dt: 18677.8839ms | tok/sec: 21.0525\n",
      "step  168 | train loss: 5.72 | val loss: 5.45 | perplexity: 232.22 | lr: 2.36e-04 | norm: 0.8604 | dt: 18981.4484ms | tok/sec: 20.7158\n",
      "step  169 | train loss: 5.36 | val loss: 5.44 | perplexity: 231.03 | lr: 2.38e-04 | norm: 1.0605 | dt: 18204.3846ms | tok/sec: 21.6001\n",
      "step  170 | train loss: 6.05 | val loss: 5.44 | perplexity: 229.35 | lr: 2.39e-04 | norm: 1.0399 | dt: 17178.7627ms | tok/sec: 22.8897\n",
      "step  171 | train loss: 5.54 | val loss: 5.42 | perplexity: 226.13 | lr: 2.41e-04 | norm: 0.7014 | dt: 17054.0142ms | tok/sec: 23.0571\n",
      "step  172 | train loss: 5.39 | val loss: 5.41 | perplexity: 224.04 | lr: 2.42e-04 | norm: 0.8540 | dt: 17108.0706ms | tok/sec: 22.9842\n",
      "step  173 | train loss: 5.69 | val loss: 5.40 | perplexity: 221.53 | lr: 2.43e-04 | norm: 0.7307 | dt: 17061.5203ms | tok/sec: 23.0469\n",
      "step  174 | train loss: 5.89 | val loss: 5.40 | perplexity: 221.73 | lr: 2.45e-04 | norm: 0.9101 | dt: 17087.9731ms | tok/sec: 23.0113\n",
      "step  175 | train loss: 5.65 | val loss: 5.40 | perplexity: 221.90 | lr: 2.46e-04 | norm: 0.6539 | dt: 17049.0074ms | tok/sec: 23.0639\n",
      "step  176 | train loss: 5.60 | val loss: 5.39 | perplexity: 218.59 | lr: 2.48e-04 | norm: 0.9223 | dt: 17061.6496ms | tok/sec: 23.0468\n",
      "step  177 | train loss: 5.61 | val loss: 5.38 | perplexity: 217.65 | lr: 2.49e-04 | norm: 0.8039 | dt: 17154.3026ms | tok/sec: 22.9223\n",
      "step  178 | train loss: 5.63 | val loss: 5.39 | perplexity: 218.24 | lr: 2.50e-04 | norm: 0.7454 | dt: 17102.1793ms | tok/sec: 22.9922\n",
      "step  179 | train loss: 5.66 | val loss: 5.38 | perplexity: 216.88 | lr: 2.52e-04 | norm: 0.7156 | dt: 17078.9990ms | tok/sec: 23.0234\n",
      "step  180 | train loss: 5.65 | val loss: 5.38 | perplexity: 216.04 | lr: 2.53e-04 | norm: 0.7860 | dt: 17105.7262ms | tok/sec: 22.9874\n",
      "step  181 | train loss: 5.61 | val loss: 5.37 | perplexity: 215.48 | lr: 2.55e-04 | norm: 0.7100 | dt: 17097.8985ms | tok/sec: 22.9979\n",
      "step  182 | train loss: 5.80 | val loss: 5.38 | perplexity: 217.29 | lr: 2.56e-04 | norm: 1.1140 | dt: 17106.9908ms | tok/sec: 22.9857\n",
      "step  183 | train loss: 5.73 | val loss: 5.39 | perplexity: 218.68 | lr: 2.57e-04 | norm: 1.0831 | dt: 17168.6785ms | tok/sec: 22.9031\n",
      "step  184 | train loss: 5.72 | val loss: 5.39 | perplexity: 219.46 | lr: 2.59e-04 | norm: 1.1727 | dt: 17072.7034ms | tok/sec: 23.0319\n",
      "step  185 | train loss: 5.75 | val loss: 5.40 | perplexity: 221.74 | lr: 2.60e-04 | norm: 1.0133 | dt: 17038.0766ms | tok/sec: 23.0787\n",
      "step  186 | train loss: 5.60 | val loss: 5.39 | perplexity: 218.42 | lr: 2.62e-04 | norm: 0.9659 | dt: 17043.7188ms | tok/sec: 23.0710\n",
      "step  187 | train loss: 5.58 | val loss: 5.39 | perplexity: 218.33 | lr: 2.63e-04 | norm: 0.7190 | dt: 17069.5009ms | tok/sec: 23.0362\n",
      "step  188 | train loss: 5.62 | val loss: 5.38 | perplexity: 216.19 | lr: 2.64e-04 | norm: 0.7580 | dt: 17125.8986ms | tok/sec: 22.9603\n",
      "step  189 | train loss: 5.54 | val loss: 5.37 | perplexity: 215.36 | lr: 2.66e-04 | norm: 0.7049 | dt: 17081.5709ms | tok/sec: 23.0199\n",
      "step  190 | train loss: 5.44 | val loss: 5.36 | perplexity: 212.54 | lr: 2.67e-04 | norm: 0.9444 | dt: 17080.3008ms | tok/sec: 23.0216\n",
      "step  191 | train loss: 5.91 | val loss: 5.36 | perplexity: 213.52 | lr: 2.69e-04 | norm: 0.7951 | dt: 17078.2924ms | tok/sec: 23.0243\n",
      "step  192 | train loss: 6.26 | val loss: 5.38 | perplexity: 216.09 | lr: 2.70e-04 | norm: 1.2725 | dt: 17043.5283ms | tok/sec: 23.0713\n",
      "step  193 | train loss: 5.87 | val loss: 5.37 | perplexity: 214.25 | lr: 2.71e-04 | norm: 1.5662 | dt: 17049.9110ms | tok/sec: 23.0626\n",
      "step  194 | train loss: 5.50 | val loss: 5.35 | perplexity: 211.34 | lr: 2.73e-04 | norm: 1.2040 | dt: 17085.8469ms | tok/sec: 23.0141\n",
      "step  195 | train loss: 5.63 | val loss: 5.35 | perplexity: 210.20 | lr: 2.74e-04 | norm: 0.7158 | dt: 17115.8447ms | tok/sec: 22.9738\n",
      "step  196 | train loss: 5.42 | val loss: 5.34 | perplexity: 208.20 | lr: 2.76e-04 | norm: 0.7725 | dt: 17110.4648ms | tok/sec: 22.9810\n",
      "step  197 | train loss: 5.30 | val loss: 5.33 | perplexity: 206.38 | lr: 2.77e-04 | norm: 0.9162 | dt: 17113.3547ms | tok/sec: 22.9771\n",
      "step  198 | train loss: 5.42 | val loss: 5.32 | perplexity: 205.18 | lr: 2.78e-04 | norm: 0.7165 | dt: 17019.6249ms | tok/sec: 23.1037\n",
      "step  199 | train loss: 5.52 | val loss: 5.32 | perplexity: 204.17 | lr: 2.80e-04 | norm: 0.7934 | dt: 17093.0173ms | tok/sec: 23.0045\n",
      "step  200 | train loss: 5.05 | val loss: 5.31 | perplexity: 203.00 | lr: 2.81e-04 | norm: 0.8855 | dt: 17094.8277ms | tok/sec: 23.0020\n",
      "step  201 | train loss: 5.62 | val loss: 5.31 | perplexity: 202.94 | lr: 2.83e-04 | norm: 0.7307 | dt: 17154.4883ms | tok/sec: 22.9220\n",
      "step  202 | train loss: 5.33 | val loss: 5.31 | perplexity: 202.06 | lr: 2.84e-04 | norm: 0.7774 | dt: 17150.7971ms | tok/sec: 22.9270\n",
      "step  203 | train loss: 5.35 | val loss: 5.30 | perplexity: 200.52 | lr: 2.85e-04 | norm: 0.6838 | dt: 17088.6726ms | tok/sec: 23.0103\n",
      "step  204 | train loss: 5.20 | val loss: 5.30 | perplexity: 200.56 | lr: 2.87e-04 | norm: 0.6577 | dt: 17084.3351ms | tok/sec: 23.0162\n",
      "step  205 | train loss: 5.58 | val loss: 5.34 | perplexity: 209.51 | lr: 2.88e-04 | norm: 0.9611 | dt: 17083.0445ms | tok/sec: 23.0179\n",
      "step  206 | train loss: 5.55 | val loss: 5.34 | perplexity: 207.89 | lr: 2.90e-04 | norm: 1.2812 | dt: 17099.6509ms | tok/sec: 22.9956\n",
      "step  207 | train loss: 5.29 | val loss: 5.32 | perplexity: 204.45 | lr: 2.91e-04 | norm: 1.1422 | dt: 17073.2565ms | tok/sec: 23.0311\n",
      "step  208 | train loss: 5.64 | val loss: 5.32 | perplexity: 204.66 | lr: 2.92e-04 | norm: 0.9152 | dt: 17076.6473ms | tok/sec: 23.0265\n",
      "step  209 | train loss: 5.51 | val loss: 5.32 | perplexity: 203.98 | lr: 2.94e-04 | norm: 1.1822 | dt: 17131.3121ms | tok/sec: 22.9531\n",
      "step  210 | train loss: 5.33 | val loss: 5.34 | perplexity: 207.62 | lr: 2.95e-04 | norm: 1.0117 | dt: 17086.7462ms | tok/sec: 23.0129\n",
      "step  211 | train loss: 5.61 | val loss: 5.33 | perplexity: 206.71 | lr: 2.97e-04 | norm: 1.2040 | dt: 17064.9695ms | tok/sec: 23.0423\n",
      "step  212 | train loss: 5.60 | val loss: 5.31 | perplexity: 202.01 | lr: 2.98e-04 | norm: 1.2786 | dt: 17056.9165ms | tok/sec: 23.0532\n",
      "step  213 | train loss: 5.65 | val loss: 5.30 | perplexity: 201.13 | lr: 2.99e-04 | norm: 0.7193 | dt: 17085.4766ms | tok/sec: 23.0146\n",
      "step  214 | train loss: 5.54 | val loss: 5.32 | perplexity: 203.39 | lr: 3.01e-04 | norm: 0.7788 | dt: 17067.2271ms | tok/sec: 23.0392\n",
      "step  215 | train loss: 6.01 | val loss: 5.33 | perplexity: 205.95 | lr: 3.02e-04 | norm: 1.0400 | dt: 17072.3860ms | tok/sec: 23.0323\n",
      "step  216 | train loss: 5.64 | val loss: 5.32 | perplexity: 204.60 | lr: 3.03e-04 | norm: 1.0034 | dt: 17135.5245ms | tok/sec: 22.9474\n",
      "step  217 | train loss: 5.54 | val loss: 5.31 | perplexity: 203.33 | lr: 3.05e-04 | norm: 0.7501 | dt: 17079.9110ms | tok/sec: 23.0221\n",
      "step  218 | train loss: 5.26 | val loss: 5.32 | perplexity: 204.57 | lr: 3.06e-04 | norm: 1.5384 | dt: 17193.3458ms | tok/sec: 22.8702\n",
      "step  219 | train loss: 5.43 | val loss: 5.31 | perplexity: 202.94 | lr: 3.08e-04 | norm: 1.2570 | dt: 17187.7680ms | tok/sec: 22.8777\n",
      "step  220 | train loss: 5.48 | val loss: 5.30 | perplexity: 201.03 | lr: 3.09e-04 | norm: 0.8978 | dt: 17056.1240ms | tok/sec: 23.0542\n",
      "step  221 | train loss: 5.63 | val loss: 5.29 | perplexity: 199.08 | lr: 3.10e-04 | norm: 0.9989 | dt: 17054.6570ms | tok/sec: 23.0562\n",
      "step  222 | train loss: 5.22 | val loss: 5.31 | perplexity: 201.57 | lr: 3.12e-04 | norm: 0.9433 | dt: 17086.0875ms | tok/sec: 23.0138\n",
      "step  223 | train loss: 5.61 | val loss: 5.30 | perplexity: 199.55 | lr: 3.13e-04 | norm: 0.9348 | dt: 17059.8490ms | tok/sec: 23.0492\n",
      "step  224 | train loss: 5.44 | val loss: 5.29 | perplexity: 199.08 | lr: 3.15e-04 | norm: 0.7016 | dt: 17071.8787ms | tok/sec: 23.0330\n",
      "step  225 | train loss: 5.48 | val loss: 5.29 | perplexity: 197.60 | lr: 3.16e-04 | norm: 0.8023 | dt: 17056.4566ms | tok/sec: 23.0538\n",
      "step  226 | train loss: 5.55 | val loss: 5.28 | perplexity: 196.00 | lr: 3.17e-04 | norm: 0.9662 | dt: 17109.3805ms | tok/sec: 22.9825\n",
      "step  227 | train loss: 5.63 | val loss: 5.29 | perplexity: 197.49 | lr: 3.19e-04 | norm: 0.5596 | dt: 17036.5257ms | tok/sec: 23.0808\n",
      "step  228 | train loss: 5.50 | val loss: 5.29 | perplexity: 197.50 | lr: 3.20e-04 | norm: 0.6367 | dt: 17038.6944ms | tok/sec: 23.0778\n",
      "step  229 | train loss: 5.43 | val loss: 5.27 | perplexity: 193.54 | lr: 3.22e-04 | norm: 0.8779 | dt: 17048.9438ms | tok/sec: 23.0640\n",
      "step  230 | train loss: 5.78 | val loss: 5.28 | perplexity: 196.40 | lr: 3.23e-04 | norm: 0.7582 | dt: 17250.8414ms | tok/sec: 22.7940\n",
      "step  231 | train loss: 5.58 | val loss: 5.28 | perplexity: 196.42 | lr: 3.24e-04 | norm: 1.0744 | dt: 17136.6911ms | tok/sec: 22.9459\n",
      "step  232 | train loss: 5.61 | val loss: 5.28 | perplexity: 195.81 | lr: 3.26e-04 | norm: 0.9315 | dt: 17131.7663ms | tok/sec: 22.9524\n",
      "step  233 | train loss: 5.47 | val loss: 5.28 | perplexity: 196.20 | lr: 3.27e-04 | norm: 0.8761 | dt: 17101.9800ms | tok/sec: 22.9924\n",
      "step  234 | train loss: 5.85 | val loss: 5.28 | perplexity: 196.98 | lr: 3.29e-04 | norm: 0.7857 | dt: 17028.2843ms | tok/sec: 23.0919\n",
      "step  235 | train loss: 5.19 | val loss: 5.28 | perplexity: 196.34 | lr: 3.30e-04 | norm: 0.9954 | dt: 17093.0347ms | tok/sec: 23.0045\n",
      "step  236 | train loss: 5.80 | val loss: 5.28 | perplexity: 195.87 | lr: 3.31e-04 | norm: 1.2632 | dt: 17033.3622ms | tok/sec: 23.0850\n",
      "step  237 | train loss: 5.40 | val loss: 5.29 | perplexity: 197.97 | lr: 3.33e-04 | norm: 0.6986 | dt: 17002.6264ms | tok/sec: 23.1268\n",
      "step  238 | train loss: 5.41 | val loss: 5.29 | perplexity: 197.77 | lr: 3.34e-04 | norm: 0.7592 | dt: 17008.4434ms | tok/sec: 23.1189\n",
      "step  239 | train loss: 5.62 | val loss: 5.30 | perplexity: 199.41 | lr: 3.36e-04 | norm: 0.6684 | dt: 17105.3586ms | tok/sec: 22.9879\n",
      "step  240 | train loss: 5.56 | val loss: 5.29 | perplexity: 198.68 | lr: 3.37e-04 | norm: 1.5740 | dt: 17085.7444ms | tok/sec: 23.0143\n",
      "step  241 | train loss: 5.48 | val loss: 5.28 | perplexity: 197.12 | lr: 3.38e-04 | norm: 0.7570 | dt: 17188.8938ms | tok/sec: 22.8762\n",
      "step  242 | train loss: 5.38 | val loss: 5.27 | perplexity: 194.57 | lr: 3.40e-04 | norm: 1.0182 | dt: 17121.2697ms | tok/sec: 22.9665\n",
      "step  243 | train loss: 5.63 | val loss: 5.27 | perplexity: 194.23 | lr: 3.41e-04 | norm: 0.6548 | dt: 17257.8692ms | tok/sec: 22.7847\n",
      "step  244 | train loss: 5.42 | val loss: 5.26 | perplexity: 192.11 | lr: 3.43e-04 | norm: 0.9799 | dt: 17117.8744ms | tok/sec: 22.9711\n",
      "step  245 | train loss: 5.15 | val loss: 5.24 | perplexity: 188.72 | lr: 3.44e-04 | norm: 0.7955 | dt: 17116.0536ms | tok/sec: 22.9735\n",
      "step  246 | train loss: 6.12 | val loss: 5.26 | perplexity: 192.89 | lr: 3.45e-04 | norm: 1.4988 | dt: 17106.2682ms | tok/sec: 22.9867\n",
      "step  247 | train loss: 6.02 | val loss: 5.32 | perplexity: 205.02 | lr: 3.47e-04 | norm: 2.2660 | dt: 17086.1747ms | tok/sec: 23.0137\n",
      "step  248 | train loss: 5.49 | val loss: 5.29 | perplexity: 198.66 | lr: 3.48e-04 | norm: 1.6826 | dt: 17009.9342ms | tok/sec: 23.1168\n",
      "step  249 | train loss: 5.31 | val loss: 5.26 | perplexity: 192.17 | lr: 3.50e-04 | norm: 0.9124 | dt: 17107.1472ms | tok/sec: 22.9855\n",
      "step  250 | train loss: 5.59 | val loss: 5.26 | perplexity: 191.86 | lr: 3.51e-04 | norm: 0.9587 | dt: 17069.4356ms | tok/sec: 23.0363\n",
      "step  251 | train loss: 5.51 | val loss: 5.26 | perplexity: 192.31 | lr: 3.52e-04 | norm: 0.9449 | dt: 17099.0543ms | tok/sec: 22.9964\n",
      "step  252 | train loss: 5.47 | val loss: 5.25 | perplexity: 191.34 | lr: 3.54e-04 | norm: 0.9361 | dt: 17102.5131ms | tok/sec: 22.9917\n",
      "step  253 | train loss: 5.51 | val loss: 5.26 | perplexity: 192.92 | lr: 3.55e-04 | norm: 0.8878 | dt: 17049.2454ms | tok/sec: 23.0635\n",
      "step  254 | train loss: 5.90 | val loss: 5.27 | perplexity: 194.11 | lr: 3.57e-04 | norm: 0.8810 | dt: 17170.3603ms | tok/sec: 22.9009\n",
      "step  255 | train loss: 5.45 | val loss: 5.26 | perplexity: 193.02 | lr: 3.58e-04 | norm: 0.8531 | dt: 17113.6944ms | tok/sec: 22.9767\n",
      "step  256 | train loss: 5.65 | val loss: 5.25 | perplexity: 190.73 | lr: 3.59e-04 | norm: 1.0044 | dt: 17065.1267ms | tok/sec: 23.0421\n",
      "step  257 | train loss: 5.32 | val loss: 5.23 | perplexity: 186.82 | lr: 3.61e-04 | norm: 0.9159 | dt: 17118.2096ms | tok/sec: 22.9706\n",
      "step  258 | train loss: 5.36 | val loss: 5.25 | perplexity: 190.15 | lr: 3.62e-04 | norm: 0.6770 | dt: 17075.1791ms | tok/sec: 23.0285\n",
      "step  259 | train loss: 5.59 | val loss: 5.25 | perplexity: 190.03 | lr: 3.64e-04 | norm: 0.8007 | dt: 17084.5735ms | tok/sec: 23.0159\n",
      "step  260 | train loss: 5.52 | val loss: 5.23 | perplexity: 186.79 | lr: 3.65e-04 | norm: 0.9594 | dt: 17021.2865ms | tok/sec: 23.1014\n",
      "step  261 | train loss: 5.50 | val loss: 5.24 | perplexity: 188.83 | lr: 3.66e-04 | norm: 0.9548 | dt: 17049.2983ms | tok/sec: 23.0635\n",
      "step  262 | train loss: 5.79 | val loss: 5.27 | perplexity: 194.09 | lr: 3.68e-04 | norm: 0.8598 | dt: 17110.9064ms | tok/sec: 22.9804\n",
      "step  263 | train loss: 5.42 | val loss: 5.25 | perplexity: 191.49 | lr: 3.69e-04 | norm: 0.9028 | dt: 17124.8875ms | tok/sec: 22.9617\n",
      "step  264 | train loss: 5.28 | val loss: 5.23 | perplexity: 186.75 | lr: 3.71e-04 | norm: 0.8491 | dt: 17209.1331ms | tok/sec: 22.8493\n",
      "step  265 | train loss: 5.25 | val loss: 5.21 | perplexity: 183.88 | lr: 3.72e-04 | norm: 0.6209 | dt: 17052.2971ms | tok/sec: 23.0594\n",
      "step  266 | train loss: 5.65 | val loss: 5.22 | perplexity: 184.46 | lr: 3.73e-04 | norm: 0.6486 | dt: 17102.8640ms | tok/sec: 22.9912\n",
      "step  267 | train loss: 5.43 | val loss: 5.22 | perplexity: 184.38 | lr: 3.75e-04 | norm: 0.6932 | dt: 17078.6881ms | tok/sec: 23.0238\n",
      "step  268 | train loss: 5.73 | val loss: 5.21 | perplexity: 183.94 | lr: 3.76e-04 | norm: 0.6477 | dt: 17057.4174ms | tok/sec: 23.0525\n",
      "step  269 | train loss: 5.37 | val loss: 5.21 | perplexity: 182.98 | lr: 3.78e-04 | norm: 0.7835 | dt: 17054.5163ms | tok/sec: 23.0564\n",
      "step  270 | train loss: 5.27 | val loss: 5.20 | perplexity: 181.74 | lr: 3.79e-04 | norm: 0.7678 | dt: 17093.9827ms | tok/sec: 23.0032\n",
      "step  271 | train loss: 5.37 | val loss: 5.21 | perplexity: 182.66 | lr: 3.80e-04 | norm: 0.6732 | dt: 17106.6065ms | tok/sec: 22.9862\n",
      "step  272 | train loss: 5.60 | val loss: 5.21 | perplexity: 183.31 | lr: 3.82e-04 | norm: 0.6659 | dt: 17087.7962ms | tok/sec: 23.0115\n",
      "step  273 | train loss: 5.21 | val loss: 5.19 | perplexity: 180.26 | lr: 3.83e-04 | norm: 0.7974 | dt: 17092.3278ms | tok/sec: 23.0054\n",
      "step  274 | train loss: 5.33 | val loss: 5.18 | perplexity: 177.94 | lr: 3.85e-04 | norm: 0.7621 | dt: 17059.8979ms | tok/sec: 23.0491\n",
      "step  275 | train loss: 5.21 | val loss: 5.18 | perplexity: 176.80 | lr: 3.86e-04 | norm: 0.6746 | dt: 17034.8105ms | tok/sec: 23.0831\n",
      "step  276 | train loss: 5.40 | val loss: 5.18 | perplexity: 177.82 | lr: 3.87e-04 | norm: 0.6616 | dt: 17071.5826ms | tok/sec: 23.0334\n",
      "step  277 | train loss: 5.33 | val loss: 5.17 | perplexity: 176.67 | lr: 3.89e-04 | norm: 1.0172 | dt: 17405.7207ms | tok/sec: 22.5912\n",
      "step  278 | train loss: 5.20 | val loss: 5.18 | perplexity: 176.80 | lr: 3.90e-04 | norm: 0.5945 | dt: 17061.0220ms | tok/sec: 23.0476\n",
      "step  279 | train loss: 5.58 | val loss: 5.17 | perplexity: 176.32 | lr: 3.92e-04 | norm: 0.9089 | dt: 17073.5137ms | tok/sec: 23.0308\n",
      "step  280 | train loss: 5.40 | val loss: 5.18 | perplexity: 178.31 | lr: 3.93e-04 | norm: 0.6644 | dt: 17047.0662ms | tok/sec: 23.0665\n",
      "step  281 | train loss: 5.29 | val loss: 5.18 | perplexity: 177.67 | lr: 3.94e-04 | norm: 0.6670 | dt: 17113.7147ms | tok/sec: 22.9767\n",
      "step  282 | train loss: 5.27 | val loss: 5.18 | perplexity: 176.89 | lr: 3.96e-04 | norm: 0.6438 | dt: 17058.7857ms | tok/sec: 23.0506\n",
      "step  283 | train loss: 5.70 | val loss: 5.18 | perplexity: 178.43 | lr: 3.97e-04 | norm: 0.8757 | dt: 17065.2442ms | tok/sec: 23.0419\n",
      "step  284 | train loss: 5.02 | val loss: 5.18 | perplexity: 177.71 | lr: 3.99e-04 | norm: 1.1848 | dt: 17249.5511ms | tok/sec: 22.7957\n",
      "step  285 | train loss: 5.49 | val loss: 5.22 | perplexity: 185.53 | lr: 4.00e-04 | norm: 0.7988 | dt: 17065.9616ms | tok/sec: 23.0410\n",
      "step  286 | train loss: 5.29 | val loss: 5.20 | perplexity: 180.71 | lr: 4.01e-04 | norm: 1.2381 | dt: 17076.6399ms | tok/sec: 23.0265\n",
      "step  287 | train loss: 5.40 | val loss: 5.19 | perplexity: 179.32 | lr: 4.03e-04 | norm: 0.7532 | dt: 17078.8121ms | tok/sec: 23.0236\n",
      "step  288 | train loss: 5.57 | val loss: 5.18 | perplexity: 177.15 | lr: 4.04e-04 | norm: 0.8021 | dt: 17052.5112ms | tok/sec: 23.0591\n",
      "step  289 | train loss: 5.45 | val loss: 5.17 | perplexity: 176.77 | lr: 4.06e-04 | norm: 0.5785 | dt: 17055.6090ms | tok/sec: 23.0549\n",
      "step  290 | train loss: 5.38 | val loss: 5.18 | perplexity: 177.08 | lr: 4.07e-04 | norm: 0.6255 | dt: 17060.0450ms | tok/sec: 23.0489\n",
      "step  291 | train loss: 5.43 | val loss: 5.19 | perplexity: 178.74 | lr: 4.08e-04 | norm: 0.6787 | dt: 17133.9581ms | tok/sec: 22.9495\n",
      "step  292 | train loss: 5.17 | val loss: 5.17 | perplexity: 175.85 | lr: 4.10e-04 | norm: 0.7089 | dt: 17062.0222ms | tok/sec: 23.0463\n",
      "step  293 | train loss: 5.21 | val loss: 5.17 | perplexity: 175.79 | lr: 4.11e-04 | norm: 0.6647 | dt: 17110.6513ms | tok/sec: 22.9808\n",
      "step  294 | train loss: 5.20 | val loss: 5.17 | perplexity: 175.55 | lr: 4.13e-04 | norm: 0.5371 | dt: 17095.7172ms | tok/sec: 23.0008\n",
      "step  295 | train loss: 5.27 | val loss: 5.16 | perplexity: 173.33 | lr: 4.14e-04 | norm: 0.6544 | dt: 17151.3457ms | tok/sec: 22.9262\n",
      "step  296 | train loss: 5.09 | val loss: 5.18 | perplexity: 177.35 | lr: 4.15e-04 | norm: 0.9414 | dt: 17115.2401ms | tok/sec: 22.9746\n",
      "step  297 | train loss: 5.32 | val loss: 5.16 | perplexity: 174.33 | lr: 4.17e-04 | norm: 0.8394 | dt: 17144.2533ms | tok/sec: 22.9357\n",
      "step  298 | train loss: 5.53 | val loss: 5.17 | perplexity: 176.59 | lr: 4.18e-04 | norm: 0.6032 | dt: 17153.8801ms | tok/sec: 22.9229\n",
      "step  299 | train loss: 5.12 | val loss: 5.16 | perplexity: 173.39 | lr: 4.20e-04 | norm: 1.1774 | dt: 17035.9561ms | tok/sec: 23.0815\n",
      "step  300 | train loss: 5.41 | val loss: 5.16 | perplexity: 174.76 | lr: 4.21e-04 | norm: 0.8429 | dt: 17094.0106ms | tok/sec: 23.0031\n",
      "step  301 | train loss: 5.18 | val loss: 5.15 | perplexity: 171.58 | lr: 4.22e-04 | norm: 0.8969 | dt: 17100.0972ms | tok/sec: 22.9950\n",
      "step  302 | train loss: 5.15 | val loss: 5.14 | perplexity: 170.77 | lr: 4.24e-04 | norm: 0.7214 | dt: 17080.0421ms | tok/sec: 23.0220\n",
      "step  303 | train loss: 5.07 | val loss: 5.13 | perplexity: 169.13 | lr: 4.25e-04 | norm: 0.7991 | dt: 17095.3023ms | tok/sec: 23.0014\n",
      "step  304 | train loss: 5.39 | val loss: 5.13 | perplexity: 168.34 | lr: 4.27e-04 | norm: 0.5432 | dt: 17098.0098ms | tok/sec: 22.9978\n",
      "step  305 | train loss: 5.37 | val loss: 5.13 | perplexity: 168.43 | lr: 4.28e-04 | norm: 0.5296 | dt: 17123.8387ms | tok/sec: 22.9631\n",
      "step  306 | train loss: 5.63 | val loss: 5.14 | perplexity: 170.17 | lr: 4.29e-04 | norm: 0.5847 | dt: 17102.6363ms | tok/sec: 22.9915\n",
      "step  307 | train loss: 5.40 | val loss: 5.12 | perplexity: 167.47 | lr: 4.31e-04 | norm: 1.0373 | dt: 17056.8316ms | tok/sec: 23.0533\n",
      "step  308 | train loss: 5.22 | val loss: 5.13 | perplexity: 168.55 | lr: 4.32e-04 | norm: 0.6724 | dt: 17067.7888ms | tok/sec: 23.0385\n",
      "step  309 | train loss: 5.22 | val loss: 5.12 | perplexity: 168.13 | lr: 4.34e-04 | norm: 0.7911 | dt: 17146.7743ms | tok/sec: 22.9324\n",
      "step  310 | train loss: 5.17 | val loss: 5.12 | perplexity: 166.85 | lr: 4.35e-04 | norm: 0.7363 | dt: 17090.1382ms | tok/sec: 23.0084\n",
      "step  311 | train loss: 5.08 | val loss: 5.12 | perplexity: 167.13 | lr: 4.36e-04 | norm: 0.5613 | dt: 17063.6537ms | tok/sec: 23.0441\n",
      "step  312 | train loss: 5.18 | val loss: 5.12 | perplexity: 167.46 | lr: 4.38e-04 | norm: 0.6471 | dt: 17096.5030ms | tok/sec: 22.9998\n",
      "step  313 | train loss: 5.58 | val loss: 5.12 | perplexity: 167.77 | lr: 4.39e-04 | norm: 0.5594 | dt: 17108.4104ms | tok/sec: 22.9838\n",
      "step  314 | train loss: 5.51 | val loss: 5.13 | perplexity: 168.70 | lr: 4.41e-04 | norm: 0.4569 | dt: 17069.8955ms | tok/sec: 23.0356\n",
      "step  315 | train loss: 5.55 | val loss: 5.13 | perplexity: 169.63 | lr: 4.42e-04 | norm: 0.8134 | dt: 17093.4985ms | tok/sec: 23.0038\n",
      "step  316 | train loss: 5.40 | val loss: 5.13 | perplexity: 168.71 | lr: 4.43e-04 | norm: 0.6051 | dt: 17128.8302ms | tok/sec: 22.9564\n",
      "step  317 | train loss: 5.25 | val loss: 5.11 | perplexity: 166.08 | lr: 4.45e-04 | norm: 0.5860 | dt: 17081.9769ms | tok/sec: 23.0193\n",
      "step  318 | train loss: 5.58 | val loss: 5.12 | perplexity: 166.92 | lr: 4.46e-04 | norm: 1.6726 | dt: 17147.7737ms | tok/sec: 22.9310\n",
      "step  319 | train loss: 5.51 | val loss: 5.12 | perplexity: 166.64 | lr: 4.48e-04 | norm: 0.8655 | dt: 17088.0642ms | tok/sec: 23.0111\n",
      "step  320 | train loss: 5.30 | val loss: 5.13 | perplexity: 168.74 | lr: 4.49e-04 | norm: 0.5952 | dt: 17184.3519ms | tok/sec: 22.8822\n",
      "step  321 | train loss: 5.40 | val loss: 5.13 | perplexity: 168.43 | lr: 4.50e-04 | norm: 0.9699 | dt: 17117.5430ms | tok/sec: 22.9715\n",
      "step  322 | train loss: 5.50 | val loss: 5.13 | perplexity: 169.00 | lr: 4.52e-04 | norm: 0.9742 | dt: 17152.9436ms | tok/sec: 22.9241\n",
      "step  323 | train loss: 5.13 | val loss: 5.12 | perplexity: 166.58 | lr: 4.53e-04 | norm: 0.7493 | dt: 17127.6875ms | tok/sec: 22.9579\n",
      "step  324 | train loss: 5.46 | val loss: 5.12 | perplexity: 167.23 | lr: 4.55e-04 | norm: 0.6606 | dt: 17076.3371ms | tok/sec: 23.0270\n",
      "step  325 | train loss: 5.36 | val loss: 5.13 | perplexity: 168.57 | lr: 4.56e-04 | norm: 0.4755 | dt: 17191.2715ms | tok/sec: 22.8730\n",
      "step  326 | train loss: 5.16 | val loss: 5.11 | perplexity: 166.06 | lr: 4.57e-04 | norm: 0.7237 | dt: 17054.4322ms | tok/sec: 23.0565\n",
      "step  327 | train loss: 5.23 | val loss: 5.10 | perplexity: 164.67 | lr: 4.59e-04 | norm: 0.6116 | dt: 17079.7825ms | tok/sec: 23.0223\n",
      "step  328 | train loss: 5.11 | val loss: 5.10 | perplexity: 163.79 | lr: 4.60e-04 | norm: 0.6767 | dt: 17138.2434ms | tok/sec: 22.9438\n",
      "step  329 | train loss: 5.37 | val loss: 5.09 | perplexity: 163.02 | lr: 4.62e-04 | norm: 0.6444 | dt: 17083.4010ms | tok/sec: 23.0174\n",
      "step  330 | train loss: 5.40 | val loss: 5.09 | perplexity: 161.85 | lr: 4.63e-04 | norm: 0.4860 | dt: 17080.7364ms | tok/sec: 23.0210\n",
      "step  331 | train loss: 5.05 | val loss: 5.08 | perplexity: 160.42 | lr: 4.64e-04 | norm: 0.7311 | dt: 17148.1404ms | tok/sec: 22.9305\n",
      "step  332 | train loss: 5.09 | val loss: 5.08 | perplexity: 161.05 | lr: 4.66e-04 | norm: 0.5777 | dt: 17207.6252ms | tok/sec: 22.8513\n",
      "step  333 | train loss: 5.42 | val loss: 5.09 | perplexity: 161.69 | lr: 4.67e-04 | norm: 0.9155 | dt: 17102.3057ms | tok/sec: 22.9920\n",
      "step  334 | train loss: 5.06 | val loss: 5.09 | perplexity: 161.71 | lr: 4.69e-04 | norm: 0.7475 | dt: 17094.0053ms | tok/sec: 23.0032\n",
      "step  335 | train loss: 5.36 | val loss: 5.09 | perplexity: 161.84 | lr: 4.70e-04 | norm: 0.6088 | dt: 17097.6288ms | tok/sec: 22.9983\n",
      "step  336 | train loss: 5.00 | val loss: 5.08 | perplexity: 160.75 | lr: 4.71e-04 | norm: 0.7189 | dt: 17132.5526ms | tok/sec: 22.9514\n",
      "step  337 | train loss: 5.31 | val loss: 5.08 | perplexity: 160.52 | lr: 4.73e-04 | norm: 0.5814 | dt: 17186.9678ms | tok/sec: 22.8787\n",
      "step  338 | train loss: 5.40 | val loss: 5.07 | perplexity: 159.15 | lr: 4.74e-04 | norm: 0.7142 | dt: 17150.2008ms | tok/sec: 22.9278\n",
      "step  339 | train loss: 5.18 | val loss: 5.07 | perplexity: 159.26 | lr: 4.76e-04 | norm: 0.5128 | dt: 17056.6118ms | tok/sec: 23.0536\n",
      "step  340 | train loss: 5.42 | val loss: 5.07 | perplexity: 159.67 | lr: 4.77e-04 | norm: 0.7130 | dt: 17100.3387ms | tok/sec: 22.9946\n",
      "step  341 | train loss: 5.34 | val loss: 5.07 | perplexity: 159.42 | lr: 4.78e-04 | norm: 0.5432 | dt: 17083.1501ms | tok/sec: 23.0178\n",
      "step  342 | train loss: 5.39 | val loss: 5.07 | perplexity: 159.17 | lr: 4.80e-04 | norm: 0.6143 | dt: 17119.5736ms | tok/sec: 22.9688\n",
      "step  343 | train loss: 5.03 | val loss: 5.06 | perplexity: 158.26 | lr: 4.81e-04 | norm: 0.6823 | dt: 17134.2487ms | tok/sec: 22.9491\n",
      "step  344 | train loss: 5.40 | val loss: 5.06 | perplexity: 157.72 | lr: 4.83e-04 | norm: 0.5150 | dt: 17148.9983ms | tok/sec: 22.9294\n",
      "step  345 | train loss: 5.17 | val loss: 5.07 | perplexity: 159.29 | lr: 4.84e-04 | norm: 1.1110 | dt: 17068.3184ms | tok/sec: 23.0378\n",
      "step  346 | train loss: 5.27 | val loss: 5.08 | perplexity: 160.83 | lr: 4.85e-04 | norm: 0.5953 | dt: 17127.6062ms | tok/sec: 22.9580\n",
      "step  347 | train loss: 5.24 | val loss: 5.07 | perplexity: 159.38 | lr: 4.87e-04 | norm: 0.6420 | dt: 17159.4741ms | tok/sec: 22.9154\n",
      "step  348 | train loss: 5.34 | val loss: 5.06 | perplexity: 158.14 | lr: 4.88e-04 | norm: 0.6057 | dt: 17092.8855ms | tok/sec: 23.0047\n",
      "step  349 | train loss: 5.16 | val loss: 5.06 | perplexity: 157.82 | lr: 4.90e-04 | norm: 0.4129 | dt: 17047.6837ms | tok/sec: 23.0657\n",
      "step  350 | train loss: 5.15 | val loss: 5.06 | perplexity: 157.96 | lr: 4.91e-04 | norm: 0.5443 | dt: 17114.4571ms | tok/sec: 22.9757\n",
      "step  351 | train loss: 4.95 | val loss: 5.07 | perplexity: 158.78 | lr: 4.92e-04 | norm: 0.5699 | dt: 17163.9230ms | tok/sec: 22.9094\n",
      "step  352 | train loss: 5.34 | val loss: 5.07 | perplexity: 159.69 | lr: 4.94e-04 | norm: 0.5271 | dt: 17106.1692ms | tok/sec: 22.9868\n",
      "step  353 | train loss: 5.27 | val loss: 5.06 | perplexity: 158.29 | lr: 4.95e-04 | norm: 0.6987 | dt: 17080.1842ms | tok/sec: 23.0218\n",
      "step  354 | train loss: 5.26 | val loss: 5.06 | perplexity: 157.89 | lr: 4.97e-04 | norm: 0.6280 | dt: 17199.8231ms | tok/sec: 22.8616\n",
      "step  355 | train loss: 4.96 | val loss: 5.07 | perplexity: 158.46 | lr: 4.98e-04 | norm: 0.6640 | dt: 17091.0928ms | tok/sec: 23.0071\n",
      "step  356 | train loss: 5.04 | val loss: 5.05 | perplexity: 156.65 | lr: 4.99e-04 | norm: 0.6079 | dt: 17083.1661ms | tok/sec: 23.0177\n",
      "step  357 | train loss: 5.43 | val loss: 5.06 | perplexity: 158.06 | lr: 5.01e-04 | norm: 0.5942 | dt: 17087.4865ms | tok/sec: 23.0119\n",
      "step  358 | train loss: 5.42 | val loss: 5.06 | perplexity: 158.03 | lr: 5.02e-04 | norm: 0.9057 | dt: 17121.5241ms | tok/sec: 22.9662\n",
      "step  359 | train loss: 5.32 | val loss: 5.06 | perplexity: 158.25 | lr: 5.03e-04 | norm: 0.5379 | dt: 17092.1957ms | tok/sec: 23.0056\n",
      "step  360 | train loss: 5.05 | val loss: 5.07 | perplexity: 158.54 | lr: 5.05e-04 | norm: 0.6716 | dt: 17061.7061ms | tok/sec: 23.0467\n",
      "step  361 | train loss: 5.07 | val loss: 5.07 | perplexity: 158.56 | lr: 5.06e-04 | norm: 0.5591 | dt: 17140.5928ms | tok/sec: 22.9406\n",
      "step  362 | train loss: 5.25 | val loss: 5.06 | perplexity: 156.83 | lr: 5.08e-04 | norm: 0.5824 | dt: 17077.2295ms | tok/sec: 23.0257\n",
      "step  363 | train loss: 5.12 | val loss: 5.06 | perplexity: 156.88 | lr: 5.09e-04 | norm: 0.4952 | dt: 17136.1721ms | tok/sec: 22.9465\n",
      "step  364 | train loss: 4.89 | val loss: 5.06 | perplexity: 157.04 | lr: 5.10e-04 | norm: 0.5321 | dt: 17106.1294ms | tok/sec: 22.9868\n",
      "step  365 | train loss: 5.28 | val loss: 5.06 | perplexity: 157.62 | lr: 5.12e-04 | norm: 0.5678 | dt: 17105.6013ms | tok/sec: 22.9876\n",
      "step  366 | train loss: 5.55 | val loss: 5.06 | perplexity: 158.04 | lr: 5.13e-04 | norm: 0.8861 | dt: 17244.4820ms | tok/sec: 22.8024\n",
      "step  367 | train loss: 5.22 | val loss: 5.06 | perplexity: 158.06 | lr: 5.15e-04 | norm: 0.7973 | dt: 17085.0909ms | tok/sec: 23.0152\n",
      "step  368 | train loss: 5.17 | val loss: 5.06 | perplexity: 157.62 | lr: 5.16e-04 | norm: 0.7745 | dt: 17136.4965ms | tok/sec: 22.9461\n",
      "step  369 | train loss: 5.35 | val loss: 5.07 | perplexity: 158.59 | lr: 5.17e-04 | norm: 0.5803 | dt: 17180.3746ms | tok/sec: 22.8875\n",
      "step  370 | train loss: 5.49 | val loss: 5.07 | perplexity: 159.07 | lr: 5.19e-04 | norm: 0.7675 | dt: 17150.6708ms | tok/sec: 22.9271\n",
      "step  371 | train loss: 5.57 | val loss: 5.07 | perplexity: 158.82 | lr: 5.20e-04 | norm: 0.6431 | dt: 17199.1911ms | tok/sec: 22.8625\n",
      "step  372 | train loss: 5.22 | val loss: 5.05 | perplexity: 156.04 | lr: 5.22e-04 | norm: 0.6440 | dt: 17147.9549ms | tok/sec: 22.9308\n",
      "step  373 | train loss: 5.02 | val loss: 5.04 | perplexity: 154.70 | lr: 5.23e-04 | norm: 0.5193 | dt: 17113.7958ms | tok/sec: 22.9766\n",
      "step  374 | train loss: 5.39 | val loss: 5.04 | perplexity: 154.69 | lr: 5.24e-04 | norm: 0.6641 | dt: 17148.4444ms | tok/sec: 22.9301\n",
      "step  375 | train loss: 5.35 | val loss: 5.04 | perplexity: 154.79 | lr: 5.26e-04 | norm: 0.6865 | dt: 17072.8433ms | tok/sec: 23.0317\n",
      "step  376 | train loss: 5.15 | val loss: 5.05 | perplexity: 155.45 | lr: 5.27e-04 | norm: 0.5725 | dt: 17123.0114ms | tok/sec: 22.9642\n",
      "step  377 | train loss: 5.15 | val loss: 5.05 | perplexity: 155.65 | lr: 5.29e-04 | norm: 0.4901 | dt: 17091.1121ms | tok/sec: 23.0070\n",
      "step  378 | train loss: 5.47 | val loss: 5.05 | perplexity: 155.57 | lr: 5.30e-04 | norm: 0.6270 | dt: 17118.4812ms | tok/sec: 22.9703\n",
      "step  379 | train loss: 5.27 | val loss: 5.04 | perplexity: 155.05 | lr: 5.31e-04 | norm: 0.5442 | dt: 17092.2039ms | tok/sec: 23.0056\n",
      "step  380 | train loss: 5.22 | val loss: 5.03 | perplexity: 153.54 | lr: 5.33e-04 | norm: 0.5398 | dt: 17059.0503ms | tok/sec: 23.0503\n",
      "step  381 | train loss: 5.23 | val loss: 5.02 | perplexity: 151.95 | lr: 5.34e-04 | norm: 0.4891 | dt: 17079.3772ms | tok/sec: 23.0229\n",
      "step  382 | train loss: 5.16 | val loss: 5.02 | perplexity: 151.32 | lr: 5.36e-04 | norm: 0.4851 | dt: 17077.9867ms | tok/sec: 23.0247\n",
      "step  383 | train loss: 5.24 | val loss: 5.02 | perplexity: 151.32 | lr: 5.37e-04 | norm: 0.5805 | dt: 17098.0222ms | tok/sec: 22.9977\n",
      "step  384 | train loss: 5.13 | val loss: 5.02 | perplexity: 151.14 | lr: 5.38e-04 | norm: 0.4903 | dt: 17096.3330ms | tok/sec: 23.0000\n",
      "step  385 | train loss: 5.26 | val loss: 5.02 | perplexity: 150.94 | lr: 5.40e-04 | norm: 0.5354 | dt: 17219.0831ms | tok/sec: 22.8361\n",
      "step  386 | train loss: 5.14 | val loss: 5.01 | perplexity: 150.66 | lr: 5.41e-04 | norm: 0.5463 | dt: 17139.1287ms | tok/sec: 22.9426\n",
      "step  387 | train loss: 5.37 | val loss: 5.02 | perplexity: 151.27 | lr: 5.43e-04 | norm: 0.5290 | dt: 17100.9634ms | tok/sec: 22.9938\n",
      "step  388 | train loss: 5.27 | val loss: 5.02 | perplexity: 151.60 | lr: 5.44e-04 | norm: 0.4587 | dt: 17102.5996ms | tok/sec: 22.9916\n",
      "step  389 | train loss: 5.33 | val loss: 5.02 | perplexity: 151.55 | lr: 5.45e-04 | norm: 0.5501 | dt: 17102.8914ms | tok/sec: 22.9912\n",
      "step  390 | train loss: 5.20 | val loss: 5.02 | perplexity: 152.01 | lr: 5.47e-04 | norm: 0.7146 | dt: 17074.7869ms | tok/sec: 23.0290\n",
      "step  391 | train loss: 5.16 | val loss: 5.02 | perplexity: 152.08 | lr: 5.48e-04 | norm: 0.5993 | dt: 17132.0579ms | tok/sec: 22.9521\n",
      "step  392 | train loss: 5.08 | val loss: 5.01 | perplexity: 150.40 | lr: 5.50e-04 | norm: 0.5461 | dt: 17115.9735ms | tok/sec: 22.9736\n",
      "step  393 | train loss: 5.06 | val loss: 5.01 | perplexity: 150.16 | lr: 5.51e-04 | norm: 0.4564 | dt: 17079.6471ms | tok/sec: 23.0225\n",
      "step  394 | train loss: 5.16 | val loss: 5.02 | perplexity: 150.96 | lr: 5.52e-04 | norm: 0.4602 | dt: 17259.7799ms | tok/sec: 22.7822\n",
      "step  395 | train loss: 5.37 | val loss: 5.03 | perplexity: 153.09 | lr: 5.54e-04 | norm: 0.5536 | dt: 17144.7692ms | tok/sec: 22.9350\n",
      "step  396 | train loss: 4.91 | val loss: 5.04 | perplexity: 154.64 | lr: 5.55e-04 | norm: 0.9855 | dt: 17062.0954ms | tok/sec: 23.0462\n",
      "step  397 | train loss: 5.34 | val loss: 5.07 | perplexity: 159.65 | lr: 5.57e-04 | norm: 0.7983 | dt: 17113.2889ms | tok/sec: 22.9772\n",
      "step  398 | train loss: 5.34 | val loss: 5.06 | perplexity: 157.85 | lr: 5.58e-04 | norm: 0.6611 | dt: 17149.7262ms | tok/sec: 22.9284\n",
      "step  399 | train loss: 5.40 | val loss: 5.07 | perplexity: 158.55 | lr: 5.59e-04 | norm: 0.6232 | dt: 17087.1851ms | tok/sec: 23.0123\n",
      "step  400 | train loss: 4.97 | val loss: 5.04 | perplexity: 154.81 | lr: 5.61e-04 | norm: 0.8427 | dt: 17087.7526ms | tok/sec: 23.0116\n",
      "step  401 | train loss: 5.05 | val loss: 5.03 | perplexity: 152.76 | lr: 5.62e-04 | norm: 0.7840 | dt: 17230.7787ms | tok/sec: 22.8206\n",
      "step  402 | train loss: 5.31 | val loss: 5.03 | perplexity: 153.23 | lr: 5.64e-04 | norm: 0.6400 | dt: 17137.7869ms | tok/sec: 22.9444\n",
      "step  403 | train loss: 5.36 | val loss: 5.05 | perplexity: 156.39 | lr: 5.65e-04 | norm: 0.5852 | dt: 17119.8285ms | tok/sec: 22.9685\n",
      "step  404 | train loss: 5.43 | val loss: 5.04 | perplexity: 154.77 | lr: 5.66e-04 | norm: 0.8612 | dt: 17146.8806ms | tok/sec: 22.9322\n",
      "step  405 | train loss: 5.06 | val loss: 5.02 | perplexity: 151.80 | lr: 5.68e-04 | norm: 0.8024 | dt: 17102.4730ms | tok/sec: 22.9918\n",
      "step  406 | train loss: 5.26 | val loss: 5.03 | perplexity: 153.25 | lr: 5.69e-04 | norm: 0.5953 | dt: 17123.5354ms | tok/sec: 22.9635\n",
      "step  407 | train loss: 5.43 | val loss: 5.05 | perplexity: 155.53 | lr: 5.71e-04 | norm: 0.7289 | dt: 17140.4002ms | tok/sec: 22.9409\n",
      "step  408 | train loss: 5.06 | val loss: 5.04 | perplexity: 154.49 | lr: 5.72e-04 | norm: 0.7694 | dt: 17125.8452ms | tok/sec: 22.9604\n",
      "step  409 | train loss: 5.18 | val loss: 5.03 | perplexity: 153.41 | lr: 5.73e-04 | norm: 0.5445 | dt: 17200.6288ms | tok/sec: 22.8606\n",
      "step  410 | train loss: 5.08 | val loss: 5.02 | perplexity: 151.18 | lr: 5.75e-04 | norm: 0.5557 | dt: 17135.8721ms | tok/sec: 22.9469\n",
      "step  411 | train loss: 5.07 | val loss: 5.01 | perplexity: 150.04 | lr: 5.76e-04 | norm: 0.5756 | dt: 17129.6551ms | tok/sec: 22.9553\n",
      "step  412 | train loss: 5.18 | val loss: 5.01 | perplexity: 149.97 | lr: 5.78e-04 | norm: 0.5000 | dt: 17164.2470ms | tok/sec: 22.9090\n",
      "step  413 | train loss: 5.39 | val loss: 5.01 | perplexity: 150.41 | lr: 5.79e-04 | norm: 0.8198 | dt: 17165.3154ms | tok/sec: 22.9076\n",
      "step  414 | train loss: 5.02 | val loss: 5.02 | perplexity: 150.74 | lr: 5.80e-04 | norm: 0.6078 | dt: 17123.7519ms | tok/sec: 22.9632\n",
      "step  415 | train loss: 5.45 | val loss: 5.02 | perplexity: 151.59 | lr: 5.82e-04 | norm: 0.5952 | dt: 17117.3677ms | tok/sec: 22.9718\n",
      "step  416 | train loss: 5.27 | val loss: 5.03 | perplexity: 152.49 | lr: 5.83e-04 | norm: 0.4674 | dt: 17064.5900ms | tok/sec: 23.0428\n",
      "step  417 | train loss: 4.87 | val loss: 5.00 | perplexity: 148.62 | lr: 5.85e-04 | norm: 0.8310 | dt: 17141.1710ms | tok/sec: 22.9399\n",
      "step  418 | train loss: 5.33 | val loss: 5.00 | perplexity: 148.53 | lr: 5.86e-04 | norm: 0.5536 | dt: 17208.4398ms | tok/sec: 22.8502\n",
      "step  419 | train loss: 5.44 | val loss: 5.00 | perplexity: 149.03 | lr: 5.87e-04 | norm: 0.5299 | dt: 17271.6711ms | tok/sec: 22.7665\n",
      "step  420 | train loss: 5.19 | val loss: 5.01 | perplexity: 149.44 | lr: 5.89e-04 | norm: 0.5431 | dt: 17129.4770ms | tok/sec: 22.9555\n",
      "step  421 | train loss: 5.12 | val loss: 5.00 | perplexity: 148.87 | lr: 5.90e-04 | norm: 0.5573 | dt: 17190.2869ms | tok/sec: 22.8743\n",
      "step  422 | train loss: 5.29 | val loss: 5.00 | perplexity: 148.62 | lr: 5.92e-04 | norm: 0.6848 | dt: 17113.5550ms | tok/sec: 22.9769\n",
      "step  423 | train loss: 4.93 | val loss: 5.00 | perplexity: 147.75 | lr: 5.93e-04 | norm: 0.6220 | dt: 17080.2541ms | tok/sec: 23.0217\n",
      "step  424 | train loss: 5.21 | val loss: 5.00 | perplexity: 148.80 | lr: 5.94e-04 | norm: 0.6474 | dt: 17075.0237ms | tok/sec: 23.0287\n",
      "step  425 | train loss: 5.28 | val loss: 4.99 | perplexity: 146.96 | lr: 5.96e-04 | norm: 0.6248 | dt: 17068.6574ms | tok/sec: 23.0373\n",
      "step  426 | train loss: 4.90 | val loss: 4.99 | perplexity: 147.48 | lr: 5.97e-04 | norm: 0.4818 | dt: 17091.0721ms | tok/sec: 23.0071\n",
      "step  427 | train loss: 4.97 | val loss: 4.98 | perplexity: 145.59 | lr: 5.99e-04 | norm: 0.6029 | dt: 17065.1088ms | tok/sec: 23.0421\n",
      "step  428 | train loss: 5.05 | val loss: 4.98 | perplexity: 144.83 | lr: 6.00e-04 | norm: 0.6500 | dt: 17181.6256ms | tok/sec: 22.8858\n",
      "step  429 | train loss: 5.15 | val loss: 4.98 | perplexity: 145.42 | lr: 6.01e-04 | norm: 0.4890 | dt: 17068.9030ms | tok/sec: 23.0370\n",
      "step  430 | train loss: 5.09 | val loss: 4.97 | perplexity: 144.66 | lr: 6.03e-04 | norm: 0.4838 | dt: 17081.7735ms | tok/sec: 23.0196\n",
      "step  431 | train loss: 5.28 | val loss: 4.98 | perplexity: 145.13 | lr: 6.04e-04 | norm: 0.6311 | dt: 17079.3009ms | tok/sec: 23.0230\n",
      "step  432 | train loss: 4.97 | val loss: 4.99 | perplexity: 146.43 | lr: 6.06e-04 | norm: 0.6799 | dt: 17078.7609ms | tok/sec: 23.0237\n",
      "step  433 | train loss: 5.00 | val loss: 4.99 | perplexity: 146.51 | lr: 6.07e-04 | norm: 0.5843 | dt: 17090.9643ms | tok/sec: 23.0072\n",
      "step  434 | train loss: 5.21 | val loss: 4.99 | perplexity: 146.85 | lr: 6.08e-04 | norm: 0.6639 | dt: 17104.5039ms | tok/sec: 22.9890\n",
      "step  435 | train loss: 5.29 | val loss: 4.98 | perplexity: 145.95 | lr: 6.10e-04 | norm: 0.9292 | dt: 17080.7788ms | tok/sec: 23.0210\n",
      "step  436 | train loss: 5.28 | val loss: 4.98 | perplexity: 145.89 | lr: 6.11e-04 | norm: 0.7239 | dt: 17090.7791ms | tok/sec: 23.0075\n",
      "step  437 | train loss: 4.75 | val loss: 4.99 | perplexity: 146.76 | lr: 6.13e-04 | norm: 0.4806 | dt: 17087.7109ms | tok/sec: 23.0116\n",
      "step  438 | train loss: 5.35 | val loss: 4.99 | perplexity: 147.41 | lr: 6.14e-04 | norm: 0.8703 | dt: 17104.4426ms | tok/sec: 22.9891\n",
      "step  439 | train loss: 5.01 | val loss: 5.00 | perplexity: 148.02 | lr: 6.15e-04 | norm: 0.9682 | dt: 17096.9267ms | tok/sec: 22.9992\n",
      "step  440 | train loss: 5.17 | val loss: 5.01 | perplexity: 149.54 | lr: 6.17e-04 | norm: 0.5789 | dt: 17133.3525ms | tok/sec: 22.9503\n",
      "step  441 | train loss: 5.18 | val loss: 5.01 | perplexity: 149.48 | lr: 6.18e-04 | norm: 0.7143 | dt: 17125.6502ms | tok/sec: 22.9606\n",
      "step  442 | train loss: 5.20 | val loss: 5.00 | perplexity: 147.96 | lr: 6.20e-04 | norm: 0.7004 | dt: 17107.9311ms | tok/sec: 22.9844\n",
      "step  443 | train loss: 5.00 | val loss: 4.99 | perplexity: 146.52 | lr: 6.21e-04 | norm: 0.6069 | dt: 17064.5685ms | tok/sec: 23.0428\n",
      "step  444 | train loss: 5.29 | val loss: 5.00 | perplexity: 147.78 | lr: 6.22e-04 | norm: 0.6568 | dt: 17127.5680ms | tok/sec: 22.9581\n",
      "step  445 | train loss: 5.43 | val loss: 5.00 | perplexity: 148.47 | lr: 6.24e-04 | norm: 0.5795 | dt: 17348.3510ms | tok/sec: 22.6659\n",
      "step  446 | train loss: 5.07 | val loss: 5.00 | perplexity: 148.47 | lr: 6.25e-04 | norm: 0.5446 | dt: 17380.4803ms | tok/sec: 22.6240\n",
      "step  447 | train loss: 5.01 | val loss: 4.99 | perplexity: 146.35 | lr: 6.27e-04 | norm: 0.6447 | dt: 17119.9696ms | tok/sec: 22.9683\n",
      "step  448 | train loss: 5.10 | val loss: 4.97 | perplexity: 144.06 | lr: 6.28e-04 | norm: 0.6433 | dt: 17137.0454ms | tok/sec: 22.9454\n",
      "step  449 | train loss: 5.29 | val loss: 4.98 | perplexity: 145.92 | lr: 6.29e-04 | norm: 0.5075 | dt: 17133.4004ms | tok/sec: 22.9503\n",
      "step  450 | train loss: 5.08 | val loss: 4.98 | perplexity: 145.94 | lr: 6.31e-04 | norm: 0.6648 | dt: 17079.3703ms | tok/sec: 23.0229\n",
      "step  451 | train loss: 5.05 | val loss: 4.98 | perplexity: 146.04 | lr: 6.32e-04 | norm: 0.6412 | dt: 17093.0583ms | tok/sec: 23.0044\n",
      "step  452 | train loss: 4.98 | val loss: 4.99 | perplexity: 146.85 | lr: 6.34e-04 | norm: 0.4732 | dt: 17186.4002ms | tok/sec: 22.8795\n",
      "step  453 | train loss: 5.24 | val loss: 4.98 | perplexity: 146.03 | lr: 6.35e-04 | norm: 0.5372 | dt: 17076.4854ms | tok/sec: 23.0268\n",
      "step  454 | train loss: 5.32 | val loss: 4.99 | perplexity: 147.07 | lr: 6.36e-04 | norm: 0.5700 | dt: 17117.8350ms | tok/sec: 22.9711\n",
      "step  455 | train loss: 5.33 | val loss: 4.99 | perplexity: 146.49 | lr: 6.38e-04 | norm: 0.5904 | dt: 17087.5440ms | tok/sec: 23.0119\n",
      "step  456 | train loss: 5.08 | val loss: 4.99 | perplexity: 146.59 | lr: 6.39e-04 | norm: 0.5608 | dt: 17057.6580ms | tok/sec: 23.0522\n",
      "step  457 | train loss: 5.12 | val loss: 4.97 | perplexity: 144.27 | lr: 6.41e-04 | norm: 0.9155 | dt: 17169.6641ms | tok/sec: 22.9018\n",
      "step  458 | train loss: 5.08 | val loss: 4.98 | perplexity: 145.31 | lr: 6.42e-04 | norm: 0.5413 | dt: 17147.3114ms | tok/sec: 22.9316\n",
      "step  459 | train loss: 4.89 | val loss: 4.98 | perplexity: 145.15 | lr: 6.43e-04 | norm: 0.5502 | dt: 17103.5779ms | tok/sec: 22.9903\n",
      "step  460 | train loss: 5.13 | val loss: 4.98 | perplexity: 144.82 | lr: 6.45e-04 | norm: 0.5544 | dt: 17026.5591ms | tok/sec: 23.0943\n",
      "step  461 | train loss: 5.21 | val loss: 4.98 | perplexity: 145.97 | lr: 6.46e-04 | norm: 0.4147 | dt: 17159.0214ms | tok/sec: 22.9160\n",
      "step  462 | train loss: 5.09 | val loss: 4.98 | perplexity: 145.84 | lr: 6.48e-04 | norm: 0.5311 | dt: 17039.9895ms | tok/sec: 23.0761\n",
      "step  463 | train loss: 5.11 | val loss: 4.97 | perplexity: 144.22 | lr: 6.49e-04 | norm: 0.5364 | dt: 17052.8204ms | tok/sec: 23.0587\n",
      "step  464 | train loss: 5.00 | val loss: 4.96 | perplexity: 142.46 | lr: 6.50e-04 | norm: 0.5477 | dt: 17133.4543ms | tok/sec: 22.9502\n",
      "step  465 | train loss: 5.10 | val loss: 4.95 | perplexity: 141.52 | lr: 6.52e-04 | norm: 0.5240 | dt: 17275.0945ms | tok/sec: 22.7620\n",
      "step  466 | train loss: 5.11 | val loss: 4.95 | perplexity: 141.88 | lr: 6.53e-04 | norm: 0.5154 | dt: 17313.4575ms | tok/sec: 22.7116\n",
      "step  467 | train loss: 5.26 | val loss: 4.97 | perplexity: 143.67 | lr: 6.55e-04 | norm: 0.5469 | dt: 17087.3394ms | tok/sec: 23.0121\n",
      "step  468 | train loss: 5.02 | val loss: 4.97 | perplexity: 144.51 | lr: 6.56e-04 | norm: 0.4858 | dt: 17055.9053ms | tok/sec: 23.0545\n",
      "step  469 | train loss: 5.15 | val loss: 4.96 | perplexity: 142.93 | lr: 6.57e-04 | norm: 0.5845 | dt: 17100.0876ms | tok/sec: 22.9950\n",
      "step  470 | train loss: 5.27 | val loss: 4.97 | perplexity: 143.97 | lr: 6.59e-04 | norm: 0.6285 | dt: 17231.5369ms | tok/sec: 22.8196\n",
      "step  471 | train loss: 5.13 | val loss: 4.97 | perplexity: 144.74 | lr: 6.60e-04 | norm: 0.4688 | dt: 17128.3863ms | tok/sec: 22.9570\n",
      "step  472 | train loss: 5.24 | val loss: 4.96 | perplexity: 143.02 | lr: 6.62e-04 | norm: 0.5578 | dt: 17103.0002ms | tok/sec: 22.9911\n",
      "step  473 | train loss: 5.29 | val loss: 4.96 | perplexity: 142.41 | lr: 6.63e-04 | norm: 0.4420 | dt: 17036.6335ms | tok/sec: 23.0806\n",
      "step  474 | train loss: 5.16 | val loss: 4.95 | perplexity: 141.74 | lr: 6.64e-04 | norm: 0.5635 | dt: 17158.9508ms | tok/sec: 22.9161\n",
      "step  475 | train loss: 5.06 | val loss: 4.96 | perplexity: 142.37 | lr: 6.66e-04 | norm: 0.5468 | dt: 17128.9065ms | tok/sec: 22.9563\n",
      "step  476 | train loss: 5.44 | val loss: 4.97 | perplexity: 144.04 | lr: 6.67e-04 | norm: 0.5317 | dt: 17182.2879ms | tok/sec: 22.8850\n",
      "step  477 | train loss: 5.12 | val loss: 4.95 | perplexity: 141.75 | lr: 6.69e-04 | norm: 0.5527 | dt: 17112.8931ms | tok/sec: 22.9778\n",
      "step  478 | train loss: 5.03 | val loss: 4.95 | perplexity: 141.52 | lr: 6.70e-04 | norm: 0.4601 | dt: 17112.2074ms | tok/sec: 22.9787\n",
      "step  479 | train loss: 5.05 | val loss: 4.95 | perplexity: 141.47 | lr: 6.71e-04 | norm: 0.7736 | dt: 17121.9554ms | tok/sec: 22.9656\n",
      "step  480 | train loss: 5.02 | val loss: 4.95 | perplexity: 141.59 | lr: 6.73e-04 | norm: 0.5937 | dt: 17113.6007ms | tok/sec: 22.9768\n",
      "step  481 | train loss: 4.99 | val loss: 4.96 | perplexity: 142.08 | lr: 6.74e-04 | norm: 0.4831 | dt: 17077.3127ms | tok/sec: 23.0256\n",
      "step  482 | train loss: 5.17 | val loss: 4.95 | perplexity: 141.43 | lr: 6.76e-04 | norm: 0.6083 | dt: 17171.4239ms | tok/sec: 22.8994\n",
      "step  483 | train loss: 5.02 | val loss: 4.95 | perplexity: 141.22 | lr: 6.77e-04 | norm: 0.4948 | dt: 17116.8532ms | tok/sec: 22.9724\n",
      "step  484 | train loss: 4.90 | val loss: 4.96 | perplexity: 142.03 | lr: 6.78e-04 | norm: 0.4559 | dt: 17080.2844ms | tok/sec: 23.0216\n",
      "step  485 | train loss: 5.15 | val loss: 4.96 | perplexity: 142.88 | lr: 6.80e-04 | norm: 0.4773 | dt: 17082.1936ms | tok/sec: 23.0191\n",
      "step  486 | train loss: 5.35 | val loss: 4.96 | perplexity: 143.20 | lr: 6.81e-04 | norm: 0.5216 | dt: 17125.0896ms | tok/sec: 22.9614\n",
      "step  487 | train loss: 4.98 | val loss: 4.95 | perplexity: 140.90 | lr: 6.83e-04 | norm: 0.6013 | dt: 17116.1916ms | tok/sec: 22.9733\n",
      "step  488 | train loss: 5.21 | val loss: 4.94 | perplexity: 139.89 | lr: 6.84e-04 | norm: 0.3911 | dt: 17178.6225ms | tok/sec: 22.8898\n",
      "step  489 | train loss: 5.35 | val loss: 4.94 | perplexity: 139.91 | lr: 6.85e-04 | norm: 0.5031 | dt: 17110.0547ms | tok/sec: 22.9816\n",
      "step  490 | train loss: 5.39 | val loss: 4.94 | perplexity: 140.23 | lr: 6.87e-04 | norm: 0.5222 | dt: 17089.8397ms | tok/sec: 23.0088\n",
      "step  491 | train loss: 6.08 | val loss: 4.97 | perplexity: 143.36 | lr: 6.88e-04 | norm: 1.0702 | dt: 17104.6391ms | tok/sec: 22.9889\n",
      "step  492 | train loss: 5.47 | val loss: 4.96 | perplexity: 143.06 | lr: 6.90e-04 | norm: 0.9734 | dt: 17143.2617ms | tok/sec: 22.9371\n",
      "step  493 | train loss: 5.28 | val loss: 4.97 | perplexity: 143.47 | lr: 6.91e-04 | norm: 0.7288 | dt: 17148.2990ms | tok/sec: 22.9303\n",
      "step  494 | train loss: 5.23 | val loss: 4.96 | perplexity: 142.09 | lr: 6.92e-04 | norm: 0.5248 | dt: 17327.9297ms | tok/sec: 22.6926\n",
      "step  495 | train loss: 5.08 | val loss: 4.95 | perplexity: 140.54 | lr: 6.94e-04 | norm: 0.5014 | dt: 17104.7983ms | tok/sec: 22.9886\n",
      "step  496 | train loss: 5.07 | val loss: 4.94 | perplexity: 140.17 | lr: 6.95e-04 | norm: 0.4741 | dt: 17164.7875ms | tok/sec: 22.9083\n",
      "step  497 | train loss: 5.05 | val loss: 4.94 | perplexity: 139.81 | lr: 6.97e-04 | norm: 0.4871 | dt: 17133.4949ms | tok/sec: 22.9501\n",
      "step  498 | train loss: 5.43 | val loss: 4.96 | perplexity: 142.24 | lr: 6.98e-04 | norm: 0.7516 | dt: 17197.0618ms | tok/sec: 22.8653\n",
      "step  499 | train loss: 5.02 | val loss: 4.97 | perplexity: 144.62 | lr: 6.99e-04 | norm: 0.8174 | dt: 17160.6109ms | tok/sec: 22.9139\n",
      "step  500 | train loss: 5.26 | val loss: 4.99 | perplexity: 146.88 | lr: 7.01e-04 | norm: 0.6271 | dt: 17100.5123ms | tok/sec: 22.9944\n",
      "step  501 | train loss: 5.15 | val loss: 4.97 | perplexity: 144.14 | lr: 7.02e-04 | norm: 0.7954 | dt: 17134.4414ms | tok/sec: 22.9489\n",
      "step  502 | train loss: 5.26 | val loss: 4.97 | perplexity: 143.65 | lr: 7.03e-04 | norm: 0.4512 | dt: 17213.7101ms | tok/sec: 22.8432\n",
      "step  503 | train loss: 5.06 | val loss: 4.96 | perplexity: 142.53 | lr: 7.05e-04 | norm: 0.5542 | dt: 17114.5272ms | tok/sec: 22.9756\n",
      "step  504 | train loss: 5.02 | val loss: 4.95 | perplexity: 140.77 | lr: 7.06e-04 | norm: 0.5871 | dt: 17330.3673ms | tok/sec: 22.6894\n",
      "step  505 | train loss: 5.20 | val loss: 4.95 | perplexity: 141.21 | lr: 7.08e-04 | norm: 0.5376 | dt: 17168.8550ms | tok/sec: 22.9029\n",
      "step  506 | train loss: 4.89 | val loss: 4.96 | perplexity: 142.18 | lr: 7.09e-04 | norm: 0.6527 | dt: 17064.8654ms | tok/sec: 23.0424\n",
      "step  507 | train loss: 5.17 | val loss: 4.96 | perplexity: 142.31 | lr: 7.10e-04 | norm: 0.4886 | dt: 17121.2060ms | tok/sec: 22.9666\n",
      "step  508 | train loss: 5.09 | val loss: 4.95 | perplexity: 141.06 | lr: 7.12e-04 | norm: 0.4924 | dt: 17093.1840ms | tok/sec: 23.0043\n",
      "step  509 | train loss: 4.92 | val loss: 4.95 | perplexity: 140.59 | lr: 7.13e-04 | norm: 0.7164 | dt: 17326.6292ms | tok/sec: 22.6943\n",
      "step  510 | train loss: 5.20 | val loss: 4.95 | perplexity: 141.41 | lr: 7.15e-04 | norm: 0.6282 | dt: 17142.0324ms | tok/sec: 22.9387\n",
      "step  511 | train loss: 5.20 | val loss: 4.96 | perplexity: 141.89 | lr: 7.16e-04 | norm: 0.7081 | dt: 17162.2205ms | tok/sec: 22.9117\n",
      "step  512 | train loss: 5.33 | val loss: 4.95 | perplexity: 140.52 | lr: 7.17e-04 | norm: 0.6250 | dt: 17114.9275ms | tok/sec: 22.9750\n",
      "step  513 | train loss: 5.06 | val loss: 4.94 | perplexity: 140.42 | lr: 7.19e-04 | norm: 0.6372 | dt: 17125.5820ms | tok/sec: 22.9607\n",
      "step  514 | train loss: 5.21 | val loss: 4.95 | perplexity: 140.76 | lr: 7.20e-04 | norm: 0.6911 | dt: 17175.5998ms | tok/sec: 22.8939\n",
      "step  515 | train loss: 5.18 | val loss: 4.95 | perplexity: 141.23 | lr: 7.22e-04 | norm: 0.6016 | dt: 17092.6297ms | tok/sec: 23.0050\n",
      "step  516 | train loss: 4.87 | val loss: 4.93 | perplexity: 138.20 | lr: 7.23e-04 | norm: 0.6710 | dt: 17068.9499ms | tok/sec: 23.0369\n",
      "step  517 | train loss: 5.25 | val loss: 4.92 | perplexity: 137.46 | lr: 7.24e-04 | norm: 0.4990 | dt: 17142.0820ms | tok/sec: 22.9386\n",
      "step  518 | train loss: 5.07 | val loss: 4.95 | perplexity: 141.40 | lr: 7.26e-04 | norm: 0.6551 | dt: 17094.6863ms | tok/sec: 23.0022\n",
      "step  519 | train loss: 4.82 | val loss: 4.92 | perplexity: 136.65 | lr: 7.27e-04 | norm: 0.8740 | dt: 17146.8689ms | tok/sec: 22.9322\n",
      "step  520 | train loss: 4.89 | val loss: 4.93 | perplexity: 138.47 | lr: 7.29e-04 | norm: 0.4507 | dt: 17051.1022ms | tok/sec: 23.0610\n",
      "step  521 | train loss: 5.27 | val loss: 4.94 | perplexity: 140.20 | lr: 7.30e-04 | norm: 0.6000 | dt: 17175.1254ms | tok/sec: 22.8945\n",
      "step  522 | train loss: 5.06 | val loss: 4.92 | perplexity: 137.26 | lr: 7.31e-04 | norm: 0.6307 | dt: 17107.3372ms | tok/sec: 22.9852\n",
      "step  523 | train loss: 5.20 | val loss: 4.91 | perplexity: 135.32 | lr: 7.33e-04 | norm: 0.4958 | dt: 17026.3810ms | tok/sec: 23.0945\n",
      "step  524 | train loss: 4.96 | val loss: 4.91 | perplexity: 136.08 | lr: 7.34e-04 | norm: 0.4026 | dt: 17094.4049ms | tok/sec: 23.0026\n",
      "step  525 | train loss: 5.35 | val loss: 4.92 | perplexity: 137.17 | lr: 7.36e-04 | norm: 0.5736 | dt: 17052.1479ms | tok/sec: 23.0596\n",
      "step  526 | train loss: 5.20 | val loss: 4.93 | perplexity: 138.24 | lr: 7.37e-04 | norm: 0.4605 | dt: 17131.8955ms | tok/sec: 22.9523\n",
      "step  527 | train loss: 5.07 | val loss: 4.91 | perplexity: 136.18 | lr: 7.38e-04 | norm: 0.6182 | dt: 17142.8492ms | tok/sec: 22.9376\n",
      "step  528 | train loss: 5.09 | val loss: 4.91 | perplexity: 135.22 | lr: 7.40e-04 | norm: 0.4349 | dt: 17139.9169ms | tok/sec: 22.9415\n",
      "step  529 | train loss: 5.04 | val loss: 4.90 | perplexity: 134.93 | lr: 7.41e-04 | norm: 0.4575 | dt: 17071.6386ms | tok/sec: 23.0333\n",
      "step  530 | train loss: 4.94 | val loss: 4.91 | perplexity: 134.98 | lr: 7.43e-04 | norm: 0.4950 | dt: 17183.4643ms | tok/sec: 22.8834\n",
      "step  531 | train loss: 5.02 | val loss: 4.90 | perplexity: 134.70 | lr: 7.44e-04 | norm: 0.4466 | dt: 17117.1796ms | tok/sec: 22.9720\n",
      "step  532 | train loss: 5.23 | val loss: 4.91 | perplexity: 134.96 | lr: 7.45e-04 | norm: 0.3947 | dt: 17092.4320ms | tok/sec: 23.0053\n",
      "step  533 | train loss: 5.08 | val loss: 4.90 | perplexity: 133.84 | lr: 7.47e-04 | norm: 0.4776 | dt: 17103.8852ms | tok/sec: 22.9899\n",
      "step  534 | train loss: 5.08 | val loss: 4.90 | perplexity: 133.94 | lr: 7.48e-04 | norm: 0.4158 | dt: 17237.5839ms | tok/sec: 22.8115\n",
      "step  535 | train loss: 5.15 | val loss: 4.90 | perplexity: 134.58 | lr: 7.50e-04 | norm: 0.4456 | dt: 17086.2675ms | tok/sec: 23.0136\n",
      "step  536 | train loss: 5.17 | val loss: 4.90 | perplexity: 134.86 | lr: 7.51e-04 | norm: 0.6562 | dt: 17161.2241ms | tok/sec: 22.9131\n",
      "step  537 | train loss: 5.14 | val loss: 4.90 | perplexity: 134.33 | lr: 7.52e-04 | norm: 0.4942 | dt: 17068.7809ms | tok/sec: 23.0371\n",
      "step  538 | train loss: 4.98 | val loss: 4.91 | perplexity: 135.22 | lr: 7.54e-04 | norm: 0.5160 | dt: 17107.4064ms | tok/sec: 22.9851\n",
      "step  539 | train loss: 5.19 | val loss: 4.90 | perplexity: 134.96 | lr: 7.55e-04 | norm: 0.4790 | dt: 17130.6705ms | tok/sec: 22.9539\n",
      "step  540 | train loss: 5.18 | val loss: 4.90 | perplexity: 134.66 | lr: 7.57e-04 | norm: 0.5521 | dt: 17110.3492ms | tok/sec: 22.9812\n",
      "step  541 | train loss: 4.84 | val loss: 4.90 | perplexity: 134.50 | lr: 7.58e-04 | norm: 0.4284 | dt: 17125.2036ms | tok/sec: 22.9612\n",
      "step  542 | train loss: 4.88 | val loss: 4.90 | perplexity: 134.52 | lr: 7.59e-04 | norm: 0.6286 | dt: 17096.8900ms | tok/sec: 22.9993\n",
      "step  543 | train loss: 4.93 | val loss: 4.90 | perplexity: 134.07 | lr: 7.61e-04 | norm: 0.5889 | dt: 17100.4255ms | tok/sec: 22.9945\n",
      "step  544 | train loss: 4.94 | val loss: 4.90 | perplexity: 133.81 | lr: 7.62e-04 | norm: 0.6095 | dt: 17111.3291ms | tok/sec: 22.9799\n",
      "step  545 | train loss: 4.98 | val loss: 4.89 | perplexity: 133.04 | lr: 7.64e-04 | norm: 0.5230 | dt: 17062.7904ms | tok/sec: 23.0452\n",
      "step  546 | train loss: 5.00 | val loss: 4.89 | perplexity: 132.82 | lr: 7.65e-04 | norm: 0.4685 | dt: 17114.7356ms | tok/sec: 22.9753\n",
      "step  547 | train loss: 5.03 | val loss: 4.89 | perplexity: 133.41 | lr: 7.66e-04 | norm: 0.5345 | dt: 17076.3831ms | tok/sec: 23.0269\n",
      "step  548 | train loss: 5.06 | val loss: 4.89 | perplexity: 133.56 | lr: 7.68e-04 | norm: 0.7590 | dt: 17126.7085ms | tok/sec: 22.9592\n",
      "step  549 | train loss: 5.31 | val loss: 4.90 | perplexity: 134.78 | lr: 7.69e-04 | norm: 0.5256 | dt: 17145.4964ms | tok/sec: 22.9341\n",
      "step  550 | train loss: 5.13 | val loss: 4.89 | perplexity: 132.76 | lr: 7.71e-04 | norm: 0.6689 | dt: 17108.8083ms | tok/sec: 22.9832\n",
      "step  551 | train loss: 5.30 | val loss: 4.90 | perplexity: 134.35 | lr: 7.72e-04 | norm: 0.5069 | dt: 17064.3086ms | tok/sec: 23.0432\n",
      "step  552 | train loss: 5.12 | val loss: 4.91 | perplexity: 135.46 | lr: 7.73e-04 | norm: 0.6449 | dt: 17238.7838ms | tok/sec: 22.8100\n",
      "step  553 | train loss: 4.89 | val loss: 4.91 | perplexity: 135.90 | lr: 7.75e-04 | norm: 0.5631 | dt: 17296.4218ms | tok/sec: 22.7340\n",
      "step  554 | train loss: 5.37 | val loss: 4.91 | perplexity: 135.28 | lr: 7.76e-04 | norm: 0.5464 | dt: 17231.5114ms | tok/sec: 22.8196\n",
      "step  555 | train loss: 4.90 | val loss: 4.90 | perplexity: 134.41 | lr: 7.78e-04 | norm: 0.4322 | dt: 17107.3260ms | tok/sec: 22.9852\n",
      "step  556 | train loss: 5.20 | val loss: 4.89 | perplexity: 132.98 | lr: 7.79e-04 | norm: 0.4976 | dt: 17091.1362ms | tok/sec: 23.0070\n",
      "step  557 | train loss: 5.21 | val loss: 4.89 | perplexity: 133.19 | lr: 7.80e-04 | norm: 0.4710 | dt: 17125.4365ms | tok/sec: 22.9609\n",
      "step  558 | train loss: 5.25 | val loss: 4.89 | perplexity: 133.07 | lr: 7.82e-04 | norm: 0.4281 | dt: 17086.6063ms | tok/sec: 23.0131\n",
      "step  559 | train loss: 5.24 | val loss: 4.91 | perplexity: 135.14 | lr: 7.83e-04 | norm: 0.4908 | dt: 17098.2416ms | tok/sec: 22.9975\n",
      "step  560 | train loss: 5.16 | val loss: 4.90 | perplexity: 134.49 | lr: 7.85e-04 | norm: 0.5423 | dt: 17125.0179ms | tok/sec: 22.9615\n",
      "step  561 | train loss: 5.06 | val loss: 4.89 | perplexity: 132.60 | lr: 7.86e-04 | norm: 0.4883 | dt: 17114.4071ms | tok/sec: 22.9757\n",
      "step  562 | train loss: 5.04 | val loss: 4.87 | perplexity: 130.67 | lr: 7.87e-04 | norm: 0.6336 | dt: 17092.4671ms | tok/sec: 23.0052\n",
      "step  563 | train loss: 5.03 | val loss: 4.88 | perplexity: 131.21 | lr: 7.89e-04 | norm: 0.5042 | dt: 17188.9431ms | tok/sec: 22.8761\n",
      "step  564 | train loss: 5.01 | val loss: 4.88 | perplexity: 131.81 | lr: 7.90e-04 | norm: 0.5381 | dt: 17087.6245ms | tok/sec: 23.0117\n",
      "step  565 | train loss: 5.32 | val loss: 4.87 | perplexity: 130.72 | lr: 7.92e-04 | norm: 0.5286 | dt: 17084.3492ms | tok/sec: 23.0162\n",
      "step  566 | train loss: 5.14 | val loss: 4.88 | perplexity: 131.18 | lr: 7.93e-04 | norm: 0.4259 | dt: 17107.6939ms | tok/sec: 22.9847\n",
      "step  567 | train loss: 5.17 | val loss: 4.88 | perplexity: 131.03 | lr: 7.94e-04 | norm: 0.5492 | dt: 17107.2419ms | tok/sec: 22.9854\n",
      "step  568 | train loss: 5.34 | val loss: 4.88 | perplexity: 131.45 | lr: 7.96e-04 | norm: 0.5501 | dt: 17121.6133ms | tok/sec: 22.9661\n",
      "step  569 | train loss: 5.22 | val loss: 4.88 | perplexity: 131.61 | lr: 7.97e-04 | norm: 0.6221 | dt: 17241.2004ms | tok/sec: 22.8068\n",
      "step  570 | train loss: 5.12 | val loss: 4.88 | perplexity: 131.22 | lr: 7.99e-04 | norm: 0.7122 | dt: 17099.2382ms | tok/sec: 22.9961\n",
      "step  571 | train loss: 5.16 | val loss: 4.88 | perplexity: 131.93 | lr: 8.00e-04 | norm: 0.6464 | dt: 17118.3951ms | tok/sec: 22.9704\n",
      "step  572 | train loss: 5.02 | val loss: 4.87 | perplexity: 130.51 | lr: 8.01e-04 | norm: 0.7143 | dt: 17095.0394ms | tok/sec: 23.0018\n",
      "step  573 | train loss: 5.22 | val loss: 4.87 | perplexity: 130.28 | lr: 8.03e-04 | norm: 0.5061 | dt: 17097.2545ms | tok/sec: 22.9988\n",
      "step  574 | train loss: 5.16 | val loss: 4.87 | perplexity: 130.97 | lr: 8.04e-04 | norm: 0.4705 | dt: 17076.1638ms | tok/sec: 23.0272\n",
      "step  575 | train loss: 5.31 | val loss: 4.88 | perplexity: 131.08 | lr: 8.06e-04 | norm: 0.4942 | dt: 17098.2497ms | tok/sec: 22.9974\n",
      "step  576 | train loss: 5.03 | val loss: 4.87 | perplexity: 129.75 | lr: 8.07e-04 | norm: 0.4982 | dt: 17140.3365ms | tok/sec: 22.9410\n",
      "step  577 | train loss: 5.06 | val loss: 4.89 | perplexity: 132.87 | lr: 8.08e-04 | norm: 0.7490 | dt: 17254.5018ms | tok/sec: 22.7892\n",
      "step  578 | train loss: 4.94 | val loss: 4.88 | perplexity: 131.77 | lr: 8.10e-04 | norm: 0.7387 | dt: 17085.9714ms | tok/sec: 23.0140\n",
      "step  579 | train loss: 4.94 | val loss: 4.89 | perplexity: 132.83 | lr: 8.11e-04 | norm: 0.5006 | dt: 17062.9697ms | tok/sec: 23.0450\n",
      "step  580 | train loss: 4.77 | val loss: 4.87 | perplexity: 130.34 | lr: 8.13e-04 | norm: 0.5968 | dt: 17075.2470ms | tok/sec: 23.0284\n",
      "step  581 | train loss: 4.91 | val loss: 4.87 | perplexity: 130.20 | lr: 8.14e-04 | norm: 0.5702 | dt: 17123.4057ms | tok/sec: 22.9637\n",
      "step  582 | train loss: 5.16 | val loss: 4.86 | perplexity: 128.62 | lr: 8.15e-04 | norm: 0.5084 | dt: 17081.6400ms | tok/sec: 23.0198\n",
      "step  583 | train loss: 4.96 | val loss: 4.86 | perplexity: 128.86 | lr: 8.17e-04 | norm: 0.4009 | dt: 17078.7694ms | tok/sec: 23.0237\n",
      "step  584 | train loss: 5.04 | val loss: 4.86 | perplexity: 128.82 | lr: 8.18e-04 | norm: 0.4170 | dt: 17091.0213ms | tok/sec: 23.0072\n",
      "step  585 | train loss: 5.35 | val loss: 4.86 | perplexity: 129.18 | lr: 8.20e-04 | norm: 0.3977 | dt: 17135.3321ms | tok/sec: 22.9477\n",
      "step  586 | train loss: 4.82 | val loss: 4.85 | perplexity: 128.25 | lr: 8.21e-04 | norm: 0.5034 | dt: 17131.6547ms | tok/sec: 22.9526\n",
      "step  587 | train loss: 5.19 | val loss: 4.87 | perplexity: 130.50 | lr: 8.22e-04 | norm: 0.5569 | dt: 17090.5194ms | tok/sec: 23.0078\n",
      "step  588 | train loss: 4.73 | val loss: 4.85 | perplexity: 127.43 | lr: 8.24e-04 | norm: 0.6634 | dt: 17147.5551ms | tok/sec: 22.9313\n",
      "step  589 | train loss: 5.13 | val loss: 4.85 | perplexity: 127.75 | lr: 8.25e-04 | norm: 0.3906 | dt: 17118.9668ms | tok/sec: 22.9696\n",
      "step  590 | train loss: 5.02 | val loss: 4.84 | perplexity: 126.95 | lr: 8.27e-04 | norm: 0.5197 | dt: 17113.3657ms | tok/sec: 22.9771\n",
      "step  591 | train loss: 5.00 | val loss: 4.85 | perplexity: 127.45 | lr: 8.28e-04 | norm: 0.4435 | dt: 17180.1019ms | tok/sec: 22.8879\n",
      "step  592 | train loss: 5.24 | val loss: 4.84 | perplexity: 126.47 | lr: 8.29e-04 | norm: 0.5820 | dt: 17130.3198ms | tok/sec: 22.9544\n",
      "step  593 | train loss: 5.05 | val loss: 4.84 | perplexity: 126.08 | lr: 8.31e-04 | norm: 0.4339 | dt: 17125.1941ms | tok/sec: 22.9613\n",
      "step  594 | train loss: 5.06 | val loss: 4.83 | perplexity: 125.46 | lr: 8.32e-04 | norm: 0.5785 | dt: 17161.3317ms | tok/sec: 22.9129\n",
      "step  595 | train loss: 5.22 | val loss: 4.84 | perplexity: 126.40 | lr: 8.34e-04 | norm: 0.4976 | dt: 17075.6891ms | tok/sec: 23.0278\n",
      "step  596 | train loss: 4.98 | val loss: 4.85 | perplexity: 127.24 | lr: 8.35e-04 | norm: 0.4985 | dt: 17084.1351ms | tok/sec: 23.0164\n",
      "step  597 | train loss: 5.07 | val loss: 4.85 | perplexity: 127.55 | lr: 8.36e-04 | norm: 0.5420 | dt: 17074.2204ms | tok/sec: 23.0298\n",
      "step  598 | train loss: 4.98 | val loss: 4.84 | perplexity: 126.43 | lr: 8.38e-04 | norm: 0.3995 | dt: 17052.6240ms | tok/sec: 23.0590\n",
      "step  599 | train loss: 4.99 | val loss: 4.84 | perplexity: 126.39 | lr: 8.39e-04 | norm: 0.5286 | dt: 17094.7001ms | tok/sec: 23.0022\n",
      "step  600 | train loss: 5.14 | val loss: 4.83 | perplexity: 125.15 | lr: 8.41e-04 | norm: 0.5649 | dt: 17579.8914ms | tok/sec: 22.3674\n",
      "step  601 | train loss: 4.99 | val loss: 4.83 | perplexity: 125.76 | lr: 8.42e-04 | norm: 0.4577 | dt: 17432.1167ms | tok/sec: 22.5570\n",
      "step  602 | train loss: 4.75 | val loss: 4.83 | perplexity: 125.32 | lr: 8.43e-04 | norm: 0.6959 | dt: 17134.7761ms | tok/sec: 22.9484\n",
      "step  603 | train loss: 4.95 | val loss: 4.85 | perplexity: 127.23 | lr: 8.45e-04 | norm: 0.6235 | dt: 17123.0047ms | tok/sec: 22.9642\n",
      "step  604 | train loss: 4.97 | val loss: 4.83 | perplexity: 125.76 | lr: 8.46e-04 | norm: 0.7170 | dt: 17080.7669ms | tok/sec: 23.0210\n",
      "step  605 | train loss: 5.04 | val loss: 4.83 | perplexity: 125.43 | lr: 8.48e-04 | norm: 0.5305 | dt: 17080.9183ms | tok/sec: 23.0208\n",
      "step  606 | train loss: 5.20 | val loss: 4.83 | perplexity: 125.17 | lr: 8.49e-04 | norm: 0.4719 | dt: 17057.4958ms | tok/sec: 23.0524\n",
      "step  607 | train loss: 5.06 | val loss: 4.82 | perplexity: 124.16 | lr: 8.50e-04 | norm: 0.5467 | dt: 17103.5626ms | tok/sec: 22.9903\n",
      "step  608 | train loss: 5.09 | val loss: 4.82 | perplexity: 123.75 | lr: 8.52e-04 | norm: 0.4695 | dt: 17097.4243ms | tok/sec: 22.9986\n",
      "step  609 | train loss: 5.13 | val loss: 4.83 | perplexity: 125.17 | lr: 8.53e-04 | norm: 0.4023 | dt: 17122.9208ms | tok/sec: 22.9643\n",
      "step  610 | train loss: 4.95 | val loss: 4.82 | perplexity: 123.46 | lr: 8.55e-04 | norm: 0.4609 | dt: 17175.1027ms | tok/sec: 22.8945\n",
      "step  611 | train loss: 5.20 | val loss: 4.82 | perplexity: 123.41 | lr: 8.56e-04 | norm: 0.4597 | dt: 17106.2438ms | tok/sec: 22.9867\n",
      "step  612 | train loss: 4.70 | val loss: 4.80 | perplexity: 121.86 | lr: 8.57e-04 | norm: 0.4544 | dt: 17168.0133ms | tok/sec: 22.9040\n",
      "step  613 | train loss: 5.11 | val loss: 4.81 | perplexity: 123.10 | lr: 8.59e-04 | norm: 0.5776 | dt: 17101.1038ms | tok/sec: 22.9936\n",
      "step  614 | train loss: 5.17 | val loss: 4.81 | perplexity: 122.63 | lr: 8.60e-04 | norm: 0.6454 | dt: 17157.0385ms | tok/sec: 22.9186\n",
      "step  615 | train loss: 4.85 | val loss: 4.82 | perplexity: 123.36 | lr: 8.62e-04 | norm: 0.4605 | dt: 17114.6972ms | tok/sec: 22.9753\n",
      "step  616 | train loss: 5.08 | val loss: 4.81 | perplexity: 122.12 | lr: 8.63e-04 | norm: 0.5314 | dt: 17197.7935ms | tok/sec: 22.8643\n",
      "step  617 | train loss: 5.18 | val loss: 4.80 | perplexity: 121.91 | lr: 8.64e-04 | norm: 0.4329 | dt: 17157.7499ms | tok/sec: 22.9177\n",
      "step  618 | train loss: 4.80 | val loss: 4.81 | perplexity: 122.51 | lr: 8.66e-04 | norm: 0.5423 | dt: 17164.4332ms | tok/sec: 22.9088\n",
      "step  619 | train loss: 4.85 | val loss: 4.81 | perplexity: 122.75 | lr: 8.67e-04 | norm: 0.7054 | dt: 17055.3408ms | tok/sec: 23.0553\n",
      "step  620 | train loss: 4.94 | val loss: 4.82 | perplexity: 124.16 | lr: 8.69e-04 | norm: 0.5277 | dt: 17150.6464ms | tok/sec: 22.9272\n",
      "step  621 | train loss: 5.09 | val loss: 4.82 | perplexity: 123.47 | lr: 8.70e-04 | norm: 0.4766 | dt: 17180.5685ms | tok/sec: 22.8873\n",
      "step  622 | train loss: 4.89 | val loss: 4.82 | perplexity: 124.01 | lr: 8.71e-04 | norm: 0.4277 | dt: 17158.7150ms | tok/sec: 22.9164\n",
      "step  623 | train loss: 4.99 | val loss: 4.81 | perplexity: 123.03 | lr: 8.73e-04 | norm: 0.5363 | dt: 17084.2173ms | tok/sec: 23.0163\n",
      "step  624 | train loss: 5.08 | val loss: 4.83 | perplexity: 125.55 | lr: 8.74e-04 | norm: 0.4845 | dt: 17102.0021ms | tok/sec: 22.9924\n",
      "step  625 | train loss: 5.17 | val loss: 4.84 | perplexity: 126.23 | lr: 8.76e-04 | norm: 0.7539 | dt: 17123.5476ms | tok/sec: 22.9635\n",
      "step  626 | train loss: 4.85 | val loss: 4.83 | perplexity: 125.27 | lr: 8.77e-04 | norm: 0.8618 | dt: 17136.8012ms | tok/sec: 22.9457\n",
      "step  627 | train loss: 5.11 | val loss: 4.85 | perplexity: 127.53 | lr: 8.78e-04 | norm: 0.5938 | dt: 17176.6777ms | tok/sec: 22.8924\n",
      "step  628 | train loss: 5.01 | val loss: 4.83 | perplexity: 124.73 | lr: 8.80e-04 | norm: 0.5696 | dt: 17160.0091ms | tok/sec: 22.9147\n",
      "step  629 | train loss: 4.93 | val loss: 4.83 | perplexity: 124.95 | lr: 8.81e-04 | norm: 0.5467 | dt: 17073.6089ms | tok/sec: 23.0306\n",
      "step  630 | train loss: 5.03 | val loss: 4.85 | perplexity: 127.24 | lr: 8.83e-04 | norm: 0.6495 | dt: 17094.8052ms | tok/sec: 23.0021\n",
      "step  631 | train loss: 4.96 | val loss: 4.84 | perplexity: 126.85 | lr: 8.84e-04 | norm: 0.5806 | dt: 17060.7264ms | tok/sec: 23.0480\n",
      "step  632 | train loss: 4.83 | val loss: 4.82 | perplexity: 123.79 | lr: 8.85e-04 | norm: 0.5721 | dt: 17078.1624ms | tok/sec: 23.0245\n",
      "step  633 | train loss: 4.81 | val loss: 4.81 | perplexity: 122.30 | lr: 8.87e-04 | norm: 0.6027 | dt: 17121.9113ms | tok/sec: 22.9657\n",
      "step  634 | train loss: 4.75 | val loss: 4.81 | perplexity: 122.79 | lr: 8.88e-04 | norm: 0.4357 | dt: 17124.5861ms | tok/sec: 22.9621\n",
      "step  635 | train loss: 5.17 | val loss: 4.82 | perplexity: 123.43 | lr: 8.90e-04 | norm: 0.4652 | dt: 17074.7528ms | tok/sec: 23.0291\n",
      "step  636 | train loss: 5.25 | val loss: 4.82 | perplexity: 123.78 | lr: 8.91e-04 | norm: 0.4802 | dt: 17130.4913ms | tok/sec: 22.9542\n",
      "step  637 | train loss: 5.18 | val loss: 4.82 | perplexity: 123.72 | lr: 8.92e-04 | norm: 0.4387 | dt: 17085.3214ms | tok/sec: 23.0148\n",
      "step  638 | train loss: 4.98 | val loss: 4.81 | perplexity: 122.53 | lr: 8.94e-04 | norm: 0.4946 | dt: 17059.7124ms | tok/sec: 23.0494\n",
      "step  639 | train loss: 4.94 | val loss: 4.80 | perplexity: 121.47 | lr: 8.95e-04 | norm: 0.4543 | dt: 17155.7264ms | tok/sec: 22.9204\n",
      "step  640 | train loss: 4.97 | val loss: 4.79 | perplexity: 120.25 | lr: 8.97e-04 | norm: 0.4445 | dt: 17108.3429ms | tok/sec: 22.9839\n",
      "step  641 | train loss: 4.98 | val loss: 4.78 | perplexity: 119.27 | lr: 8.98e-04 | norm: 0.4196 | dt: 17120.5044ms | tok/sec: 22.9675\n",
      "step  642 | train loss: 4.96 | val loss: 4.79 | perplexity: 120.84 | lr: 8.99e-04 | norm: 0.5294 | dt: 17089.9310ms | tok/sec: 23.0086\n",
      "step  643 | train loss: 5.03 | val loss: 4.80 | perplexity: 121.83 | lr: 9.01e-04 | norm: 0.7278 | dt: 17051.5385ms | tok/sec: 23.0604\n",
      "step  644 | train loss: 5.19 | val loss: 4.80 | perplexity: 121.09 | lr: 9.02e-04 | norm: 0.8859 | dt: 17079.2744ms | tok/sec: 23.0230\n",
      "step  645 | train loss: 4.94 | val loss: 4.80 | perplexity: 121.53 | lr: 9.03e-04 | norm: 0.4417 | dt: 17122.5159ms | tok/sec: 22.9648\n",
      "step  646 | train loss: 4.67 | val loss: 4.82 | perplexity: 123.58 | lr: 9.05e-04 | norm: 0.7033 | dt: 17072.8800ms | tok/sec: 23.0316\n",
      "step  647 | train loss: 4.79 | val loss: 4.81 | perplexity: 122.58 | lr: 9.06e-04 | norm: 0.6001 | dt: 17078.8391ms | tok/sec: 23.0236\n",
      "step  648 | train loss: 4.84 | val loss: 4.80 | perplexity: 121.15 | lr: 9.08e-04 | norm: 0.5430 | dt: 17044.2257ms | tok/sec: 23.0703\n",
      "step  649 | train loss: 4.84 | val loss: 4.79 | perplexity: 120.00 | lr: 9.09e-04 | norm: 0.5585 | dt: 17032.7728ms | tok/sec: 23.0858\n",
      "step  650 | train loss: 4.87 | val loss: 4.79 | perplexity: 119.74 | lr: 9.10e-04 | norm: 0.4953 | dt: 17087.3835ms | tok/sec: 23.0121\n",
      "step  651 | train loss: 4.98 | val loss: 4.80 | perplexity: 121.47 | lr: 9.12e-04 | norm: 0.5494 | dt: 17052.0818ms | tok/sec: 23.0597\n",
      "step  652 | train loss: 4.84 | val loss: 4.80 | perplexity: 121.67 | lr: 9.13e-04 | norm: 0.6081 | dt: 17153.3990ms | tok/sec: 22.9235\n",
      "step  653 | train loss: 4.92 | val loss: 4.79 | perplexity: 120.58 | lr: 9.15e-04 | norm: 0.6276 | dt: 17059.7742ms | tok/sec: 23.0493\n",
      "step  654 | train loss: 5.17 | val loss: 4.80 | perplexity: 121.76 | lr: 9.16e-04 | norm: 0.6828 | dt: 17128.3114ms | tok/sec: 22.9571\n",
      "step  655 | train loss: 4.95 | val loss: 4.79 | perplexity: 120.03 | lr: 9.17e-04 | norm: 0.6924 | dt: 17077.7802ms | tok/sec: 23.0250\n",
      "step  656 | train loss: 4.84 | val loss: 4.80 | perplexity: 121.90 | lr: 9.19e-04 | norm: 0.5149 | dt: 17079.1054ms | tok/sec: 23.0232\n",
      "step  657 | train loss: 4.80 | val loss: 4.77 | perplexity: 118.39 | lr: 9.20e-04 | norm: 0.7074 | dt: 17060.9069ms | tok/sec: 23.0478\n",
      "step  658 | train loss: 4.96 | val loss: 4.78 | perplexity: 118.69 | lr: 9.22e-04 | norm: 0.5628 | dt: 17074.0249ms | tok/sec: 23.0301\n",
      "step  659 | train loss: 4.97 | val loss: 4.77 | perplexity: 117.52 | lr: 9.23e-04 | norm: 0.5664 | dt: 17608.7041ms | tok/sec: 22.3308\n",
      "step  660 | train loss: 4.58 | val loss: 4.80 | perplexity: 121.66 | lr: 9.24e-04 | norm: 1.0665 | dt: 17079.3858ms | tok/sec: 23.0228\n",
      "step  661 | train loss: 4.70 | val loss: 4.79 | perplexity: 120.61 | lr: 9.26e-04 | norm: 0.7206 | dt: 17079.1655ms | tok/sec: 23.0231\n",
      "step  662 | train loss: 4.77 | val loss: 4.78 | perplexity: 118.78 | lr: 9.27e-04 | norm: 0.9745 | dt: 17075.0816ms | tok/sec: 23.0286\n",
      "step  663 | train loss: 4.72 | val loss: 4.77 | perplexity: 118.38 | lr: 9.29e-04 | norm: 0.6568 | dt: 17066.9749ms | tok/sec: 23.0396\n",
      "step  664 | train loss: 5.17 | val loss: 4.79 | perplexity: 120.62 | lr: 9.30e-04 | norm: 0.7213 | dt: 17088.7692ms | tok/sec: 23.0102\n",
      "step  665 | train loss: 5.03 | val loss: 4.78 | perplexity: 119.59 | lr: 9.31e-04 | norm: 0.6761 | dt: 17095.7825ms | tok/sec: 23.0008\n",
      "step  666 | train loss: 5.17 | val loss: 4.80 | perplexity: 121.14 | lr: 9.33e-04 | norm: 0.5691 | dt: 17058.3930ms | tok/sec: 23.0512\n",
      "step  667 | train loss: 4.85 | val loss: 4.80 | perplexity: 121.35 | lr: 9.34e-04 | norm: 0.7340 | dt: 17125.8757ms | tok/sec: 22.9603\n",
      "step  668 | train loss: 5.04 | val loss: 4.79 | perplexity: 119.77 | lr: 9.36e-04 | norm: 0.6353 | dt: 17132.6520ms | tok/sec: 22.9513\n",
      "step  669 | train loss: 4.95 | val loss: 4.78 | perplexity: 118.53 | lr: 9.37e-04 | norm: 0.4854 | dt: 17142.3237ms | tok/sec: 22.9383\n",
      "step  670 | train loss: 5.26 | val loss: 4.79 | perplexity: 120.56 | lr: 9.38e-04 | norm: 0.7659 | dt: 17105.3777ms | tok/sec: 22.9879\n",
      "step  671 | train loss: 4.98 | val loss: 4.78 | perplexity: 118.53 | lr: 9.40e-04 | norm: 0.7053 | dt: 17123.2033ms | tok/sec: 22.9639\n",
      "step  672 | train loss: 4.95 | val loss: 4.76 | perplexity: 116.70 | lr: 9.41e-04 | norm: 0.5442 | dt: 17119.0159ms | tok/sec: 22.9695\n",
      "step  673 | train loss: 4.94 | val loss: 4.77 | perplexity: 118.03 | lr: 9.43e-04 | norm: 0.4567 | dt: 17073.0679ms | tok/sec: 23.0314\n",
      "step  674 | train loss: 4.99 | val loss: 4.78 | perplexity: 119.04 | lr: 9.44e-04 | norm: 0.5551 | dt: 17145.2639ms | tok/sec: 22.9344\n",
      "step  675 | train loss: 4.98 | val loss: 4.77 | perplexity: 117.49 | lr: 9.45e-04 | norm: 0.5626 | dt: 17192.7199ms | tok/sec: 22.8711\n",
      "step  676 | train loss: 5.14 | val loss: 4.75 | perplexity: 115.99 | lr: 9.47e-04 | norm: 0.5420 | dt: 17098.3067ms | tok/sec: 22.9974\n",
      "step  677 | train loss: 5.10 | val loss: 4.75 | perplexity: 115.53 | lr: 9.48e-04 | norm: 0.4555 | dt: 17068.4454ms | tok/sec: 23.0376\n",
      "step  678 | train loss: 4.77 | val loss: 4.73 | perplexity: 113.62 | lr: 9.50e-04 | norm: 0.6449 | dt: 17103.3623ms | tok/sec: 22.9906\n",
      "step  679 | train loss: 4.93 | val loss: 4.75 | perplexity: 115.05 | lr: 9.51e-04 | norm: 0.6685 | dt: 17083.2996ms | tok/sec: 23.0176\n",
      "step  680 | train loss: 4.98 | val loss: 4.75 | perplexity: 115.53 | lr: 9.52e-04 | norm: 0.6352 | dt: 17093.5020ms | tok/sec: 23.0038\n",
      "step  681 | train loss: 4.86 | val loss: 4.76 | perplexity: 116.59 | lr: 9.54e-04 | norm: 0.5557 | dt: 17092.7033ms | tok/sec: 23.0049\n",
      "step  682 | train loss: 4.84 | val loss: 4.74 | perplexity: 114.08 | lr: 9.55e-04 | norm: 0.8545 | dt: 17071.3487ms | tok/sec: 23.0337\n",
      "step  683 | train loss: 4.87 | val loss: 4.73 | perplexity: 113.18 | lr: 9.57e-04 | norm: 0.6391 | dt: 17096.6713ms | tok/sec: 22.9996\n",
      "step  684 | train loss: 4.74 | val loss: 4.76 | perplexity: 116.58 | lr: 9.58e-04 | norm: 0.7483 | dt: 17089.0937ms | tok/sec: 23.0098\n",
      "step  685 | train loss: 5.59 | val loss: 4.76 | perplexity: 116.45 | lr: 9.59e-04 | norm: 0.8436 | dt: 17152.7450ms | tok/sec: 22.9244\n",
      "step  686 | train loss: 4.77 | val loss: 4.73 | perplexity: 113.64 | lr: 9.61e-04 | norm: 0.8686 | dt: 17087.9943ms | tok/sec: 23.0112\n",
      "step  687 | train loss: 4.81 | val loss: 4.75 | perplexity: 115.31 | lr: 9.62e-04 | norm: 0.9164 | dt: 17109.6163ms | tok/sec: 22.9822\n",
      "step  688 | train loss: 4.69 | val loss: 4.72 | perplexity: 111.94 | lr: 9.64e-04 | norm: 0.9824 | dt: 17059.0684ms | tok/sec: 23.0503\n",
      "step  689 | train loss: 4.79 | val loss: 4.73 | perplexity: 113.08 | lr: 9.65e-04 | norm: 0.5595 | dt: 17106.2074ms | tok/sec: 22.9867\n",
      "step  690 | train loss: 4.93 | val loss: 4.75 | perplexity: 115.11 | lr: 9.66e-04 | norm: 0.8023 | dt: 17090.3196ms | tok/sec: 23.0081\n",
      "step  691 | train loss: 4.73 | val loss: 4.76 | perplexity: 116.56 | lr: 9.68e-04 | norm: 0.8570 | dt: 17185.2345ms | tok/sec: 22.8810\n",
      "step  692 | train loss: 4.54 | val loss: 4.75 | perplexity: 116.09 | lr: 9.69e-04 | norm: 1.0317 | dt: 17122.7245ms | tok/sec: 22.9646\n",
      "step  693 | train loss: 4.69 | val loss: 4.75 | perplexity: 115.21 | lr: 9.71e-04 | norm: 0.6901 | dt: 17116.8137ms | tok/sec: 22.9725\n",
      "step  694 | train loss: 4.54 | val loss: 4.74 | perplexity: 114.23 | lr: 9.72e-04 | norm: 0.7274 | dt: 17119.5669ms | tok/sec: 22.9688\n",
      "step  695 | train loss: 5.60 | val loss: 4.75 | perplexity: 115.85 | lr: 9.73e-04 | norm: 0.9281 | dt: 17114.9952ms | tok/sec: 22.9749\n",
      "step  696 | train loss: 5.17 | val loss: 4.76 | perplexity: 116.42 | lr: 9.75e-04 | norm: 0.7300 | dt: 17048.7361ms | tok/sec: 23.0642\n",
      "step  697 | train loss: 4.67 | val loss: 4.74 | perplexity: 114.75 | lr: 9.76e-04 | norm: 0.9280 | dt: 17123.6284ms | tok/sec: 22.9634\n",
      "step  698 | train loss: 4.58 | val loss: 4.72 | perplexity: 112.30 | lr: 9.78e-04 | norm: 0.6924 | dt: 17145.6509ms | tok/sec: 22.9339\n",
      "step  699 | train loss: 4.51 | val loss: 4.72 | perplexity: 112.15 | lr: 9.79e-04 | norm: 0.6384 | dt: 17094.6693ms | tok/sec: 23.0023\n",
      "step  700 | train loss: 4.81 | val loss: 4.72 | perplexity: 112.41 | lr: 9.80e-04 | norm: 0.5418 | dt: 17120.6903ms | tok/sec: 22.9673\n",
      "step  701 | train loss: 4.96 | val loss: 4.73 | perplexity: 113.22 | lr: 9.82e-04 | norm: 0.5375 | dt: 17072.0882ms | tok/sec: 23.0327\n",
      "step  702 | train loss: 4.94 | val loss: 4.71 | perplexity: 110.52 | lr: 9.83e-04 | norm: 0.7613 | dt: 17106.3137ms | tok/sec: 22.9866\n",
      "step  703 | train loss: 4.58 | val loss: 4.71 | perplexity: 110.52 | lr: 9.85e-04 | norm: 0.6175 | dt: 17093.2686ms | tok/sec: 23.0041\n",
      "step  704 | train loss: 4.81 | val loss: 4.73 | perplexity: 113.28 | lr: 9.86e-04 | norm: 0.7944 | dt: 17066.2096ms | tok/sec: 23.0406\n",
      "step  705 | train loss: 4.57 | val loss: 4.73 | perplexity: 112.84 | lr: 9.87e-04 | norm: 0.7851 | dt: 17185.3595ms | tok/sec: 22.8809\n",
      "step  706 | train loss: 4.87 | val loss: 4.71 | perplexity: 110.67 | lr: 9.89e-04 | norm: 0.8705 | dt: 17116.9198ms | tok/sec: 22.9724\n",
      "step  707 | train loss: 4.82 | val loss: 4.71 | perplexity: 110.54 | lr: 9.90e-04 | norm: 0.6644 | dt: 17151.6025ms | tok/sec: 22.9259\n",
      "step  708 | train loss: 4.73 | val loss: 4.69 | perplexity: 108.77 | lr: 9.92e-04 | norm: 0.7184 | dt: 17128.3462ms | tok/sec: 22.9570\n",
      "step  709 | train loss: 4.74 | val loss: 4.70 | perplexity: 109.61 | lr: 9.93e-04 | norm: 0.6899 | dt: 17128.3524ms | tok/sec: 22.9570\n",
      "step  710 | train loss: 4.62 | val loss: 4.69 | perplexity: 108.72 | lr: 9.94e-04 | norm: 0.7218 | dt: 17120.3015ms | tok/sec: 22.9678\n",
      "step  711 | train loss: 4.70 | val loss: 4.68 | perplexity: 107.70 | lr: 9.96e-04 | norm: 0.6880 | dt: 17060.2603ms | tok/sec: 23.0487\n",
      "step  712 | train loss: 4.75 | val loss: 4.68 | perplexity: 108.16 | lr: 9.97e-04 | norm: 0.7881 | dt: 17086.2436ms | tok/sec: 23.0136\n",
      "step  713 | train loss: 4.81 | val loss: 4.67 | perplexity: 106.28 | lr: 9.99e-04 | norm: 0.7767 | dt: 17116.3073ms | tok/sec: 22.9732\n",
      "step  714 | train loss: 5.06 | val loss: 4.67 | perplexity: 106.39 | lr: 1.00e-03 | norm: 0.5548 | dt: 17105.3681ms | tok/sec: 22.9879\n",
      "step  715 | train loss: 4.77 | val loss: 4.66 | perplexity: 105.13 | lr: 1.00e-03 | norm: 0.6377 | dt: 17092.7157ms | tok/sec: 23.0049\n",
      "step  716 | train loss: 4.60 | val loss: 4.65 | perplexity: 104.15 | lr: 1.00e-03 | norm: 0.5024 | dt: 17137.7945ms | tok/sec: 22.9444\n",
      "step  717 | train loss: 4.57 | val loss: 4.64 | perplexity: 103.82 | lr: 1.00e-03 | norm: 0.5698 | dt: 17117.8782ms | tok/sec: 22.9711\n",
      "step  718 | train loss: 4.41 | val loss: 4.64 | perplexity: 103.18 | lr: 1.00e-03 | norm: 0.7215 | dt: 17062.2408ms | tok/sec: 23.0460\n",
      "step  719 | train loss: 4.64 | val loss: 4.64 | perplexity: 104.01 | lr: 1.00e-03 | norm: 0.5451 | dt: 17149.8868ms | tok/sec: 22.9282\n",
      "step  720 | train loss: 4.68 | val loss: 4.63 | perplexity: 102.66 | lr: 1.00e-03 | norm: 0.5988 | dt: 17078.5344ms | tok/sec: 23.0240\n",
      "step  721 | train loss: 4.58 | val loss: 4.63 | perplexity: 102.38 | lr: 1.00e-03 | norm: 0.5034 | dt: 17060.4441ms | tok/sec: 23.0484\n",
      "step  722 | train loss: 4.48 | val loss: 4.61 | perplexity: 100.88 | lr: 1.00e-03 | norm: 0.5774 | dt: 17152.0278ms | tok/sec: 22.9253\n",
      "step  723 | train loss: 4.40 | val loss: 4.62 | perplexity: 101.67 | lr: 1.00e-03 | norm: 0.9715 | dt: 17111.1667ms | tok/sec: 22.9801\n",
      "step  724 | train loss: 4.55 | val loss: 4.62 | perplexity: 101.13 | lr: 1.00e-03 | norm: 0.8088 | dt: 17173.1148ms | tok/sec: 22.8972\n",
      "step  725 | train loss: 4.46 | val loss: 4.61 | perplexity: 100.73 | lr: 1.00e-03 | norm: 0.5118 | dt: 17112.9234ms | tok/sec: 22.9777\n",
      "step  726 | train loss: 4.70 | val loss: 4.60 | perplexity: 99.57 | lr: 1.00e-03 | norm: 0.5236 | dt: 17132.2238ms | tok/sec: 22.9518\n",
      "step  727 | train loss: 5.10 | val loss: 4.63 | perplexity: 102.43 | lr: 1.00e-03 | norm: 0.6104 | dt: 17128.2647ms | tok/sec: 22.9571\n",
      "step  728 | train loss: 4.76 | val loss: 4.62 | perplexity: 101.11 | lr: 1.00e-03 | norm: 0.6420 | dt: 17158.5786ms | tok/sec: 22.9166\n",
      "step  729 | train loss: 4.86 | val loss: 4.61 | perplexity: 100.18 | lr: 1.00e-03 | norm: 0.5822 | dt: 17044.5008ms | tok/sec: 23.0700\n",
      "step  730 | train loss: 4.67 | val loss: 4.60 | perplexity: 99.40 | lr: 1.00e-03 | norm: 0.6196 | dt: 17108.9897ms | tok/sec: 22.9830\n",
      "step  731 | train loss: 4.62 | val loss: 4.59 | perplexity: 98.46 | lr: 1.00e-03 | norm: 0.6360 | dt: 17118.8035ms | tok/sec: 22.9698\n",
      "step  732 | train loss: 4.49 | val loss: 4.62 | perplexity: 101.59 | lr: 1.00e-03 | norm: 0.8428 | dt: 17083.7693ms | tok/sec: 23.0169\n",
      "step  733 | train loss: 4.50 | val loss: 4.60 | perplexity: 99.50 | lr: 1.00e-03 | norm: 1.0340 | dt: 17088.1844ms | tok/sec: 23.0110\n",
      "step  734 | train loss: 4.67 | val loss: 4.59 | perplexity: 98.94 | lr: 1.00e-03 | norm: 0.7885 | dt: 17059.7920ms | tok/sec: 23.0493\n",
      "step  735 | train loss: 4.96 | val loss: 4.62 | perplexity: 101.64 | lr: 1.00e-03 | norm: 0.5595 | dt: 17132.4925ms | tok/sec: 22.9515\n",
      "step  736 | train loss: 4.59 | val loss: 4.60 | perplexity: 99.32 | lr: 1.00e-03 | norm: 0.8472 | dt: 17099.9424ms | tok/sec: 22.9952\n",
      "step  737 | train loss: 4.65 | val loss: 4.59 | perplexity: 98.27 | lr: 1.00e-03 | norm: 0.6862 | dt: 17073.3120ms | tok/sec: 23.0310\n",
      "step  738 | train loss: 4.42 | val loss: 4.59 | perplexity: 98.39 | lr: 1.00e-03 | norm: 0.6134 | dt: 17090.7352ms | tok/sec: 23.0076\n",
      "step  739 | train loss: 4.54 | val loss: 4.58 | perplexity: 97.79 | lr: 1.00e-03 | norm: 0.6309 | dt: 17034.5988ms | tok/sec: 23.0834\n",
      "step  740 | train loss: 4.87 | val loss: 4.57 | perplexity: 96.84 | lr: 1.00e-03 | norm: 0.5532 | dt: 17081.9635ms | tok/sec: 23.0194\n",
      "step  741 | train loss: 4.44 | val loss: 4.58 | perplexity: 97.56 | lr: 9.99e-04 | norm: 0.6585 | dt: 17090.2667ms | tok/sec: 23.0082\n",
      "step  742 | train loss: 4.77 | val loss: 4.59 | perplexity: 98.66 | lr: 9.99e-04 | norm: 0.7449 | dt: 17138.0830ms | tok/sec: 22.9440\n",
      "step  743 | train loss: 5.42 | val loss: 4.63 | perplexity: 102.26 | lr: 9.99e-04 | norm: 1.2036 | dt: 17103.1475ms | tok/sec: 22.9909\n",
      "step  744 | train loss: 5.10 | val loss: 4.63 | perplexity: 102.87 | lr: 9.99e-04 | norm: 0.8955 | dt: 17049.0024ms | tok/sec: 23.0639\n",
      "step  745 | train loss: 4.81 | val loss: 4.60 | perplexity: 99.92 | lr: 9.99e-04 | norm: 0.8119 | dt: 17077.6548ms | tok/sec: 23.0252\n",
      "step  746 | train loss: 4.89 | val loss: 4.60 | perplexity: 99.10 | lr: 9.99e-04 | norm: 0.7123 | dt: 17071.6379ms | tok/sec: 23.0333\n",
      "step  747 | train loss: 4.70 | val loss: 4.59 | perplexity: 98.37 | lr: 9.99e-04 | norm: 0.7376 | dt: 17135.3338ms | tok/sec: 22.9477\n",
      "step  748 | train loss: 4.52 | val loss: 4.58 | perplexity: 97.65 | lr: 9.99e-04 | norm: 0.7014 | dt: 17044.6548ms | tok/sec: 23.0698\n",
      "step  749 | train loss: 4.57 | val loss: 4.56 | perplexity: 95.37 | lr: 9.99e-04 | norm: 0.6701 | dt: 17074.6417ms | tok/sec: 23.0292\n",
      "step  750 | train loss: 4.53 | val loss: 4.55 | perplexity: 94.49 | lr: 9.99e-04 | norm: 0.5300 | dt: 17066.3481ms | tok/sec: 23.0404\n",
      "step  751 | train loss: 4.86 | val loss: 4.55 | perplexity: 94.45 | lr: 9.99e-04 | norm: 0.4581 | dt: 17144.8829ms | tok/sec: 22.9349\n",
      "step  752 | train loss: 4.37 | val loss: 4.55 | perplexity: 94.34 | lr: 9.99e-04 | norm: 0.5402 | dt: 17053.9188ms | tok/sec: 23.0572\n",
      "step  753 | train loss: 4.71 | val loss: 4.54 | perplexity: 94.02 | lr: 9.99e-04 | norm: 0.5763 | dt: 17135.4759ms | tok/sec: 22.9475\n",
      "step  754 | train loss: 4.83 | val loss: 4.54 | perplexity: 94.08 | lr: 9.99e-04 | norm: 0.6270 | dt: 17132.3884ms | tok/sec: 22.9516\n",
      "step  755 | train loss: 4.55 | val loss: 4.54 | perplexity: 93.40 | lr: 9.99e-04 | norm: 0.6248 | dt: 17132.5922ms | tok/sec: 22.9513\n",
      "step  756 | train loss: 4.61 | val loss: 4.53 | perplexity: 92.98 | lr: 9.99e-04 | norm: 0.6557 | dt: 17091.1729ms | tok/sec: 23.0070\n",
      "step  757 | train loss: 4.77 | val loss: 4.53 | perplexity: 92.39 | lr: 9.99e-04 | norm: 0.5225 | dt: 17109.2288ms | tok/sec: 22.9827\n",
      "step  758 | train loss: 5.58 | val loss: 4.56 | perplexity: 95.12 | lr: 9.99e-04 | norm: 0.6749 | dt: 17087.6625ms | tok/sec: 23.0117\n",
      "step  759 | train loss: 4.66 | val loss: 4.54 | perplexity: 93.85 | lr: 9.98e-04 | norm: 0.9582 | dt: 17062.7325ms | tok/sec: 23.0453\n",
      "step  760 | train loss: 4.57 | val loss: 4.56 | perplexity: 95.62 | lr: 9.98e-04 | norm: 0.6396 | dt: 17096.4968ms | tok/sec: 22.9998\n",
      "step  761 | train loss: 4.90 | val loss: 4.57 | perplexity: 96.84 | lr: 9.98e-04 | norm: 0.8942 | dt: 17071.8699ms | tok/sec: 23.0330\n",
      "step  762 | train loss: 4.65 | val loss: 4.56 | perplexity: 95.26 | lr: 9.98e-04 | norm: 0.7439 | dt: 17061.3441ms | tok/sec: 23.0472\n",
      "step  763 | train loss: 4.49 | val loss: 4.55 | perplexity: 94.83 | lr: 9.98e-04 | norm: 0.6445 | dt: 17074.8005ms | tok/sec: 23.0290\n",
      "step  764 | train loss: 4.78 | val loss: 4.54 | perplexity: 93.85 | lr: 9.98e-04 | norm: 0.8173 | dt: 17017.6315ms | tok/sec: 23.1064\n",
      "step  765 | train loss: 4.73 | val loss: 4.55 | perplexity: 94.39 | lr: 9.98e-04 | norm: 0.6329 | dt: 17080.5471ms | tok/sec: 23.0213\n",
      "step  766 | train loss: 4.55 | val loss: 4.55 | perplexity: 94.78 | lr: 9.98e-04 | norm: 0.5617 | dt: 17060.4260ms | tok/sec: 23.0484\n",
      "step  767 | train loss: 4.66 | val loss: 4.56 | perplexity: 95.20 | lr: 9.98e-04 | norm: 0.5911 | dt: 17097.3017ms | tok/sec: 22.9987\n",
      "step  768 | train loss: 4.59 | val loss: 4.55 | perplexity: 94.18 | lr: 9.98e-04 | norm: 0.5832 | dt: 17130.1522ms | tok/sec: 22.9546\n",
      "step  769 | train loss: 4.77 | val loss: 4.54 | perplexity: 93.67 | lr: 9.98e-04 | norm: 0.5386 | dt: 17096.9954ms | tok/sec: 22.9991\n",
      "step  770 | train loss: 4.19 | val loss: 4.55 | perplexity: 94.80 | lr: 9.98e-04 | norm: 0.8568 | dt: 17101.3098ms | tok/sec: 22.9933\n",
      "step  771 | train loss: 4.92 | val loss: 4.55 | perplexity: 94.24 | lr: 9.98e-04 | norm: 0.6866 | dt: 17129.2615ms | tok/sec: 22.9558\n",
      "step  772 | train loss: 4.51 | val loss: 4.54 | perplexity: 94.03 | lr: 9.97e-04 | norm: 0.6867 | dt: 17446.5945ms | tok/sec: 22.5383\n",
      "step  773 | train loss: 4.40 | val loss: 4.57 | perplexity: 96.41 | lr: 9.97e-04 | norm: 0.6894 | dt: 17239.1660ms | tok/sec: 22.8095\n",
      "step  774 | train loss: 4.45 | val loss: 4.56 | perplexity: 95.52 | lr: 9.97e-04 | norm: 0.8097 | dt: 17329.0524ms | tok/sec: 22.6911\n",
      "step  775 | train loss: 4.56 | val loss: 4.55 | perplexity: 94.84 | lr: 9.97e-04 | norm: 0.5830 | dt: 17086.4716ms | tok/sec: 23.0133\n",
      "step  776 | train loss: 5.14 | val loss: 4.57 | perplexity: 96.23 | lr: 9.97e-04 | norm: 0.7551 | dt: 17033.6070ms | tok/sec: 23.0847\n",
      "step  777 | train loss: 4.87 | val loss: 4.56 | perplexity: 95.49 | lr: 9.97e-04 | norm: 0.8145 | dt: 17119.8161ms | tok/sec: 22.9685\n",
      "step  778 | train loss: 4.70 | val loss: 4.55 | perplexity: 94.36 | lr: 9.97e-04 | norm: 0.6862 | dt: 17211.5624ms | tok/sec: 22.8460\n",
      "step  779 | train loss: 4.71 | val loss: 4.54 | perplexity: 93.62 | lr: 9.97e-04 | norm: 0.5772 | dt: 17110.9006ms | tok/sec: 22.9804\n",
      "step  780 | train loss: 4.46 | val loss: 4.54 | perplexity: 93.59 | lr: 9.97e-04 | norm: 0.6346 | dt: 17098.0699ms | tok/sec: 22.9977\n",
      "step  781 | train loss: 4.97 | val loss: 4.57 | perplexity: 96.19 | lr: 9.97e-04 | norm: 0.7031 | dt: 17465.0085ms | tok/sec: 22.5145\n",
      "step  782 | train loss: 4.50 | val loss: 4.55 | perplexity: 94.67 | lr: 9.96e-04 | norm: 0.9215 | dt: 17141.2530ms | tok/sec: 22.9397\n",
      "step  783 | train loss: 4.38 | val loss: 4.55 | perplexity: 94.69 | lr: 9.96e-04 | norm: 0.6209 | dt: 17102.6027ms | tok/sec: 22.9916\n",
      "step  784 | train loss: 4.55 | val loss: 4.54 | perplexity: 94.01 | lr: 9.96e-04 | norm: 0.6182 | dt: 17099.8900ms | tok/sec: 22.9952\n",
      "step  785 | train loss: 4.65 | val loss: 4.54 | perplexity: 93.58 | lr: 9.96e-04 | norm: 0.5411 | dt: 17130.6663ms | tok/sec: 22.9539\n",
      "step  786 | train loss: 4.73 | val loss: 4.55 | perplexity: 94.46 | lr: 9.96e-04 | norm: 0.5704 | dt: 17057.7986ms | tok/sec: 23.0520\n",
      "step  787 | train loss: 4.46 | val loss: 4.54 | perplexity: 94.07 | lr: 9.96e-04 | norm: 0.5419 | dt: 17084.4259ms | tok/sec: 23.0160\n",
      "step  788 | train loss: 4.61 | val loss: 4.53 | perplexity: 92.37 | lr: 9.96e-04 | norm: 0.6996 | dt: 17048.5191ms | tok/sec: 23.0645\n",
      "step  789 | train loss: 4.67 | val loss: 4.52 | perplexity: 92.03 | lr: 9.96e-04 | norm: 0.5260 | dt: 17144.8214ms | tok/sec: 22.9350\n",
      "step  790 | train loss: 4.50 | val loss: 4.51 | perplexity: 90.90 | lr: 9.96e-04 | norm: 0.5444 | dt: 17115.0928ms | tok/sec: 22.9748\n",
      "step  791 | train loss: 4.61 | val loss: 4.50 | perplexity: 89.93 | lr: 9.95e-04 | norm: 0.5403 | dt: 17008.4670ms | tok/sec: 23.1188\n",
      "step  792 | train loss: 4.58 | val loss: 4.50 | perplexity: 89.95 | lr: 9.95e-04 | norm: 0.5179 | dt: 17067.0235ms | tok/sec: 23.0395\n",
      "step  793 | train loss: 4.83 | val loss: 4.50 | perplexity: 90.24 | lr: 9.95e-04 | norm: 0.6125 | dt: 17118.4833ms | tok/sec: 22.9703\n",
      "step  794 | train loss: 4.79 | val loss: 4.49 | perplexity: 89.45 | lr: 9.95e-04 | norm: 0.5576 | dt: 17111.9585ms | tok/sec: 22.9790\n",
      "step  795 | train loss: 4.68 | val loss: 4.49 | perplexity: 88.81 | lr: 9.95e-04 | norm: 0.4297 | dt: 17080.1866ms | tok/sec: 23.0218\n",
      "step  796 | train loss: 4.26 | val loss: 4.48 | perplexity: 88.17 | lr: 9.95e-04 | norm: 0.5427 | dt: 17117.6388ms | tok/sec: 22.9714\n",
      "step  797 | train loss: 4.52 | val loss: 4.48 | perplexity: 88.67 | lr: 9.95e-04 | norm: 0.4572 | dt: 17072.0954ms | tok/sec: 23.0327\n",
      "step  798 | train loss: 4.73 | val loss: 4.47 | perplexity: 87.75 | lr: 9.95e-04 | norm: 0.5699 | dt: 17049.6638ms | tok/sec: 23.0630\n",
      "step  799 | train loss: 4.63 | val loss: 4.47 | perplexity: 87.37 | lr: 9.94e-04 | norm: 0.5619 | dt: 17117.0812ms | tok/sec: 22.9721\n",
      "step  800 | train loss: 4.33 | val loss: 4.48 | perplexity: 88.60 | lr: 9.94e-04 | norm: 0.7530 | dt: 17071.8834ms | tok/sec: 23.0330\n",
      "step  801 | train loss: 4.49 | val loss: 4.48 | perplexity: 88.45 | lr: 9.94e-04 | norm: 0.7019 | dt: 17029.2130ms | tok/sec: 23.0907\n",
      "step  802 | train loss: 4.61 | val loss: 4.49 | perplexity: 89.37 | lr: 9.94e-04 | norm: 0.6146 | dt: 17069.3545ms | tok/sec: 23.0364\n",
      "step  803 | train loss: 4.39 | val loss: 4.48 | perplexity: 87.92 | lr: 9.94e-04 | norm: 0.6699 | dt: 17063.4582ms | tok/sec: 23.0443\n",
      "step  804 | train loss: 4.36 | val loss: 4.48 | perplexity: 88.61 | lr: 9.94e-04 | norm: 0.5720 | dt: 17141.9418ms | tok/sec: 22.9388\n",
      "step  805 | train loss: 4.70 | val loss: 4.49 | perplexity: 88.68 | lr: 9.94e-04 | norm: 0.6113 | dt: 17057.9023ms | tok/sec: 23.0518\n",
      "step  806 | train loss: 4.48 | val loss: 4.48 | perplexity: 88.45 | lr: 9.94e-04 | norm: 0.5724 | dt: 17098.2528ms | tok/sec: 22.9974\n",
      "step  807 | train loss: 4.58 | val loss: 4.48 | perplexity: 88.08 | lr: 9.93e-04 | norm: 0.5181 | dt: 17192.6403ms | tok/sec: 22.8712\n",
      "step  808 | train loss: 4.43 | val loss: 4.47 | perplexity: 87.27 | lr: 9.93e-04 | norm: 0.5478 | dt: 17080.0567ms | tok/sec: 23.0219\n",
      "step  809 | train loss: 4.51 | val loss: 4.46 | perplexity: 86.76 | lr: 9.93e-04 | norm: 0.6020 | dt: 17096.8277ms | tok/sec: 22.9994\n",
      "step  810 | train loss: 4.66 | val loss: 4.46 | perplexity: 86.61 | lr: 9.93e-04 | norm: 0.5599 | dt: 17078.2487ms | tok/sec: 23.0244\n",
      "step  811 | train loss: 4.52 | val loss: 4.46 | perplexity: 86.19 | lr: 9.93e-04 | norm: 0.4529 | dt: 17145.5548ms | tok/sec: 22.9340\n",
      "step  812 | train loss: 4.45 | val loss: 4.45 | perplexity: 85.49 | lr: 9.93e-04 | norm: 0.4518 | dt: 17051.1281ms | tok/sec: 23.0610\n",
      "step  813 | train loss: 4.35 | val loss: 4.44 | perplexity: 85.14 | lr: 9.93e-04 | norm: 0.5560 | dt: 17041.3375ms | tok/sec: 23.0742\n",
      "step  814 | train loss: 4.54 | val loss: 4.45 | perplexity: 85.33 | lr: 9.92e-04 | norm: 0.4322 | dt: 17134.2556ms | tok/sec: 22.9491\n",
      "step  815 | train loss: 4.79 | val loss: 4.44 | perplexity: 84.83 | lr: 9.92e-04 | norm: 0.5161 | dt: 17088.5897ms | tok/sec: 23.0104\n",
      "step  816 | train loss: 4.53 | val loss: 4.46 | perplexity: 86.26 | lr: 9.92e-04 | norm: 0.5412 | dt: 17088.7787ms | tok/sec: 23.0102\n",
      "step  817 | train loss: 4.53 | val loss: 4.45 | perplexity: 85.32 | lr: 9.92e-04 | norm: 0.6298 | dt: 17053.0422ms | tok/sec: 23.0584\n",
      "step  818 | train loss: 4.39 | val loss: 4.46 | perplexity: 86.77 | lr: 9.92e-04 | norm: 0.6119 | dt: 17084.3468ms | tok/sec: 23.0162\n",
      "step  819 | train loss: 4.73 | val loss: 4.45 | perplexity: 85.30 | lr: 9.92e-04 | norm: 0.7860 | dt: 17083.9100ms | tok/sec: 23.0167\n",
      "step  820 | train loss: 4.68 | val loss: 4.44 | perplexity: 84.64 | lr: 9.91e-04 | norm: 0.5768 | dt: 17083.6384ms | tok/sec: 23.0171\n",
      "step  821 | train loss: 4.54 | val loss: 4.43 | perplexity: 84.23 | lr: 9.91e-04 | norm: 0.5860 | dt: 17078.0337ms | tok/sec: 23.0247\n",
      "step  822 | train loss: 4.38 | val loss: 4.43 | perplexity: 84.32 | lr: 9.91e-04 | norm: 0.5472 | dt: 17104.1260ms | tok/sec: 22.9895\n",
      "step  823 | train loss: 4.33 | val loss: 4.43 | perplexity: 83.68 | lr: 9.91e-04 | norm: 0.6308 | dt: 17068.5532ms | tok/sec: 23.0375\n",
      "step  824 | train loss: 4.23 | val loss: 4.43 | perplexity: 83.83 | lr: 9.91e-04 | norm: 0.5302 | dt: 17128.4323ms | tok/sec: 22.9569\n",
      "step  825 | train loss: 4.81 | val loss: 4.43 | perplexity: 83.72 | lr: 9.91e-04 | norm: 0.4875 | dt: 17150.0094ms | tok/sec: 22.9280\n",
      "step  826 | train loss: 4.47 | val loss: 4.43 | perplexity: 83.63 | lr: 9.90e-04 | norm: 0.4731 | dt: 17327.4639ms | tok/sec: 22.6932\n",
      "step  827 | train loss: 4.69 | val loss: 4.42 | perplexity: 83.30 | lr: 9.90e-04 | norm: 0.5038 | dt: 17112.3619ms | tok/sec: 22.9785\n",
      "step  828 | train loss: 4.59 | val loss: 4.42 | perplexity: 83.15 | lr: 9.90e-04 | norm: 0.4597 | dt: 17135.6049ms | tok/sec: 22.9473\n",
      "step  829 | train loss: 4.67 | val loss: 4.42 | perplexity: 82.94 | lr: 9.90e-04 | norm: 0.4447 | dt: 17103.0827ms | tok/sec: 22.9909\n",
      "step  830 | train loss: 4.70 | val loss: 4.41 | perplexity: 82.46 | lr: 9.90e-04 | norm: 0.3991 | dt: 17084.6448ms | tok/sec: 23.0158\n",
      "step  831 | train loss: 4.96 | val loss: 4.42 | perplexity: 82.84 | lr: 9.90e-04 | norm: 0.5382 | dt: 17092.4311ms | tok/sec: 23.0053\n",
      "step  832 | train loss: 4.26 | val loss: 4.41 | perplexity: 82.30 | lr: 9.89e-04 | norm: 0.5992 | dt: 17092.5965ms | tok/sec: 23.0050\n",
      "step  833 | train loss: 4.58 | val loss: 4.41 | perplexity: 82.14 | lr: 9.89e-04 | norm: 0.4738 | dt: 17089.0753ms | tok/sec: 23.0098\n",
      "step  834 | train loss: 4.58 | val loss: 4.40 | perplexity: 81.79 | lr: 9.89e-04 | norm: 0.4652 | dt: 17083.0941ms | tok/sec: 23.0178\n",
      "step  835 | train loss: 4.61 | val loss: 4.42 | perplexity: 83.32 | lr: 9.89e-04 | norm: 0.4882 | dt: 17055.9626ms | tok/sec: 23.0545\n",
      "step  836 | train loss: 4.53 | val loss: 4.41 | perplexity: 82.50 | lr: 9.89e-04 | norm: 0.5984 | dt: 17092.2296ms | tok/sec: 23.0055\n",
      "step  837 | train loss: 4.97 | val loss: 4.41 | perplexity: 82.43 | lr: 9.88e-04 | norm: 0.4387 | dt: 17194.6695ms | tok/sec: 22.8685\n",
      "step  838 | train loss: 4.62 | val loss: 4.41 | perplexity: 81.97 | lr: 9.88e-04 | norm: 0.5063 | dt: 17104.7437ms | tok/sec: 22.9887\n",
      "step  839 | train loss: 4.40 | val loss: 4.41 | perplexity: 81.90 | lr: 9.88e-04 | norm: 0.6576 | dt: 17124.2056ms | tok/sec: 22.9626\n",
      "step  840 | train loss: 4.66 | val loss: 4.41 | perplexity: 82.00 | lr: 9.88e-04 | norm: 0.4965 | dt: 17133.9991ms | tok/sec: 22.9495\n",
      "step  841 | train loss: 4.55 | val loss: 4.39 | perplexity: 80.79 | lr: 9.88e-04 | norm: 0.6641 | dt: 17089.8352ms | tok/sec: 23.0088\n",
      "step  842 | train loss: 4.67 | val loss: 4.39 | perplexity: 81.01 | lr: 9.87e-04 | norm: 0.5709 | dt: 17160.5976ms | tok/sec: 22.9139\n",
      "step  843 | train loss: 4.57 | val loss: 4.39 | perplexity: 80.59 | lr: 9.87e-04 | norm: 0.5065 | dt: 17123.2970ms | tok/sec: 22.9638\n",
      "step  844 | train loss: 4.57 | val loss: 4.39 | perplexity: 80.82 | lr: 9.87e-04 | norm: 0.6112 | dt: 17085.6872ms | tok/sec: 23.0144\n",
      "step  845 | train loss: 4.45 | val loss: 4.38 | perplexity: 79.77 | lr: 9.87e-04 | norm: 0.5072 | dt: 17084.2607ms | tok/sec: 23.0163\n",
      "step  846 | train loss: 4.67 | val loss: 4.38 | perplexity: 79.49 | lr: 9.87e-04 | norm: 0.4381 | dt: 17027.0131ms | tok/sec: 23.0937\n",
      "step  847 | train loss: 4.39 | val loss: 4.38 | perplexity: 79.61 | lr: 9.86e-04 | norm: 0.5083 | dt: 17093.9171ms | tok/sec: 23.0033\n",
      "step  848 | train loss: 4.31 | val loss: 4.37 | perplexity: 79.17 | lr: 9.86e-04 | norm: 0.4573 | dt: 17077.5418ms | tok/sec: 23.0253\n",
      "step  849 | train loss: 4.71 | val loss: 4.37 | perplexity: 79.31 | lr: 9.86e-04 | norm: 0.5456 | dt: 17343.5364ms | tok/sec: 22.6722\n",
      "step  850 | train loss: 4.76 | val loss: 4.38 | perplexity: 79.66 | lr: 9.86e-04 | norm: 0.4852 | dt: 17099.7190ms | tok/sec: 22.9955\n",
      "step  851 | train loss: 4.40 | val loss: 4.37 | perplexity: 79.33 | lr: 9.86e-04 | norm: 0.5747 | dt: 17070.7150ms | tok/sec: 23.0345\n",
      "step  852 | train loss: 4.51 | val loss: 4.38 | perplexity: 79.88 | lr: 9.85e-04 | norm: 0.5457 | dt: 17096.0438ms | tok/sec: 23.0004\n",
      "step  853 | train loss: 4.53 | val loss: 4.38 | perplexity: 80.12 | lr: 9.85e-04 | norm: 0.5278 | dt: 17057.1127ms | tok/sec: 23.0529\n",
      "step  854 | train loss: 4.52 | val loss: 4.38 | perplexity: 79.91 | lr: 9.85e-04 | norm: 0.4677 | dt: 17072.0754ms | tok/sec: 23.0327\n",
      "step  855 | train loss: 4.80 | val loss: 4.37 | perplexity: 79.26 | lr: 9.85e-04 | norm: 0.4446 | dt: 17052.1786ms | tok/sec: 23.0596\n",
      "step  856 | train loss: 4.58 | val loss: 4.37 | perplexity: 78.91 | lr: 9.85e-04 | norm: 0.3966 | dt: 17057.7021ms | tok/sec: 23.0521\n",
      "step  857 | train loss: 4.30 | val loss: 4.36 | perplexity: 78.14 | lr: 9.84e-04 | norm: 0.4299 | dt: 17087.5604ms | tok/sec: 23.0118\n",
      "step  858 | train loss: 4.56 | val loss: 4.36 | perplexity: 78.34 | lr: 9.84e-04 | norm: 0.4575 | dt: 17101.4702ms | tok/sec: 22.9931\n",
      "step  859 | train loss: 4.88 | val loss: 4.36 | perplexity: 78.50 | lr: 9.84e-04 | norm: 0.5559 | dt: 17129.1444ms | tok/sec: 22.9560\n",
      "step  860 | train loss: 4.67 | val loss: 4.38 | perplexity: 79.61 | lr: 9.84e-04 | norm: 0.5147 | dt: 17073.0011ms | tok/sec: 23.0315\n",
      "step  861 | train loss: 4.62 | val loss: 4.37 | perplexity: 78.92 | lr: 9.83e-04 | norm: 0.5659 | dt: 17176.1999ms | tok/sec: 22.8931\n",
      "step  862 | train loss: 4.47 | val loss: 4.36 | perplexity: 78.57 | lr: 9.83e-04 | norm: 0.5939 | dt: 17038.6322ms | tok/sec: 23.0779\n",
      "step  863 | train loss: 4.71 | val loss: 4.36 | perplexity: 78.54 | lr: 9.83e-04 | norm: 0.5681 | dt: 17067.8365ms | tok/sec: 23.0384\n",
      "step  864 | train loss: 4.61 | val loss: 4.35 | perplexity: 77.83 | lr: 9.83e-04 | norm: 0.5825 | dt: 17082.6249ms | tok/sec: 23.0185\n",
      "step  865 | train loss: 4.65 | val loss: 4.36 | perplexity: 78.43 | lr: 9.83e-04 | norm: 0.5195 | dt: 17096.2691ms | tok/sec: 23.0001\n",
      "step  866 | train loss: 4.56 | val loss: 4.35 | perplexity: 77.80 | lr: 9.82e-04 | norm: 0.6273 | dt: 17103.0209ms | tok/sec: 22.9910\n",
      "step  867 | train loss: 5.08 | val loss: 4.39 | perplexity: 80.70 | lr: 9.82e-04 | norm: 0.9965 | dt: 17092.8943ms | tok/sec: 23.0046\n",
      "step  868 | train loss: 4.54 | val loss: 4.41 | perplexity: 82.45 | lr: 9.82e-04 | norm: 0.6066 | dt: 17181.6950ms | tok/sec: 22.8858\n",
      "step  869 | train loss: 4.76 | val loss: 4.41 | perplexity: 82.38 | lr: 9.82e-04 | norm: 0.6442 | dt: 17069.1822ms | tok/sec: 23.0366\n",
      "step  870 | train loss: 4.70 | val loss: 4.42 | perplexity: 83.45 | lr: 9.81e-04 | norm: 0.5639 | dt: 17045.0990ms | tok/sec: 23.0692\n",
      "step  871 | train loss: 4.73 | val loss: 4.40 | perplexity: 81.79 | lr: 9.81e-04 | norm: 0.9310 | dt: 17096.0827ms | tok/sec: 23.0004\n",
      "step  872 | train loss: 4.61 | val loss: 4.41 | perplexity: 81.89 | lr: 9.81e-04 | norm: 0.5829 | dt: 17064.3840ms | tok/sec: 23.0431\n",
      "step  873 | train loss: 4.59 | val loss: 4.40 | perplexity: 81.23 | lr: 9.81e-04 | norm: 0.5173 | dt: 17092.6354ms | tok/sec: 23.0050\n",
      "step  874 | train loss: 4.64 | val loss: 4.38 | perplexity: 79.46 | lr: 9.80e-04 | norm: 0.5804 | dt: 17089.0653ms | tok/sec: 23.0098\n",
      "step  875 | train loss: 4.54 | val loss: 4.38 | perplexity: 79.75 | lr: 9.80e-04 | norm: 0.4557 | dt: 17006.8142ms | tok/sec: 23.1211\n",
      "step  876 | train loss: 4.76 | val loss: 4.36 | perplexity: 78.34 | lr: 9.80e-04 | norm: 0.5920 | dt: 17083.1227ms | tok/sec: 23.0178\n",
      "step  877 | train loss: 5.29 | val loss: 4.36 | perplexity: 78.60 | lr: 9.80e-04 | norm: 0.7394 | dt: 17088.5005ms | tok/sec: 23.0106\n",
      "step  878 | train loss: 4.92 | val loss: 4.36 | perplexity: 78.37 | lr: 9.79e-04 | norm: 0.6054 | dt: 17055.8894ms | tok/sec: 23.0546\n",
      "step  879 | train loss: 4.62 | val loss: 4.37 | perplexity: 78.89 | lr: 9.79e-04 | norm: 0.4362 | dt: 17047.2593ms | tok/sec: 23.0662\n",
      "step  880 | train loss: 4.49 | val loss: 4.36 | perplexity: 77.96 | lr: 9.79e-04 | norm: 0.4734 | dt: 17188.8130ms | tok/sec: 22.8763\n",
      "step  881 | train loss: 4.58 | val loss: 4.35 | perplexity: 77.75 | lr: 9.79e-04 | norm: 0.4464 | dt: 17072.1266ms | tok/sec: 23.0326\n",
      "step  882 | train loss: 4.36 | val loss: 4.34 | perplexity: 76.86 | lr: 9.78e-04 | norm: 0.5056 | dt: 17094.6257ms | tok/sec: 23.0023\n",
      "step  883 | train loss: 4.47 | val loss: 4.34 | perplexity: 76.79 | lr: 9.78e-04 | norm: 0.4254 | dt: 17082.6135ms | tok/sec: 23.0185\n",
      "step  884 | train loss: 4.52 | val loss: 4.34 | perplexity: 76.51 | lr: 9.78e-04 | norm: 0.5278 | dt: 17237.2742ms | tok/sec: 22.8120\n",
      "step  885 | train loss: 4.06 | val loss: 4.33 | perplexity: 75.84 | lr: 9.78e-04 | norm: 0.5257 | dt: 17088.6366ms | tok/sec: 23.0104\n",
      "step  886 | train loss: 4.48 | val loss: 4.32 | perplexity: 75.33 | lr: 9.77e-04 | norm: 0.4124 | dt: 17046.2711ms | tok/sec: 23.0676\n",
      "step  887 | train loss: 4.44 | val loss: 4.32 | perplexity: 75.51 | lr: 9.77e-04 | norm: 0.4237 | dt: 17061.0812ms | tok/sec: 23.0475\n",
      "step  888 | train loss: 4.43 | val loss: 4.33 | perplexity: 75.61 | lr: 9.77e-04 | norm: 0.4319 | dt: 17071.8017ms | tok/sec: 23.0331\n",
      "step  889 | train loss: 4.28 | val loss: 4.32 | perplexity: 75.07 | lr: 9.77e-04 | norm: 0.3975 | dt: 17071.3935ms | tok/sec: 23.0336\n",
      "step  890 | train loss: 4.60 | val loss: 4.35 | perplexity: 77.23 | lr: 9.76e-04 | norm: 0.4992 | dt: 17142.9358ms | tok/sec: 22.9375\n",
      "step  891 | train loss: 4.69 | val loss: 4.36 | perplexity: 78.09 | lr: 9.76e-04 | norm: 0.5860 | dt: 17066.9498ms | tok/sec: 23.0396\n",
      "step  892 | train loss: 4.26 | val loss: 4.37 | perplexity: 79.27 | lr: 9.76e-04 | norm: 0.8249 | dt: 17011.4317ms | tok/sec: 23.1148\n",
      "step  893 | train loss: 4.69 | val loss: 4.36 | perplexity: 78.62 | lr: 9.75e-04 | norm: 0.7860 | dt: 17139.8180ms | tok/sec: 22.9417\n",
      "step  894 | train loss: 4.56 | val loss: 4.39 | perplexity: 80.76 | lr: 9.75e-04 | norm: 0.8802 | dt: 17037.2131ms | tok/sec: 23.0798\n",
      "step  895 | train loss: 4.43 | val loss: 4.36 | perplexity: 78.48 | lr: 9.75e-04 | norm: 0.9499 | dt: 17105.9675ms | tok/sec: 22.9871\n",
      "step  896 | train loss: 4.59 | val loss: 4.35 | perplexity: 77.52 | lr: 9.75e-04 | norm: 0.6642 | dt: 17051.5070ms | tok/sec: 23.0605\n",
      "step  897 | train loss: 4.64 | val loss: 4.35 | perplexity: 77.52 | lr: 9.74e-04 | norm: 0.5488 | dt: 17116.2183ms | tok/sec: 22.9733\n",
      "step  898 | train loss: 4.52 | val loss: 4.36 | perplexity: 78.17 | lr: 9.74e-04 | norm: 0.4808 | dt: 17121.1255ms | tok/sec: 22.9667\n",
      "step  899 | train loss: 4.87 | val loss: 4.37 | perplexity: 79.34 | lr: 9.74e-04 | norm: 0.5888 | dt: 17085.7625ms | tok/sec: 23.0142\n",
      "step  900 | train loss: 5.05 | val loss: 4.40 | perplexity: 81.19 | lr: 9.73e-04 | norm: 1.0984 | dt: 17093.6675ms | tok/sec: 23.0036\n",
      "step  901 | train loss: 4.63 | val loss: 4.36 | perplexity: 78.56 | lr: 9.73e-04 | norm: 0.9140 | dt: 17042.9158ms | tok/sec: 23.0721\n",
      "step  902 | train loss: 4.60 | val loss: 4.36 | perplexity: 78.59 | lr: 9.73e-04 | norm: 0.5985 | dt: 17108.0186ms | tok/sec: 22.9843\n",
      "step  903 | train loss: 4.48 | val loss: 4.36 | perplexity: 78.44 | lr: 9.73e-04 | norm: 0.7894 | dt: 17179.5435ms | tok/sec: 22.8886\n",
      "step  904 | train loss: 4.38 | val loss: 4.36 | perplexity: 78.23 | lr: 9.72e-04 | norm: 0.5885 | dt: 17111.2933ms | tok/sec: 22.9799\n",
      "step  905 | train loss: 4.41 | val loss: 4.36 | perplexity: 77.93 | lr: 9.72e-04 | norm: 0.4973 | dt: 17139.8585ms | tok/sec: 22.9416\n",
      "step  906 | train loss: 4.83 | val loss: 4.35 | perplexity: 77.72 | lr: 9.72e-04 | norm: 0.5820 | dt: 17081.6684ms | tok/sec: 23.0198\n",
      "step  907 | train loss: 4.32 | val loss: 4.35 | perplexity: 77.14 | lr: 9.71e-04 | norm: 0.6516 | dt: 17039.7658ms | tok/sec: 23.0764\n",
      "step  908 | train loss: 4.63 | val loss: 4.35 | perplexity: 77.24 | lr: 9.71e-04 | norm: 0.5088 | dt: 17090.4570ms | tok/sec: 23.0079\n",
      "step  909 | train loss: 4.44 | val loss: 4.34 | perplexity: 76.40 | lr: 9.71e-04 | norm: 0.6189 | dt: 17043.4525ms | tok/sec: 23.0714\n",
      "step  910 | train loss: 4.67 | val loss: 4.34 | perplexity: 76.84 | lr: 9.71e-04 | norm: 0.7483 | dt: 17067.4946ms | tok/sec: 23.0389\n",
      "step  911 | train loss: 4.40 | val loss: 4.34 | perplexity: 76.75 | lr: 9.70e-04 | norm: 0.5938 | dt: 17099.1833ms | tok/sec: 22.9962\n",
      "step  912 | train loss: 4.82 | val loss: 4.34 | perplexity: 76.52 | lr: 9.70e-04 | norm: 0.6488 | dt: 17025.6982ms | tok/sec: 23.0954\n",
      "step  913 | train loss: 4.53 | val loss: 4.34 | perplexity: 76.76 | lr: 9.70e-04 | norm: 0.4610 | dt: 17122.8833ms | tok/sec: 22.9644\n",
      "step  914 | train loss: 4.57 | val loss: 4.33 | perplexity: 75.98 | lr: 9.69e-04 | norm: 0.4937 | dt: 17033.8082ms | tok/sec: 23.0844\n",
      "step  915 | train loss: 4.63 | val loss: 4.33 | perplexity: 75.86 | lr: 9.69e-04 | norm: 0.4599 | dt: 17103.2298ms | tok/sec: 22.9907\n",
      "step  916 | train loss: 4.69 | val loss: 4.33 | perplexity: 75.88 | lr: 9.69e-04 | norm: 0.5677 | dt: 17418.7562ms | tok/sec: 22.5743\n",
      "step  917 | train loss: 4.69 | val loss: 4.33 | perplexity: 76.14 | lr: 9.68e-04 | norm: 0.7519 | dt: 17074.1582ms | tok/sec: 23.0299\n",
      "step  918 | train loss: 4.61 | val loss: 4.33 | perplexity: 76.07 | lr: 9.68e-04 | norm: 0.5604 | dt: 17069.6573ms | tok/sec: 23.0360\n",
      "step  919 | train loss: 4.85 | val loss: 4.33 | perplexity: 76.16 | lr: 9.68e-04 | norm: 0.6059 | dt: 17075.3934ms | tok/sec: 23.0282\n",
      "step  920 | train loss: 4.26 | val loss: 4.33 | perplexity: 76.05 | lr: 9.68e-04 | norm: 0.7606 | dt: 17052.9563ms | tok/sec: 23.0585\n",
      "step  921 | train loss: 4.61 | val loss: 4.34 | perplexity: 77.07 | lr: 9.67e-04 | norm: 0.6687 | dt: 17073.6415ms | tok/sec: 23.0306\n",
      "step  922 | train loss: 4.47 | val loss: 4.33 | perplexity: 76.13 | lr: 9.67e-04 | norm: 0.6972 | dt: 17037.7014ms | tok/sec: 23.0792\n",
      "step  923 | train loss: 4.59 | val loss: 4.33 | perplexity: 76.11 | lr: 9.67e-04 | norm: 0.6184 | dt: 17135.7248ms | tok/sec: 22.9471\n",
      "step  924 | train loss: 4.58 | val loss: 4.33 | perplexity: 75.80 | lr: 9.66e-04 | norm: 0.5577 | dt: 17073.7190ms | tok/sec: 23.0305\n",
      "step  925 | train loss: 4.66 | val loss: 4.33 | perplexity: 75.73 | lr: 9.66e-04 | norm: 1.1031 | dt: 17133.1999ms | tok/sec: 22.9505\n",
      "step  926 | train loss: 4.57 | val loss: 4.34 | perplexity: 76.68 | lr: 9.66e-04 | norm: 0.5677 | dt: 17104.1965ms | tok/sec: 22.9894\n",
      "step  927 | train loss: 4.45 | val loss: 4.33 | perplexity: 75.75 | lr: 9.65e-04 | norm: 0.7619 | dt: 17151.0909ms | tok/sec: 22.9266\n",
      "step  928 | train loss: 4.63 | val loss: 4.32 | perplexity: 75.47 | lr: 9.65e-04 | norm: 0.5172 | dt: 17099.9577ms | tok/sec: 22.9951\n",
      "step  929 | train loss: 4.68 | val loss: 4.32 | perplexity: 75.14 | lr: 9.65e-04 | norm: 0.5723 | dt: 17124.8710ms | tok/sec: 22.9617\n",
      "step  930 | train loss: 4.23 | val loss: 4.31 | perplexity: 74.54 | lr: 9.64e-04 | norm: 0.5608 | dt: 17091.2695ms | tok/sec: 23.0068\n",
      "step  931 | train loss: 4.54 | val loss: 4.31 | perplexity: 74.54 | lr: 9.64e-04 | norm: 0.5319 | dt: 17046.6688ms | tok/sec: 23.0670\n",
      "step  932 | train loss: 5.73 | val loss: 4.34 | perplexity: 76.57 | lr: 9.64e-04 | norm: 1.6458 | dt: 17111.2292ms | tok/sec: 22.9800\n",
      "step  933 | train loss: 4.52 | val loss: 4.33 | perplexity: 75.88 | lr: 9.63e-04 | norm: 0.5900 | dt: 17356.1509ms | tok/sec: 22.6557\n",
      "step  934 | train loss: 4.48 | val loss: 4.32 | perplexity: 75.37 | lr: 9.63e-04 | norm: 0.5469 | dt: 17101.0101ms | tok/sec: 22.9937\n",
      "step  935 | train loss: 4.48 | val loss: 4.32 | perplexity: 75.30 | lr: 9.63e-04 | norm: 0.5628 | dt: 17058.8586ms | tok/sec: 23.0505\n",
      "step  936 | train loss: 4.47 | val loss: 4.31 | perplexity: 74.35 | lr: 9.62e-04 | norm: 0.5897 | dt: 17115.0830ms | tok/sec: 22.9748\n",
      "step  937 | train loss: 4.56 | val loss: 4.30 | perplexity: 73.93 | lr: 9.62e-04 | norm: 0.5257 | dt: 17129.4963ms | tok/sec: 22.9555\n",
      "step  938 | train loss: 4.59 | val loss: 4.30 | perplexity: 73.50 | lr: 9.62e-04 | norm: 0.5232 | dt: 17144.2118ms | tok/sec: 22.9358\n",
      "step  939 | train loss: 4.80 | val loss: 4.30 | perplexity: 73.77 | lr: 9.61e-04 | norm: 0.5316 | dt: 17169.5006ms | tok/sec: 22.9020\n",
      "step  940 | train loss: 4.66 | val loss: 4.31 | perplexity: 74.10 | lr: 9.61e-04 | norm: 0.5763 | dt: 17099.7932ms | tok/sec: 22.9954\n",
      "step  941 | train loss: 4.55 | val loss: 4.30 | perplexity: 73.98 | lr: 9.61e-04 | norm: 0.5221 | dt: 17132.8743ms | tok/sec: 22.9510\n",
      "step  942 | train loss: 4.49 | val loss: 4.30 | perplexity: 73.56 | lr: 9.60e-04 | norm: 0.5488 | dt: 17082.7935ms | tok/sec: 23.0182\n",
      "step  943 | train loss: 4.34 | val loss: 4.29 | perplexity: 73.20 | lr: 9.60e-04 | norm: 0.4824 | dt: 17142.7689ms | tok/sec: 22.9377\n",
      "step  944 | train loss: 4.61 | val loss: 4.29 | perplexity: 72.67 | lr: 9.60e-04 | norm: 0.5220 | dt: 17111.4740ms | tok/sec: 22.9797\n",
      "step  945 | train loss: 4.64 | val loss: 4.28 | perplexity: 72.42 | lr: 9.59e-04 | norm: 0.3887 | dt: 17046.9942ms | tok/sec: 23.0666\n",
      "step  946 | train loss: 4.63 | val loss: 4.29 | perplexity: 72.63 | lr: 9.59e-04 | norm: 0.6162 | dt: 17089.6451ms | tok/sec: 23.0090\n",
      "step  947 | train loss: 4.69 | val loss: 4.29 | perplexity: 72.73 | lr: 9.59e-04 | norm: 0.5337 | dt: 17084.4064ms | tok/sec: 23.0161\n",
      "step  948 | train loss: 4.74 | val loss: 4.28 | perplexity: 72.45 | lr: 9.58e-04 | norm: 0.4898 | dt: 17115.6733ms | tok/sec: 22.9740\n",
      "step  949 | train loss: 4.28 | val loss: 4.28 | perplexity: 72.21 | lr: 9.58e-04 | norm: 0.4927 | dt: 17173.8043ms | tok/sec: 22.8963\n",
      "step  950 | train loss: 4.33 | val loss: 4.28 | perplexity: 72.02 | lr: 9.57e-04 | norm: 0.4815 | dt: 17165.4842ms | tok/sec: 22.9074\n",
      "step  951 | train loss: 4.69 | val loss: 4.28 | perplexity: 72.31 | lr: 9.57e-04 | norm: 0.5210 | dt: 17132.5099ms | tok/sec: 22.9515\n",
      "step  952 | train loss: 4.61 | val loss: 4.29 | perplexity: 72.93 | lr: 9.57e-04 | norm: 0.5873 | dt: 17125.4554ms | tok/sec: 22.9609\n",
      "step  953 | train loss: 4.80 | val loss: 4.30 | perplexity: 73.76 | lr: 9.56e-04 | norm: 0.5732 | dt: 17144.9974ms | tok/sec: 22.9347\n",
      "step  954 | train loss: 4.45 | val loss: 4.28 | perplexity: 72.36 | lr: 9.56e-04 | norm: 0.5408 | dt: 17180.5253ms | tok/sec: 22.8873\n",
      "step  955 | train loss: 4.36 | val loss: 4.27 | perplexity: 71.78 | lr: 9.56e-04 | norm: 0.4880 | dt: 17152.3631ms | tok/sec: 22.9249\n",
      "step  956 | train loss: 4.42 | val loss: 4.28 | perplexity: 71.93 | lr: 9.55e-04 | norm: 0.4708 | dt: 17138.3674ms | tok/sec: 22.9436\n",
      "step  957 | train loss: 4.86 | val loss: 4.29 | perplexity: 72.62 | lr: 9.55e-04 | norm: 0.5244 | dt: 17096.1456ms | tok/sec: 23.0003\n",
      "step  958 | train loss: 4.31 | val loss: 4.27 | perplexity: 71.85 | lr: 9.55e-04 | norm: 0.6017 | dt: 17139.1883ms | tok/sec: 22.9425\n",
      "step  959 | train loss: 4.39 | val loss: 4.26 | perplexity: 70.76 | lr: 9.54e-04 | norm: 0.6224 | dt: 17122.2763ms | tok/sec: 22.9652\n",
      "step  960 | train loss: 4.30 | val loss: 4.26 | perplexity: 70.63 | lr: 9.54e-04 | norm: 0.4626 | dt: 17257.9622ms | tok/sec: 22.7846\n",
      "step  961 | train loss: 4.49 | val loss: 4.25 | perplexity: 70.35 | lr: 9.53e-04 | norm: 0.5346 | dt: 17118.2437ms | tok/sec: 22.9706\n",
      "step  962 | train loss: 4.31 | val loss: 4.25 | perplexity: 70.25 | lr: 9.53e-04 | norm: 0.5626 | dt: 17100.4426ms | tok/sec: 22.9945\n",
      "step  963 | train loss: 4.41 | val loss: 4.25 | perplexity: 70.06 | lr: 9.53e-04 | norm: 0.5145 | dt: 17178.1871ms | tok/sec: 22.8904\n",
      "step  964 | train loss: 4.34 | val loss: 4.25 | perplexity: 70.21 | lr: 9.52e-04 | norm: 0.4596 | dt: 17123.6980ms | tok/sec: 22.9633\n",
      "step  965 | train loss: 4.55 | val loss: 4.25 | perplexity: 70.44 | lr: 9.52e-04 | norm: 0.5586 | dt: 17153.7988ms | tok/sec: 22.9230\n",
      "step  966 | train loss: 4.41 | val loss: 4.25 | perplexity: 70.14 | lr: 9.52e-04 | norm: 0.5142 | dt: 17570.1520ms | tok/sec: 22.3798\n",
      "step  967 | train loss: 4.17 | val loss: 4.25 | perplexity: 70.21 | lr: 9.51e-04 | norm: 0.5004 | dt: 17130.7406ms | tok/sec: 22.9538\n",
      "step  968 | train loss: 4.73 | val loss: 4.26 | perplexity: 70.66 | lr: 9.51e-04 | norm: 0.5682 | dt: 17198.8542ms | tok/sec: 22.8629\n",
      "step  969 | train loss: 4.02 | val loss: 4.25 | perplexity: 70.20 | lr: 9.50e-04 | norm: 0.5946 | dt: 17101.8701ms | tok/sec: 22.9926\n",
      "step  970 | train loss: 4.34 | val loss: 4.25 | perplexity: 70.28 | lr: 9.50e-04 | norm: 0.5813 | dt: 17104.6283ms | tok/sec: 22.9889\n",
      "step  971 | train loss: 4.42 | val loss: 4.26 | perplexity: 70.46 | lr: 9.50e-04 | norm: 0.5092 | dt: 17135.7598ms | tok/sec: 22.9471\n",
      "step  972 | train loss: 4.36 | val loss: 4.24 | perplexity: 69.66 | lr: 9.49e-04 | norm: 0.5813 | dt: 17281.7516ms | tok/sec: 22.7532\n",
      "step  973 | train loss: 4.55 | val loss: 4.25 | perplexity: 70.44 | lr: 9.49e-04 | norm: 0.4711 | dt: 17183.9924ms | tok/sec: 22.8827\n",
      "step  974 | train loss: 4.49 | val loss: 4.26 | perplexity: 70.46 | lr: 9.49e-04 | norm: 0.4814 | dt: 17103.9090ms | tok/sec: 22.9898\n",
      "step  975 | train loss: 4.29 | val loss: 4.26 | perplexity: 71.05 | lr: 9.48e-04 | norm: 0.4763 | dt: 17146.4264ms | tok/sec: 22.9328\n",
      "step  976 | train loss: 4.49 | val loss: 4.26 | perplexity: 70.94 | lr: 9.48e-04 | norm: 0.5209 | dt: 17122.7131ms | tok/sec: 22.9646\n",
      "step  977 | train loss: 4.37 | val loss: 4.25 | perplexity: 70.22 | lr: 9.47e-04 | norm: 0.4399 | dt: 17255.4531ms | tok/sec: 22.7879\n",
      "step  978 | train loss: 4.28 | val loss: 4.25 | perplexity: 69.89 | lr: 9.47e-04 | norm: 0.5336 | dt: 17055.4776ms | tok/sec: 23.0551\n",
      "step  979 | train loss: 4.27 | val loss: 4.24 | perplexity: 69.57 | lr: 9.47e-04 | norm: 0.4115 | dt: 17161.3834ms | tok/sec: 22.9128\n",
      "step  980 | train loss: 4.33 | val loss: 4.23 | perplexity: 69.05 | lr: 9.46e-04 | norm: 0.4928 | dt: 17172.6692ms | tok/sec: 22.8978\n",
      "step  981 | train loss: 4.20 | val loss: 4.24 | perplexity: 69.31 | lr: 9.46e-04 | norm: 0.6123 | dt: 17135.2792ms | tok/sec: 22.9477\n",
      "step  982 | train loss: 4.23 | val loss: 4.24 | perplexity: 69.22 | lr: 9.45e-04 | norm: 0.5159 | dt: 17202.2336ms | tok/sec: 22.8584\n",
      "step  983 | train loss: 4.54 | val loss: 4.24 | perplexity: 69.22 | lr: 9.45e-04 | norm: 0.4934 | dt: 17086.5750ms | tok/sec: 23.0132\n",
      "step  984 | train loss: 4.28 | val loss: 4.25 | perplexity: 69.81 | lr: 9.45e-04 | norm: 0.5540 | dt: 17146.2801ms | tok/sec: 22.9330\n",
      "step  985 | train loss: 4.38 | val loss: 4.24 | perplexity: 69.18 | lr: 9.44e-04 | norm: 0.5352 | dt: 17152.0424ms | tok/sec: 22.9253\n",
      "step  986 | train loss: 4.31 | val loss: 4.23 | perplexity: 68.61 | lr: 9.44e-04 | norm: 0.4908 | dt: 17130.2795ms | tok/sec: 22.9544\n",
      "step  987 | train loss: 4.34 | val loss: 4.23 | perplexity: 68.39 | lr: 9.43e-04 | norm: 0.4017 | dt: 17151.9456ms | tok/sec: 22.9254\n",
      "step  988 | train loss: 4.07 | val loss: 4.23 | perplexity: 68.39 | lr: 9.43e-04 | norm: 0.4968 | dt: 17215.8031ms | tok/sec: 22.8404\n",
      "step  989 | train loss: 4.50 | val loss: 4.22 | perplexity: 67.98 | lr: 9.43e-04 | norm: 0.4690 | dt: 17122.8089ms | tok/sec: 22.9645\n",
      "step  990 | train loss: 4.45 | val loss: 4.22 | perplexity: 67.83 | lr: 9.42e-04 | norm: 0.5114 | dt: 17079.6292ms | tok/sec: 23.0225\n",
      "step  991 | train loss: 4.59 | val loss: 4.23 | perplexity: 68.38 | lr: 9.42e-04 | norm: 0.4013 | dt: 17150.9440ms | tok/sec: 22.9268\n",
      "step  992 | train loss: 4.62 | val loss: 4.23 | perplexity: 68.48 | lr: 9.41e-04 | norm: 0.4710 | dt: 17147.1939ms | tok/sec: 22.9318\n",
      "step  993 | train loss: 4.28 | val loss: 4.22 | perplexity: 67.97 | lr: 9.41e-04 | norm: 0.5221 | dt: 17242.0640ms | tok/sec: 22.8056\n",
      "step  994 | train loss: 4.31 | val loss: 4.22 | perplexity: 68.08 | lr: 9.40e-04 | norm: 0.4890 | dt: 17067.3857ms | tok/sec: 23.0390\n",
      "step  995 | train loss: 4.10 | val loss: 4.22 | perplexity: 67.77 | lr: 9.40e-04 | norm: 0.5358 | dt: 17124.8586ms | tok/sec: 22.9617\n",
      "step  996 | train loss: 4.33 | val loss: 4.21 | perplexity: 67.06 | lr: 9.40e-04 | norm: 0.5379 | dt: 17129.0884ms | tok/sec: 22.9560\n",
      "step  997 | train loss: 4.15 | val loss: 4.21 | perplexity: 67.27 | lr: 9.39e-04 | norm: 0.4665 | dt: 17115.1273ms | tok/sec: 22.9748\n",
      "step  998 | train loss: 4.55 | val loss: 4.20 | perplexity: 66.97 | lr: 9.39e-04 | norm: 0.5640 | dt: 17163.7459ms | tok/sec: 22.9097\n",
      "step  999 | train loss: 4.66 | val loss: 4.21 | perplexity: 67.50 | lr: 9.38e-04 | norm: 0.4430 | dt: 17114.9471ms | tok/sec: 22.9750\n",
      "step 1000 | train loss: 4.57 | val loss: 4.22 | perplexity: 67.91 | lr: 9.38e-04 | norm: 0.4953 | dt: 17103.6596ms | tok/sec: 22.9902\n",
      "step 1001 | train loss: 4.49 | val loss: 4.22 | perplexity: 67.76 | lr: 9.38e-04 | norm: 0.4793 | dt: 17171.1783ms | tok/sec: 22.8998\n",
      "step 1002 | train loss: 4.37 | val loss: 4.21 | perplexity: 67.16 | lr: 9.37e-04 | norm: 0.4686 | dt: 17161.8576ms | tok/sec: 22.9122\n",
      "step 1003 | train loss: 4.63 | val loss: 4.21 | perplexity: 67.62 | lr: 9.37e-04 | norm: 1.0741 | dt: 17146.9016ms | tok/sec: 22.9322\n",
      "step 1004 | train loss: 4.51 | val loss: 4.23 | perplexity: 68.48 | lr: 9.36e-04 | norm: 0.5655 | dt: 17120.7786ms | tok/sec: 22.9672\n",
      "step 1005 | train loss: 4.52 | val loss: 4.22 | perplexity: 68.18 | lr: 9.36e-04 | norm: 1.0235 | dt: 17088.3892ms | tok/sec: 23.0107\n",
      "step 1006 | train loss: 4.50 | val loss: 4.23 | perplexity: 68.64 | lr: 9.35e-04 | norm: 0.6855 | dt: 17121.8603ms | tok/sec: 22.9657\n",
      "step 1007 | train loss: 4.57 | val loss: 4.22 | perplexity: 68.18 | lr: 9.35e-04 | norm: 0.6528 | dt: 17134.4259ms | tok/sec: 22.9489\n",
      "step 1008 | train loss: 4.26 | val loss: 4.21 | perplexity: 67.35 | lr: 9.35e-04 | norm: 0.4940 | dt: 17126.5063ms | tok/sec: 22.9595\n",
      "step 1009 | train loss: 4.53 | val loss: 4.21 | perplexity: 67.12 | lr: 9.34e-04 | norm: 0.5426 | dt: 17080.4605ms | tok/sec: 23.0214\n",
      "step 1010 | train loss: 4.46 | val loss: 4.21 | perplexity: 67.24 | lr: 9.34e-04 | norm: 0.4321 | dt: 17145.4449ms | tok/sec: 22.9341\n",
      "step 1011 | train loss: 4.26 | val loss: 4.20 | perplexity: 66.47 | lr: 9.33e-04 | norm: 0.5183 | dt: 17170.1007ms | tok/sec: 22.9012\n",
      "step 1012 | train loss: 4.29 | val loss: 4.19 | perplexity: 66.20 | lr: 9.33e-04 | norm: 0.4709 | dt: 17358.1028ms | tok/sec: 22.6532\n",
      "step 1013 | train loss: 4.13 | val loss: 4.19 | perplexity: 66.03 | lr: 9.32e-04 | norm: 0.5275 | dt: 17127.4676ms | tok/sec: 22.9582\n",
      "step 1014 | train loss: 4.45 | val loss: 4.20 | perplexity: 66.49 | lr: 9.32e-04 | norm: 0.4865 | dt: 17092.6178ms | tok/sec: 23.0050\n",
      "step 1015 | train loss: 4.43 | val loss: 4.20 | perplexity: 66.36 | lr: 9.31e-04 | norm: 0.5771 | dt: 17147.3753ms | tok/sec: 22.9316\n",
      "step 1016 | train loss: 4.17 | val loss: 4.19 | perplexity: 65.96 | lr: 9.31e-04 | norm: 0.6096 | dt: 17158.5040ms | tok/sec: 22.9167\n",
      "step 1017 | train loss: 4.17 | val loss: 4.19 | perplexity: 65.90 | lr: 9.31e-04 | norm: 0.5667 | dt: 17180.8896ms | tok/sec: 22.8868\n",
      "step 1018 | train loss: 4.50 | val loss: 4.20 | perplexity: 66.43 | lr: 9.30e-04 | norm: 0.4647 | dt: 17158.1779ms | tok/sec: 22.9171\n",
      "step 1019 | train loss: 4.23 | val loss: 4.19 | perplexity: 65.94 | lr: 9.30e-04 | norm: 0.5623 | dt: 17168.7474ms | tok/sec: 22.9030\n",
      "step 1020 | train loss: 4.25 | val loss: 4.19 | perplexity: 65.77 | lr: 9.29e-04 | norm: 0.4604 | dt: 17103.7612ms | tok/sec: 22.9900\n",
      "step 1021 | train loss: 4.35 | val loss: 4.18 | perplexity: 65.47 | lr: 9.29e-04 | norm: 0.5626 | dt: 17161.3097ms | tok/sec: 22.9129\n",
      "step 1022 | train loss: 4.22 | val loss: 4.18 | perplexity: 65.28 | lr: 9.28e-04 | norm: 0.4539 | dt: 17097.0967ms | tok/sec: 22.9990\n",
      "step 1023 | train loss: 4.61 | val loss: 4.18 | perplexity: 65.30 | lr: 9.28e-04 | norm: 0.6659 | dt: 17133.1749ms | tok/sec: 22.9506\n",
      "step 1024 | train loss: 4.37 | val loss: 4.18 | perplexity: 65.17 | lr: 9.27e-04 | norm: 0.3984 | dt: 17096.0829ms | tok/sec: 23.0004\n",
      "step 1025 | train loss: 4.40 | val loss: 4.17 | perplexity: 64.79 | lr: 9.27e-04 | norm: 0.4772 | dt: 17203.0199ms | tok/sec: 22.8574\n",
      "step 1026 | train loss: 4.45 | val loss: 4.18 | perplexity: 65.27 | lr: 9.26e-04 | norm: 0.5424 | dt: 17112.5734ms | tok/sec: 22.9782\n",
      "step 1027 | train loss: 4.39 | val loss: 4.18 | perplexity: 65.60 | lr: 9.26e-04 | norm: 0.5453 | dt: 17169.4164ms | tok/sec: 22.9021\n",
      "step 1028 | train loss: 4.16 | val loss: 4.18 | perplexity: 65.05 | lr: 9.26e-04 | norm: 0.5094 | dt: 17278.0483ms | tok/sec: 22.7581\n",
      "step 1029 | train loss: 4.27 | val loss: 4.17 | perplexity: 64.71 | lr: 9.25e-04 | norm: 0.4525 | dt: 17144.2156ms | tok/sec: 22.9358\n",
      "step 1030 | train loss: 4.48 | val loss: 4.18 | perplexity: 65.56 | lr: 9.25e-04 | norm: 0.6631 | dt: 17185.1490ms | tok/sec: 22.8812\n",
      "step 1031 | train loss: 4.24 | val loss: 4.18 | perplexity: 65.31 | lr: 9.24e-04 | norm: 0.4812 | dt: 17136.6954ms | tok/sec: 22.9458\n",
      "step 1032 | train loss: 4.28 | val loss: 4.17 | perplexity: 64.93 | lr: 9.24e-04 | norm: 0.4360 | dt: 17160.1431ms | tok/sec: 22.9145\n",
      "step 1033 | train loss: 4.47 | val loss: 4.17 | perplexity: 64.57 | lr: 9.23e-04 | norm: 0.4159 | dt: 17107.9988ms | tok/sec: 22.9843\n",
      "step 1034 | train loss: 4.30 | val loss: 4.17 | perplexity: 64.46 | lr: 9.23e-04 | norm: 0.3881 | dt: 17131.9058ms | tok/sec: 22.9523\n",
      "step 1035 | train loss: 4.30 | val loss: 4.16 | perplexity: 64.38 | lr: 9.22e-04 | norm: 0.4160 | dt: 17161.3185ms | tok/sec: 22.9129\n",
      "step 1036 | train loss: 4.12 | val loss: 4.17 | perplexity: 64.44 | lr: 9.22e-04 | norm: 0.5006 | dt: 17139.4880ms | tok/sec: 22.9421\n",
      "step 1037 | train loss: 4.26 | val loss: 4.16 | perplexity: 64.01 | lr: 9.21e-04 | norm: 0.4093 | dt: 17158.4802ms | tok/sec: 22.9167\n",
      "step 1038 | train loss: 4.44 | val loss: 4.16 | perplexity: 64.21 | lr: 9.21e-04 | norm: 0.3569 | dt: 17076.1309ms | tok/sec: 23.0272\n",
      "step 1039 | train loss: 4.25 | val loss: 4.16 | perplexity: 64.35 | lr: 9.20e-04 | norm: 0.4397 | dt: 17130.8749ms | tok/sec: 22.9536\n",
      "step 1040 | train loss: 4.26 | val loss: 4.16 | perplexity: 63.90 | lr: 9.20e-04 | norm: 0.5264 | dt: 17128.4056ms | tok/sec: 22.9570\n",
      "step 1041 | train loss: 4.06 | val loss: 4.16 | perplexity: 63.80 | lr: 9.19e-04 | norm: 0.4138 | dt: 17364.6250ms | tok/sec: 22.6447\n",
      "step 1042 | train loss: 4.45 | val loss: 4.16 | perplexity: 64.02 | lr: 9.19e-04 | norm: 0.4829 | dt: 17202.8308ms | tok/sec: 22.8576\n",
      "step 1043 | train loss: 4.45 | val loss: 4.16 | perplexity: 64.14 | lr: 9.18e-04 | norm: 0.6694 | dt: 17130.3577ms | tok/sec: 22.9543\n",
      "step 1044 | train loss: 4.40 | val loss: 4.16 | perplexity: 64.30 | lr: 9.18e-04 | norm: 0.4486 | dt: 17073.7514ms | tok/sec: 23.0304\n",
      "step 1045 | train loss: 4.29 | val loss: 4.16 | perplexity: 64.17 | lr: 9.17e-04 | norm: 0.5561 | dt: 17109.8368ms | tok/sec: 22.9819\n",
      "step 1046 | train loss: 4.15 | val loss: 4.15 | perplexity: 63.72 | lr: 9.17e-04 | norm: 0.4668 | dt: 17142.2544ms | tok/sec: 22.9384\n",
      "step 1047 | train loss: 4.29 | val loss: 4.16 | perplexity: 63.87 | lr: 9.17e-04 | norm: 0.4284 | dt: 17133.0085ms | tok/sec: 22.9508\n",
      "step 1048 | train loss: 4.27 | val loss: 4.15 | perplexity: 63.45 | lr: 9.16e-04 | norm: 0.4515 | dt: 17158.2906ms | tok/sec: 22.9170\n",
      "step 1049 | train loss: 3.92 | val loss: 4.15 | perplexity: 63.43 | lr: 9.16e-04 | norm: 0.4692 | dt: 17134.0632ms | tok/sec: 22.9494\n",
      "step 1050 | train loss: 4.22 | val loss: 4.16 | perplexity: 64.12 | lr: 9.15e-04 | norm: 0.4367 | dt: 17106.5614ms | tok/sec: 22.9863\n",
      "step 1051 | train loss: 4.53 | val loss: 4.16 | perplexity: 64.38 | lr: 9.15e-04 | norm: 0.5572 | dt: 17108.1073ms | tok/sec: 22.9842\n",
      "step 1052 | train loss: 4.38 | val loss: 4.16 | perplexity: 63.77 | lr: 9.14e-04 | norm: 0.4953 | dt: 17127.8529ms | tok/sec: 22.9577\n",
      "step 1053 | train loss: 4.16 | val loss: 4.16 | perplexity: 64.18 | lr: 9.14e-04 | norm: 0.5028 | dt: 17183.1274ms | tok/sec: 22.8838\n",
      "step 1054 | train loss: 4.42 | val loss: 4.16 | perplexity: 63.95 | lr: 9.13e-04 | norm: 0.5175 | dt: 17110.0147ms | tok/sec: 22.9816\n",
      "step 1055 | train loss: 4.50 | val loss: 4.16 | perplexity: 64.19 | lr: 9.13e-04 | norm: 0.5153 | dt: 17157.4829ms | tok/sec: 22.9180\n",
      "step 1056 | train loss: 4.78 | val loss: 4.17 | perplexity: 64.86 | lr: 9.12e-04 | norm: 0.5386 | dt: 17120.2445ms | tok/sec: 22.9679\n",
      "step 1057 | train loss: 4.30 | val loss: 4.16 | perplexity: 63.80 | lr: 9.12e-04 | norm: 0.4879 | dt: 17136.0288ms | tok/sec: 22.9467\n",
      "step 1058 | train loss: 4.06 | val loss: 4.15 | perplexity: 63.66 | lr: 9.11e-04 | norm: 0.4258 | dt: 17409.0049ms | tok/sec: 22.5869\n",
      "step 1059 | train loss: 4.45 | val loss: 4.14 | perplexity: 63.05 | lr: 9.11e-04 | norm: 0.5538 | dt: 17176.6431ms | tok/sec: 22.8925\n",
      "step 1060 | train loss: 4.46 | val loss: 4.15 | perplexity: 63.46 | lr: 9.10e-04 | norm: 0.4558 | dt: 17438.4451ms | tok/sec: 22.5488\n",
      "step 1061 | train loss: 4.31 | val loss: 4.14 | perplexity: 63.05 | lr: 9.10e-04 | norm: 0.4942 | dt: 17125.5434ms | tok/sec: 22.9608\n",
      "step 1062 | train loss: 4.21 | val loss: 4.15 | perplexity: 63.25 | lr: 9.09e-04 | norm: 0.4776 | dt: 17077.0206ms | tok/sec: 23.0260\n",
      "step 1063 | train loss: 4.53 | val loss: 4.14 | perplexity: 62.89 | lr: 9.09e-04 | norm: 0.5128 | dt: 17165.4961ms | tok/sec: 22.9073\n",
      "step 1064 | train loss: 4.45 | val loss: 4.14 | perplexity: 62.60 | lr: 9.08e-04 | norm: 0.4348 | dt: 17139.7521ms | tok/sec: 22.9418\n",
      "step 1065 | train loss: 4.27 | val loss: 4.13 | perplexity: 62.37 | lr: 9.08e-04 | norm: 0.4789 | dt: 17121.3217ms | tok/sec: 22.9665\n",
      "step 1066 | train loss: 4.23 | val loss: 4.13 | perplexity: 62.35 | lr: 9.07e-04 | norm: 0.4197 | dt: 17153.5025ms | tok/sec: 22.9234\n",
      "step 1067 | train loss: 4.39 | val loss: 4.13 | perplexity: 61.98 | lr: 9.07e-04 | norm: 0.4581 | dt: 17186.6345ms | tok/sec: 22.8792\n",
      "step 1068 | train loss: 4.14 | val loss: 4.13 | perplexity: 62.35 | lr: 9.06e-04 | norm: 0.4607 | dt: 17109.7662ms | tok/sec: 22.9820\n",
      "step 1069 | train loss: 4.31 | val loss: 4.13 | perplexity: 61.90 | lr: 9.05e-04 | norm: 0.4642 | dt: 17167.3958ms | tok/sec: 22.9048\n",
      "step 1070 | train loss: 4.12 | val loss: 4.13 | perplexity: 62.39 | lr: 9.05e-04 | norm: 0.4869 | dt: 17076.9260ms | tok/sec: 23.0262\n",
      "step 1071 | train loss: 4.41 | val loss: 4.13 | perplexity: 62.42 | lr: 9.04e-04 | norm: 0.5366 | dt: 17084.1174ms | tok/sec: 23.0165\n",
      "step 1072 | train loss: 4.41 | val loss: 4.13 | perplexity: 62.47 | lr: 9.04e-04 | norm: 0.4534 | dt: 17116.1666ms | tok/sec: 22.9734\n",
      "step 1073 | train loss: 4.31 | val loss: 4.12 | perplexity: 61.80 | lr: 9.03e-04 | norm: 0.4759 | dt: 17207.1235ms | tok/sec: 22.8519\n",
      "step 1074 | train loss: 4.47 | val loss: 4.12 | perplexity: 61.82 | lr: 9.03e-04 | norm: 0.4594 | dt: 17104.9292ms | tok/sec: 22.9885\n",
      "step 1075 | train loss: 4.31 | val loss: 4.13 | perplexity: 62.21 | lr: 9.02e-04 | norm: 0.4452 | dt: 17221.6532ms | tok/sec: 22.8327\n",
      "step 1076 | train loss: 4.23 | val loss: 4.12 | perplexity: 61.74 | lr: 9.02e-04 | norm: 0.4842 | dt: 17066.4518ms | tok/sec: 23.0403\n",
      "step 1077 | train loss: 4.25 | val loss: 4.12 | perplexity: 61.33 | lr: 9.01e-04 | norm: 0.4167 | dt: 17151.7379ms | tok/sec: 22.9257\n",
      "step 1078 | train loss: 4.07 | val loss: 4.11 | perplexity: 61.14 | lr: 9.01e-04 | norm: 0.4001 | dt: 17168.5467ms | tok/sec: 22.9033\n",
      "step 1079 | train loss: 4.28 | val loss: 4.11 | perplexity: 60.92 | lr: 9.00e-04 | norm: 0.4970 | dt: 17164.5114ms | tok/sec: 22.9087\n",
      "step 1080 | train loss: 4.48 | val loss: 4.12 | perplexity: 61.33 | lr: 9.00e-04 | norm: 0.5162 | dt: 17159.6596ms | tok/sec: 22.9151\n",
      "step 1081 | train loss: 4.11 | val loss: 4.13 | perplexity: 62.06 | lr: 8.99e-04 | norm: 0.5649 | dt: 17123.9438ms | tok/sec: 22.9629\n",
      "step 1082 | train loss: 4.38 | val loss: 4.13 | perplexity: 62.25 | lr: 8.99e-04 | norm: 0.5763 | dt: 17140.2380ms | tok/sec: 22.9411\n",
      "step 1083 | train loss: 4.29 | val loss: 4.13 | perplexity: 62.00 | lr: 8.98e-04 | norm: 0.4430 | dt: 17153.8360ms | tok/sec: 22.9229\n",
      "step 1084 | train loss: 4.46 | val loss: 4.12 | perplexity: 61.62 | lr: 8.98e-04 | norm: 0.5109 | dt: 17056.2418ms | tok/sec: 23.0541\n",
      "step 1085 | train loss: 4.20 | val loss: 4.12 | perplexity: 61.38 | lr: 8.97e-04 | norm: 0.5561 | dt: 17134.8743ms | tok/sec: 22.9483\n",
      "step 1086 | train loss: 4.08 | val loss: 4.12 | perplexity: 61.72 | lr: 8.97e-04 | norm: 0.5509 | dt: 17115.6130ms | tok/sec: 22.9741\n",
      "step 1087 | train loss: 4.24 | val loss: 4.12 | perplexity: 61.82 | lr: 8.96e-04 | norm: 0.5169 | dt: 17154.3403ms | tok/sec: 22.9222\n",
      "step 1088 | train loss: 4.57 | val loss: 4.13 | perplexity: 62.46 | lr: 8.95e-04 | norm: 0.4475 | dt: 17147.0616ms | tok/sec: 22.9320\n",
      "step 1089 | train loss: 4.37 | val loss: 4.13 | perplexity: 62.05 | lr: 8.95e-04 | norm: 0.4572 | dt: 17263.1886ms | tok/sec: 22.7777\n",
      "step 1090 | train loss: 4.29 | val loss: 4.12 | perplexity: 61.56 | lr: 8.94e-04 | norm: 0.5359 | dt: 17096.1909ms | tok/sec: 23.0002\n",
      "step 1091 | train loss: 4.16 | val loss: 4.12 | perplexity: 61.48 | lr: 8.94e-04 | norm: 0.4111 | dt: 17116.7238ms | tok/sec: 22.9726\n",
      "step 1092 | train loss: 4.51 | val loss: 4.12 | perplexity: 61.54 | lr: 8.93e-04 | norm: 0.4773 | dt: 17139.7028ms | tok/sec: 22.9418\n",
      "step 1093 | train loss: 4.20 | val loss: 4.11 | perplexity: 60.97 | lr: 8.93e-04 | norm: 0.5193 | dt: 17134.5417ms | tok/sec: 22.9487\n",
      "step 1094 | train loss: 4.20 | val loss: 4.11 | perplexity: 60.88 | lr: 8.92e-04 | norm: 0.4149 | dt: 17075.8710ms | tok/sec: 23.0276\n",
      "step 1095 | train loss: 4.29 | val loss: 4.11 | perplexity: 61.09 | lr: 8.92e-04 | norm: 0.4293 | dt: 17050.3137ms | tok/sec: 23.0621\n",
      "step 1096 | train loss: 4.07 | val loss: 4.11 | perplexity: 60.77 | lr: 8.91e-04 | norm: 0.4505 | dt: 17181.6792ms | tok/sec: 22.8858\n",
      "step 1097 | train loss: 4.10 | val loss: 4.10 | perplexity: 60.63 | lr: 8.91e-04 | norm: 0.4098 | dt: 17108.7835ms | tok/sec: 22.9833\n",
      "step 1098 | train loss: 4.57 | val loss: 4.10 | perplexity: 60.57 | lr: 8.90e-04 | norm: 0.5800 | dt: 17154.4375ms | tok/sec: 22.9221\n",
      "step 1099 | train loss: 4.16 | val loss: 4.10 | perplexity: 60.29 | lr: 8.90e-04 | norm: 0.4582 | dt: 17180.8691ms | tok/sec: 22.8869\n",
      "step 1100 | train loss: 4.39 | val loss: 4.10 | perplexity: 60.19 | lr: 8.89e-04 | norm: 0.5067 | dt: 17114.6696ms | tok/sec: 22.9754\n",
      "step 1101 | train loss: 4.42 | val loss: 4.10 | perplexity: 60.07 | lr: 8.88e-04 | norm: 0.4897 | dt: 17109.5738ms | tok/sec: 22.9822\n",
      "step 1102 | train loss: 4.13 | val loss: 4.09 | perplexity: 59.87 | lr: 8.88e-04 | norm: 0.3903 | dt: 17124.4586ms | tok/sec: 22.9622\n",
      "step 1103 | train loss: 4.18 | val loss: 4.09 | perplexity: 59.75 | lr: 8.87e-04 | norm: 0.4211 | dt: 17086.0500ms | tok/sec: 23.0139\n",
      "step 1104 | train loss: 4.53 | val loss: 4.09 | perplexity: 59.92 | lr: 8.87e-04 | norm: 0.4642 | dt: 17122.1404ms | tok/sec: 22.9654\n",
      "step 1105 | train loss: 4.32 | val loss: 4.09 | perplexity: 59.90 | lr: 8.86e-04 | norm: 0.4153 | dt: 17201.3237ms | tok/sec: 22.8596\n",
      "step 1106 | train loss: 4.17 | val loss: 4.09 | perplexity: 59.58 | lr: 8.86e-04 | norm: 0.4287 | dt: 17094.2612ms | tok/sec: 23.0028\n",
      "step 1107 | train loss: 4.31 | val loss: 4.09 | perplexity: 59.55 | lr: 8.85e-04 | norm: 0.4022 | dt: 17085.6287ms | tok/sec: 23.0144\n",
      "step 1108 | train loss: 4.11 | val loss: 4.09 | perplexity: 59.72 | lr: 8.85e-04 | norm: 0.4418 | dt: 17139.7216ms | tok/sec: 22.9418\n",
      "step 1109 | train loss: 4.28 | val loss: 4.08 | perplexity: 59.39 | lr: 8.84e-04 | norm: 0.4473 | dt: 17127.6803ms | tok/sec: 22.9579\n",
      "step 1110 | train loss: 4.43 | val loss: 4.09 | perplexity: 59.47 | lr: 8.83e-04 | norm: 0.4991 | dt: 17113.6401ms | tok/sec: 22.9768\n",
      "step 1111 | train loss: 4.03 | val loss: 4.08 | perplexity: 58.89 | lr: 8.83e-04 | norm: 0.4895 | dt: 17119.4398ms | tok/sec: 22.9690\n",
      "step 1112 | train loss: 4.10 | val loss: 4.07 | perplexity: 58.81 | lr: 8.82e-04 | norm: 0.4377 | dt: 17098.2594ms | tok/sec: 22.9974\n",
      "step 1113 | train loss: 3.98 | val loss: 4.08 | perplexity: 58.86 | lr: 8.82e-04 | norm: 0.5616 | dt: 17127.9991ms | tok/sec: 22.9575\n",
      "step 1114 | train loss: 4.26 | val loss: 4.08 | perplexity: 59.00 | lr: 8.81e-04 | norm: 0.4167 | dt: 17773.3617ms | tok/sec: 22.1239\n",
      "step 1115 | train loss: 4.14 | val loss: 4.07 | perplexity: 58.71 | lr: 8.81e-04 | norm: 0.3896 | dt: 17089.3834ms | tok/sec: 23.0094\n",
      "step 1116 | train loss: 4.47 | val loss: 4.08 | perplexity: 59.04 | lr: 8.80e-04 | norm: 0.5118 | dt: 17073.9605ms | tok/sec: 23.0302\n",
      "step 1117 | train loss: 3.98 | val loss: 4.08 | perplexity: 59.02 | lr: 8.79e-04 | norm: 0.4698 | dt: 17071.6214ms | tok/sec: 23.0333\n",
      "step 1118 | train loss: 4.00 | val loss: 4.07 | perplexity: 58.41 | lr: 8.79e-04 | norm: 0.4827 | dt: 17120.6105ms | tok/sec: 22.9674\n",
      "step 1119 | train loss: 4.25 | val loss: 4.07 | perplexity: 58.30 | lr: 8.78e-04 | norm: 0.4655 | dt: 17094.3809ms | tok/sec: 23.0026\n",
      "step 1120 | train loss: 4.32 | val loss: 4.08 | perplexity: 59.09 | lr: 8.78e-04 | norm: 0.4686 | dt: 17148.1159ms | tok/sec: 22.9306\n",
      "step 1121 | train loss: 4.30 | val loss: 4.08 | perplexity: 59.44 | lr: 8.77e-04 | norm: 0.7353 | dt: 17135.2768ms | tok/sec: 22.9477\n",
      "step 1122 | train loss: 3.87 | val loss: 4.08 | perplexity: 59.29 | lr: 8.77e-04 | norm: 0.4627 | dt: 17174.7987ms | tok/sec: 22.8949\n",
      "step 1123 | train loss: 4.27 | val loss: 4.08 | perplexity: 59.26 | lr: 8.76e-04 | norm: 0.5331 | dt: 17124.2213ms | tok/sec: 22.9626\n",
      "step 1124 | train loss: 4.20 | val loss: 4.08 | perplexity: 58.90 | lr: 8.75e-04 | norm: 0.5750 | dt: 17093.7314ms | tok/sec: 23.0035\n",
      "step 1125 | train loss: 4.10 | val loss: 4.08 | perplexity: 59.37 | lr: 8.75e-04 | norm: 0.5111 | dt: 17138.7787ms | tok/sec: 22.9431\n",
      "step 1126 | train loss: 4.20 | val loss: 4.08 | perplexity: 58.97 | lr: 8.74e-04 | norm: 0.6097 | dt: 17171.6480ms | tok/sec: 22.8991\n",
      "step 1127 | train loss: 4.19 | val loss: 4.08 | perplexity: 58.88 | lr: 8.74e-04 | norm: 0.5608 | dt: 17096.1764ms | tok/sec: 23.0002\n",
      "step 1128 | train loss: 4.09 | val loss: 4.07 | perplexity: 58.57 | lr: 8.73e-04 | norm: 0.4652 | dt: 17125.2973ms | tok/sec: 22.9611\n",
      "step 1129 | train loss: 4.32 | val loss: 4.07 | perplexity: 58.40 | lr: 8.72e-04 | norm: 0.5199 | dt: 17207.3407ms | tok/sec: 22.8516\n",
      "step 1130 | train loss: 4.47 | val loss: 4.06 | perplexity: 57.87 | lr: 8.72e-04 | norm: 0.4782 | dt: 17170.9886ms | tok/sec: 22.9000\n",
      "step 1131 | train loss: 4.25 | val loss: 4.06 | perplexity: 57.71 | lr: 8.71e-04 | norm: 0.3635 | dt: 17140.0058ms | tok/sec: 22.9414\n",
      "step 1132 | train loss: 4.03 | val loss: 4.06 | perplexity: 57.78 | lr: 8.71e-04 | norm: 0.4986 | dt: 17155.3288ms | tok/sec: 22.9209\n",
      "step 1133 | train loss: 4.14 | val loss: 4.05 | perplexity: 57.61 | lr: 8.70e-04 | norm: 0.4582 | dt: 17140.0988ms | tok/sec: 22.9413\n",
      "step 1134 | train loss: 4.41 | val loss: 4.06 | perplexity: 57.94 | lr: 8.70e-04 | norm: 0.4758 | dt: 17199.8265ms | tok/sec: 22.8616\n",
      "step 1135 | train loss: 4.04 | val loss: 4.06 | perplexity: 57.89 | lr: 8.69e-04 | norm: 0.5745 | dt: 17301.6577ms | tok/sec: 22.7271\n",
      "step 1136 | train loss: 4.10 | val loss: 4.06 | perplexity: 58.00 | lr: 8.68e-04 | norm: 0.4861 | dt: 17126.6611ms | tok/sec: 22.9593\n",
      "step 1137 | train loss: 4.23 | val loss: 4.07 | perplexity: 58.82 | lr: 8.68e-04 | norm: 0.5666 | dt: 17166.1201ms | tok/sec: 22.9065\n",
      "step 1138 | train loss: 4.18 | val loss: 4.06 | perplexity: 58.24 | lr: 8.67e-04 | norm: 0.5486 | dt: 17155.8542ms | tok/sec: 22.9202\n",
      "step 1139 | train loss: 4.31 | val loss: 4.06 | perplexity: 57.80 | lr: 8.67e-04 | norm: 0.4667 | dt: 17144.7170ms | tok/sec: 22.9351\n",
      "step 1140 | train loss: 4.44 | val loss: 4.06 | perplexity: 58.00 | lr: 8.66e-04 | norm: 0.5102 | dt: 17157.2297ms | tok/sec: 22.9184\n",
      "step 1141 | train loss: 4.08 | val loss: 4.05 | perplexity: 57.55 | lr: 8.65e-04 | norm: 0.6330 | dt: 17157.0165ms | tok/sec: 22.9187\n",
      "step 1142 | train loss: 3.95 | val loss: 4.05 | perplexity: 57.20 | lr: 8.65e-04 | norm: 0.4978 | dt: 17168.2179ms | tok/sec: 22.9037\n",
      "step 1143 | train loss: 4.30 | val loss: 4.05 | perplexity: 57.43 | lr: 8.64e-04 | norm: 0.3917 | dt: 17194.2050ms | tok/sec: 22.8691\n",
      "step 1144 | train loss: 3.89 | val loss: 4.05 | perplexity: 57.45 | lr: 8.64e-04 | norm: 0.4636 | dt: 17187.5830ms | tok/sec: 22.8779\n",
      "step 1145 | train loss: 4.24 | val loss: 4.05 | perplexity: 57.19 | lr: 8.63e-04 | norm: 0.4492 | dt: 17109.2696ms | tok/sec: 22.9826\n",
      "step 1146 | train loss: 4.26 | val loss: 4.05 | perplexity: 57.15 | lr: 8.62e-04 | norm: 0.3786 | dt: 17161.9260ms | tok/sec: 22.9121\n",
      "step 1147 | train loss: 4.30 | val loss: 4.04 | perplexity: 57.08 | lr: 8.62e-04 | norm: 0.4499 | dt: 17126.1506ms | tok/sec: 22.9600\n",
      "step 1148 | train loss: 4.11 | val loss: 4.05 | perplexity: 57.26 | lr: 8.61e-04 | norm: 0.3988 | dt: 17139.4603ms | tok/sec: 22.9421\n",
      "step 1149 | train loss: 4.11 | val loss: 4.04 | perplexity: 56.94 | lr: 8.61e-04 | norm: 0.4720 | dt: 17090.9693ms | tok/sec: 23.0072\n",
      "step 1150 | train loss: 4.04 | val loss: 4.04 | perplexity: 56.75 | lr: 8.60e-04 | norm: 0.4292 | dt: 17130.0521ms | tok/sec: 22.9547\n",
      "step 1151 | train loss: 4.07 | val loss: 4.04 | perplexity: 56.74 | lr: 8.59e-04 | norm: 0.4523 | dt: 17082.6170ms | tok/sec: 23.0185\n",
      "step 1152 | train loss: 4.33 | val loss: 4.04 | perplexity: 56.82 | lr: 8.59e-04 | norm: 0.4646 | dt: 17070.3740ms | tok/sec: 23.0350\n",
      "step 1153 | train loss: 4.12 | val loss: 4.05 | perplexity: 57.18 | lr: 8.58e-04 | norm: 0.4630 | dt: 17138.2394ms | tok/sec: 22.9438\n",
      "step 1154 | train loss: 4.20 | val loss: 4.04 | perplexity: 57.03 | lr: 8.57e-04 | norm: 0.4572 | dt: 17114.9948ms | tok/sec: 22.9749\n",
      "step 1155 | train loss: 4.24 | val loss: 4.04 | perplexity: 56.86 | lr: 8.57e-04 | norm: 0.5738 | dt: 17120.5509ms | tok/sec: 22.9675\n",
      "step 1156 | train loss: 4.32 | val loss: 4.04 | perplexity: 57.04 | lr: 8.56e-04 | norm: 0.3826 | dt: 17175.8182ms | tok/sec: 22.8936\n",
      "step 1157 | train loss: 4.20 | val loss: 4.03 | perplexity: 56.48 | lr: 8.56e-04 | norm: 0.4083 | dt: 17114.9936ms | tok/sec: 22.9749\n",
      "step 1158 | train loss: 4.29 | val loss: 4.03 | perplexity: 56.37 | lr: 8.55e-04 | norm: 0.4768 | dt: 17188.5438ms | tok/sec: 22.8766\n",
      "step 1159 | train loss: 4.25 | val loss: 4.03 | perplexity: 56.02 | lr: 8.54e-04 | norm: 0.5151 | dt: 17195.7989ms | tok/sec: 22.8670\n",
      "step 1160 | train loss: 4.17 | val loss: 4.03 | perplexity: 56.22 | lr: 8.54e-04 | norm: 0.4624 | dt: 17150.4369ms | tok/sec: 22.9275\n",
      "step 1161 | train loss: 4.39 | val loss: 4.03 | perplexity: 56.51 | lr: 8.53e-04 | norm: 0.4483 | dt: 17249.7082ms | tok/sec: 22.7955\n",
      "step 1162 | train loss: 4.29 | val loss: 4.03 | perplexity: 56.46 | lr: 8.53e-04 | norm: 0.4262 | dt: 17187.2740ms | tok/sec: 22.8783\n",
      "step 1163 | train loss: 4.11 | val loss: 4.03 | perplexity: 56.21 | lr: 8.52e-04 | norm: 0.4548 | dt: 17143.6114ms | tok/sec: 22.9366\n",
      "step 1164 | train loss: 4.08 | val loss: 4.03 | perplexity: 56.13 | lr: 8.51e-04 | norm: 0.4608 | dt: 17110.2481ms | tok/sec: 22.9813\n",
      "step 1165 | train loss: 3.95 | val loss: 4.03 | perplexity: 56.00 | lr: 8.51e-04 | norm: 0.4497 | dt: 17099.3838ms | tok/sec: 22.9959\n",
      "step 1166 | train loss: 4.07 | val loss: 4.02 | perplexity: 55.88 | lr: 8.50e-04 | norm: 0.4386 | dt: 17147.9712ms | tok/sec: 22.9308\n",
      "step 1167 | train loss: 4.03 | val loss: 4.02 | perplexity: 55.80 | lr: 8.49e-04 | norm: 0.3916 | dt: 17339.0346ms | tok/sec: 22.6781\n",
      "step 1168 | train loss: 4.20 | val loss: 4.02 | perplexity: 55.63 | lr: 8.49e-04 | norm: 0.4463 | dt: 17105.4208ms | tok/sec: 22.9878\n",
      "step 1169 | train loss: 3.90 | val loss: 4.02 | perplexity: 55.49 | lr: 8.48e-04 | norm: 0.3327 | dt: 17124.1014ms | tok/sec: 22.9627\n",
      "step 1170 | train loss: 4.17 | val loss: 4.01 | perplexity: 55.31 | lr: 8.48e-04 | norm: 0.4167 | dt: 17146.5826ms | tok/sec: 22.9326\n",
      "step 1171 | train loss: 4.45 | val loss: 4.02 | perplexity: 55.59 | lr: 8.47e-04 | norm: 0.4550 | dt: 17154.0871ms | tok/sec: 22.9226\n",
      "step 1172 | train loss: 3.84 | val loss: 4.02 | perplexity: 55.64 | lr: 8.46e-04 | norm: 0.4721 | dt: 17117.0974ms | tok/sec: 22.9721\n",
      "step 1173 | train loss: 4.23 | val loss: 4.01 | perplexity: 55.38 | lr: 8.46e-04 | norm: 0.4586 | dt: 17133.4119ms | tok/sec: 22.9502\n",
      "step 1174 | train loss: 4.48 | val loss: 4.02 | perplexity: 55.52 | lr: 8.45e-04 | norm: 0.5390 | dt: 17182.5478ms | tok/sec: 22.8846\n",
      "step 1175 | train loss: 4.38 | val loss: 4.01 | perplexity: 55.30 | lr: 8.44e-04 | norm: 0.4705 | dt: 17175.2117ms | tok/sec: 22.8944\n",
      "step 1176 | train loss: 4.73 | val loss: 4.02 | perplexity: 55.70 | lr: 8.44e-04 | norm: 0.5684 | dt: 17099.1776ms | tok/sec: 22.9962\n",
      "step 1177 | train loss: 4.97 | val loss: 4.03 | perplexity: 56.22 | lr: 8.43e-04 | norm: 1.4681 | dt: 17150.1563ms | tok/sec: 22.9278\n",
      "step 1178 | train loss: 4.31 | val loss: 4.03 | perplexity: 56.35 | lr: 8.42e-04 | norm: 0.4812 | dt: 17142.5126ms | tok/sec: 22.9381\n",
      "step 1179 | train loss: 4.12 | val loss: 4.02 | perplexity: 55.98 | lr: 8.42e-04 | norm: 0.4694 | dt: 17171.0138ms | tok/sec: 22.9000\n",
      "step 1180 | train loss: 4.21 | val loss: 4.01 | perplexity: 55.08 | lr: 8.41e-04 | norm: 0.5048 | dt: 17162.9734ms | tok/sec: 22.9107\n",
      "step 1181 | train loss: 4.09 | val loss: 4.00 | perplexity: 54.63 | lr: 8.41e-04 | norm: 0.4458 | dt: 17116.2903ms | tok/sec: 22.9732\n",
      "step 1182 | train loss: 4.07 | val loss: 4.00 | perplexity: 54.85 | lr: 8.40e-04 | norm: 0.4643 | dt: 17150.3873ms | tok/sec: 22.9275\n",
      "step 1183 | train loss: 4.13 | val loss: 4.00 | perplexity: 54.74 | lr: 8.39e-04 | norm: 0.5910 | dt: 17191.6552ms | tok/sec: 22.8725\n",
      "step 1184 | train loss: 4.26 | val loss: 4.01 | perplexity: 55.10 | lr: 8.39e-04 | norm: 0.5804 | dt: 17161.3982ms | tok/sec: 22.9128\n",
      "step 1185 | train loss: 4.33 | val loss: 4.01 | perplexity: 55.31 | lr: 8.38e-04 | norm: 0.5137 | dt: 17154.6249ms | tok/sec: 22.9219\n",
      "step 1186 | train loss: 4.08 | val loss: 4.01 | perplexity: 55.04 | lr: 8.37e-04 | norm: 0.5506 | dt: 17147.1269ms | tok/sec: 22.9319\n",
      "step 1187 | train loss: 4.29 | val loss: 4.00 | perplexity: 54.85 | lr: 8.37e-04 | norm: 0.5307 | dt: 17110.0225ms | tok/sec: 22.9816\n",
      "step 1188 | train loss: 4.09 | val loss: 4.01 | perplexity: 54.88 | lr: 8.36e-04 | norm: 0.4805 | dt: 17160.8849ms | tok/sec: 22.9135\n",
      "step 1189 | train loss: 3.91 | val loss: 4.00 | perplexity: 54.62 | lr: 8.35e-04 | norm: 0.4505 | dt: 17175.2887ms | tok/sec: 22.8943\n",
      "step 1190 | train loss: 4.29 | val loss: 4.00 | perplexity: 54.55 | lr: 8.35e-04 | norm: 0.5020 | dt: 17143.1324ms | tok/sec: 22.9372\n",
      "step 1191 | train loss: 4.00 | val loss: 4.01 | perplexity: 55.33 | lr: 8.34e-04 | norm: 0.4638 | dt: 17184.2203ms | tok/sec: 22.8824\n",
      "step 1192 | train loss: 4.22 | val loss: 4.02 | perplexity: 55.47 | lr: 8.33e-04 | norm: 0.4742 | dt: 17198.7479ms | tok/sec: 22.8631\n",
      "step 1193 | train loss: 4.26 | val loss: 4.01 | perplexity: 55.06 | lr: 8.33e-04 | norm: 0.5007 | dt: 17173.2860ms | tok/sec: 22.8970\n",
      "step 1194 | train loss: 3.86 | val loss: 4.01 | perplexity: 55.09 | lr: 8.32e-04 | norm: 0.5404 | dt: 17165.3168ms | tok/sec: 22.9076\n",
      "step 1195 | train loss: 4.22 | val loss: 4.01 | perplexity: 54.90 | lr: 8.32e-04 | norm: 0.4943 | dt: 17109.4513ms | tok/sec: 22.9824\n",
      "step 1196 | train loss: 4.21 | val loss: 4.01 | perplexity: 54.97 | lr: 8.31e-04 | norm: 0.4757 | dt: 17181.9372ms | tok/sec: 22.8854\n",
      "step 1197 | train loss: 4.50 | val loss: 4.01 | perplexity: 54.92 | lr: 8.30e-04 | norm: 0.4765 | dt: 17191.9875ms | tok/sec: 22.8721\n",
      "step 1198 | train loss: 4.03 | val loss: 4.00 | perplexity: 54.76 | lr: 8.30e-04 | norm: 0.4934 | dt: 17178.3705ms | tok/sec: 22.8902\n",
      "step 1199 | train loss: 4.25 | val loss: 4.00 | perplexity: 54.73 | lr: 8.29e-04 | norm: 0.4779 | dt: 17154.6443ms | tok/sec: 22.9218\n",
      "step 1200 | train loss: 4.08 | val loss: 4.00 | perplexity: 54.56 | lr: 8.28e-04 | norm: 0.5086 | dt: 17131.2470ms | tok/sec: 22.9531\n",
      "step 1201 | train loss: 3.96 | val loss: 3.99 | perplexity: 54.07 | lr: 8.28e-04 | norm: 0.4800 | dt: 17156.6684ms | tok/sec: 22.9191\n",
      "step 1202 | train loss: 4.07 | val loss: 3.99 | perplexity: 53.95 | lr: 8.27e-04 | norm: 0.3770 | dt: 17176.7123ms | tok/sec: 22.8924\n",
      "step 1203 | train loss: 4.23 | val loss: 3.99 | perplexity: 53.80 | lr: 8.26e-04 | norm: 0.5562 | dt: 17140.8496ms | tok/sec: 22.9403\n",
      "step 1204 | train loss: 3.88 | val loss: 3.99 | perplexity: 54.06 | lr: 8.26e-04 | norm: 0.3915 | dt: 17221.3781ms | tok/sec: 22.8330\n",
      "step 1205 | train loss: 3.89 | val loss: 3.98 | perplexity: 53.76 | lr: 8.25e-04 | norm: 0.4150 | dt: 17136.5297ms | tok/sec: 22.9461\n",
      "step 1206 | train loss: 4.35 | val loss: 3.99 | perplexity: 53.83 | lr: 8.24e-04 | norm: 0.4023 | dt: 17171.4284ms | tok/sec: 22.8994\n",
      "step 1207 | train loss: 4.10 | val loss: 3.97 | perplexity: 53.24 | lr: 8.24e-04 | norm: 0.4433 | dt: 17234.2944ms | tok/sec: 22.8159\n",
      "step 1208 | train loss: 4.16 | val loss: 3.97 | perplexity: 53.03 | lr: 8.23e-04 | norm: 0.3541 | dt: 17119.9470ms | tok/sec: 22.9683\n",
      "step 1209 | train loss: 4.15 | val loss: 3.97 | perplexity: 52.81 | lr: 8.22e-04 | norm: 0.3861 | dt: 17090.3203ms | tok/sec: 23.0081\n",
      "step 1210 | train loss: 4.36 | val loss: 3.97 | perplexity: 52.91 | lr: 8.22e-04 | norm: 0.3853 | dt: 17163.8486ms | tok/sec: 22.9095\n",
      "step 1211 | train loss: 4.26 | val loss: 3.97 | perplexity: 53.10 | lr: 8.21e-04 | norm: 0.3714 | dt: 17080.7784ms | tok/sec: 23.0210\n",
      "step 1212 | train loss: 4.15 | val loss: 3.97 | perplexity: 52.83 | lr: 8.20e-04 | norm: 0.3862 | dt: 17166.0194ms | tok/sec: 22.9067\n",
      "step 1213 | train loss: 4.15 | val loss: 3.96 | perplexity: 52.47 | lr: 8.20e-04 | norm: 0.3827 | dt: 17160.7370ms | tok/sec: 22.9137\n",
      "step 1214 | train loss: 4.05 | val loss: 3.96 | perplexity: 52.33 | lr: 8.19e-04 | norm: 0.4684 | dt: 17118.6333ms | tok/sec: 22.9701\n",
      "step 1215 | train loss: 3.95 | val loss: 3.96 | perplexity: 52.29 | lr: 8.18e-04 | norm: 0.3983 | dt: 17135.9043ms | tok/sec: 22.9469\n",
      "step 1216 | train loss: 3.98 | val loss: 3.95 | perplexity: 52.16 | lr: 8.18e-04 | norm: 0.3666 | dt: 17136.1501ms | tok/sec: 22.9466\n",
      "step 1217 | train loss: 4.22 | val loss: 3.96 | perplexity: 52.24 | lr: 8.17e-04 | norm: 0.4065 | dt: 17137.5847ms | tok/sec: 22.9447\n",
      "step 1218 | train loss: 4.29 | val loss: 3.96 | perplexity: 52.36 | lr: 8.16e-04 | norm: 0.4161 | dt: 17137.7044ms | tok/sec: 22.9445\n",
      "step 1219 | train loss: 4.05 | val loss: 3.96 | perplexity: 52.20 | lr: 8.16e-04 | norm: 0.3665 | dt: 17139.2837ms | tok/sec: 22.9424\n",
      "step 1220 | train loss: 4.09 | val loss: 3.95 | perplexity: 51.97 | lr: 8.15e-04 | norm: 0.3732 | dt: 17156.1389ms | tok/sec: 22.9198\n",
      "step 1221 | train loss: 4.21 | val loss: 3.96 | perplexity: 52.27 | lr: 8.14e-04 | norm: 0.5292 | dt: 17175.4851ms | tok/sec: 22.8940\n",
      "step 1222 | train loss: 4.29 | val loss: 3.96 | perplexity: 52.55 | lr: 8.13e-04 | norm: 0.4473 | dt: 17130.6007ms | tok/sec: 22.9540\n",
      "step 1223 | train loss: 4.05 | val loss: 3.96 | perplexity: 52.31 | lr: 8.13e-04 | norm: 0.5061 | dt: 17144.9537ms | tok/sec: 22.9348\n",
      "step 1224 | train loss: 4.13 | val loss: 3.96 | perplexity: 52.60 | lr: 8.12e-04 | norm: 0.4209 | dt: 17091.5155ms | tok/sec: 23.0065\n",
      "step 1225 | train loss: 4.46 | val loss: 3.97 | perplexity: 52.82 | lr: 8.11e-04 | norm: 0.8537 | dt: 17088.5665ms | tok/sec: 23.0105\n",
      "step 1226 | train loss: 3.88 | val loss: 3.96 | perplexity: 52.54 | lr: 8.11e-04 | norm: 0.5118 | dt: 17163.4963ms | tok/sec: 22.9100\n",
      "step 1227 | train loss: 4.02 | val loss: 3.96 | perplexity: 52.69 | lr: 8.10e-04 | norm: 0.5894 | dt: 17147.6212ms | tok/sec: 22.9312\n",
      "step 1228 | train loss: 3.87 | val loss: 3.96 | perplexity: 52.66 | lr: 8.09e-04 | norm: 0.5179 | dt: 17121.2871ms | tok/sec: 22.9665\n",
      "step 1229 | train loss: 3.87 | val loss: 3.96 | perplexity: 52.67 | lr: 8.09e-04 | norm: 0.5737 | dt: 17123.1360ms | tok/sec: 22.9640\n",
      "step 1230 | train loss: 4.01 | val loss: 3.96 | perplexity: 52.57 | lr: 8.08e-04 | norm: 0.4619 | dt: 17142.2269ms | tok/sec: 22.9384\n",
      "step 1231 | train loss: 4.05 | val loss: 3.97 | perplexity: 52.86 | lr: 8.07e-04 | norm: 0.4147 | dt: 17156.3759ms | tok/sec: 22.9195\n",
      "step 1232 | train loss: 4.09 | val loss: 3.96 | perplexity: 52.36 | lr: 8.07e-04 | norm: 0.4669 | dt: 17109.4651ms | tok/sec: 22.9824\n",
      "step 1233 | train loss: 4.19 | val loss: 3.95 | perplexity: 52.19 | lr: 8.06e-04 | norm: 0.4144 | dt: 17145.7679ms | tok/sec: 22.9337\n",
      "step 1234 | train loss: 4.08 | val loss: 3.95 | perplexity: 52.19 | lr: 8.05e-04 | norm: 0.4879 | dt: 17188.6652ms | tok/sec: 22.8765\n",
      "step 1235 | train loss: 4.24 | val loss: 3.95 | perplexity: 52.14 | lr: 8.05e-04 | norm: 0.6265 | dt: 17108.2799ms | tok/sec: 22.9840\n",
      "step 1236 | train loss: 4.26 | val loss: 3.96 | perplexity: 52.30 | lr: 8.04e-04 | norm: 0.4736 | dt: 17154.5134ms | tok/sec: 22.9220\n",
      "step 1237 | train loss: 4.12 | val loss: 3.96 | perplexity: 52.31 | lr: 8.03e-04 | norm: 0.5467 | dt: 17136.5137ms | tok/sec: 22.9461\n",
      "step 1238 | train loss: 3.94 | val loss: 3.95 | perplexity: 52.13 | lr: 8.02e-04 | norm: 0.4676 | dt: 17050.5300ms | tok/sec: 23.0618\n",
      "step 1239 | train loss: 4.38 | val loss: 3.95 | perplexity: 51.84 | lr: 8.02e-04 | norm: 0.4961 | dt: 17109.6115ms | tok/sec: 22.9822\n",
      "step 1240 | train loss: 4.03 | val loss: 3.94 | perplexity: 51.55 | lr: 8.01e-04 | norm: 0.4245 | dt: 17111.0740ms | tok/sec: 22.9802\n",
      "step 1241 | train loss: 4.14 | val loss: 3.94 | perplexity: 51.45 | lr: 8.00e-04 | norm: 0.4397 | dt: 17142.6957ms | tok/sec: 22.9378\n",
      "step 1242 | train loss: 4.28 | val loss: 3.94 | perplexity: 51.52 | lr: 8.00e-04 | norm: 0.5048 | dt: 17180.6405ms | tok/sec: 22.8872\n",
      "step 1243 | train loss: 4.15 | val loss: 3.95 | perplexity: 51.70 | lr: 7.99e-04 | norm: 0.3681 | dt: 17165.4665ms | tok/sec: 22.9074\n",
      "step 1244 | train loss: 4.37 | val loss: 3.95 | perplexity: 51.74 | lr: 7.98e-04 | norm: 0.4857 | dt: 17150.6400ms | tok/sec: 22.9272\n",
      "step 1245 | train loss: 4.13 | val loss: 3.94 | perplexity: 51.47 | lr: 7.98e-04 | norm: 0.4781 | dt: 17126.1961ms | tok/sec: 22.9599\n",
      "step 1246 | train loss: 4.16 | val loss: 3.95 | perplexity: 51.76 | lr: 7.97e-04 | norm: 0.3811 | dt: 17165.2336ms | tok/sec: 22.9077\n",
      "step 1247 | train loss: 3.98 | val loss: 3.94 | perplexity: 51.61 | lr: 7.96e-04 | norm: 0.5630 | dt: 17102.1724ms | tok/sec: 22.9922\n",
      "step 1248 | train loss: 4.02 | val loss: 3.94 | perplexity: 51.30 | lr: 7.95e-04 | norm: 0.5168 | dt: 17344.1880ms | tok/sec: 22.6713\n",
      "step 1249 | train loss: 4.07 | val loss: 3.94 | perplexity: 51.37 | lr: 7.95e-04 | norm: 0.4192 | dt: 17079.0930ms | tok/sec: 23.0232\n",
      "step 1250 | train loss: 4.24 | val loss: 3.93 | perplexity: 50.92 | lr: 7.94e-04 | norm: 0.5406 | dt: 17100.3950ms | tok/sec: 22.9946\n",
      "step 1251 | train loss: 4.19 | val loss: 3.93 | perplexity: 50.95 | lr: 7.93e-04 | norm: 0.4407 | dt: 17080.9081ms | tok/sec: 23.0208\n",
      "step 1252 | train loss: 4.26 | val loss: 3.93 | perplexity: 50.71 | lr: 7.93e-04 | norm: 0.4934 | dt: 17117.0065ms | tok/sec: 22.9722\n",
      "step 1253 | train loss: 4.40 | val loss: 3.94 | perplexity: 51.38 | lr: 7.92e-04 | norm: 0.5238 | dt: 17126.6980ms | tok/sec: 22.9592\n",
      "step 1254 | train loss: 4.26 | val loss: 3.93 | perplexity: 51.05 | lr: 7.91e-04 | norm: 0.5970 | dt: 17140.9841ms | tok/sec: 22.9401\n",
      "step 1255 | train loss: 4.32 | val loss: 3.94 | perplexity: 51.32 | lr: 7.91e-04 | norm: 0.4393 | dt: 17124.6064ms | tok/sec: 22.9620\n",
      "step 1256 | train loss: 4.05 | val loss: 3.94 | perplexity: 51.22 | lr: 7.90e-04 | norm: 0.5155 | dt: 17063.0875ms | tok/sec: 23.0448\n",
      "step 1257 | train loss: 4.24 | val loss: 3.93 | perplexity: 50.99 | lr: 7.89e-04 | norm: 0.5538 | dt: 17098.4128ms | tok/sec: 22.9972\n",
      "step 1258 | train loss: 4.27 | val loss: 3.93 | perplexity: 50.75 | lr: 7.88e-04 | norm: 0.4768 | dt: 17149.9233ms | tok/sec: 22.9281\n",
      "step 1259 | train loss: 4.12 | val loss: 3.93 | perplexity: 50.73 | lr: 7.88e-04 | norm: 0.3687 | dt: 17084.8441ms | tok/sec: 23.0155\n",
      "step 1260 | train loss: 4.25 | val loss: 3.94 | perplexity: 51.51 | lr: 7.87e-04 | norm: 0.6068 | dt: 17094.1057ms | tok/sec: 23.0030\n",
      "step 1261 | train loss: 4.14 | val loss: 3.93 | perplexity: 51.06 | lr: 7.86e-04 | norm: 0.5208 | dt: 17204.7765ms | tok/sec: 22.8550\n",
      "step 1262 | train loss: 4.02 | val loss: 3.93 | perplexity: 50.76 | lr: 7.86e-04 | norm: 0.4652 | dt: 17147.0568ms | tok/sec: 22.9320\n",
      "step 1263 | train loss: 4.01 | val loss: 3.92 | perplexity: 50.54 | lr: 7.85e-04 | norm: 0.4269 | dt: 17106.0724ms | tok/sec: 22.9869\n",
      "step 1264 | train loss: 4.01 | val loss: 3.92 | perplexity: 50.64 | lr: 7.84e-04 | norm: 0.3925 | dt: 17207.4795ms | tok/sec: 22.8515\n",
      "step 1265 | train loss: 3.69 | val loss: 3.92 | perplexity: 50.42 | lr: 7.83e-04 | norm: 0.5871 | dt: 17140.9140ms | tok/sec: 22.9402\n",
      "step 1266 | train loss: 3.92 | val loss: 3.92 | perplexity: 50.24 | lr: 7.83e-04 | norm: 0.3848 | dt: 17137.0049ms | tok/sec: 22.9454\n",
      "step 1267 | train loss: 4.13 | val loss: 3.91 | perplexity: 50.00 | lr: 7.82e-04 | norm: 0.4153 | dt: 17151.4173ms | tok/sec: 22.9262\n",
      "step 1268 | train loss: 4.01 | val loss: 3.91 | perplexity: 49.85 | lr: 7.81e-04 | norm: 0.4314 | dt: 17079.0877ms | tok/sec: 23.0232\n",
      "step 1269 | train loss: 3.95 | val loss: 3.91 | perplexity: 49.86 | lr: 7.81e-04 | norm: 0.3952 | dt: 17113.6637ms | tok/sec: 22.9767\n",
      "step 1270 | train loss: 4.45 | val loss: 3.92 | perplexity: 50.27 | lr: 7.80e-04 | norm: 0.5142 | dt: 17167.7630ms | tok/sec: 22.9043\n",
      "step 1271 | train loss: 3.97 | val loss: 3.91 | perplexity: 49.88 | lr: 7.79e-04 | norm: 0.4781 | dt: 17178.5738ms | tok/sec: 22.8899\n",
      "step 1272 | train loss: 4.09 | val loss: 3.91 | perplexity: 49.68 | lr: 7.78e-04 | norm: 0.4173 | dt: 17158.1156ms | tok/sec: 22.9172\n",
      "step 1273 | train loss: 3.81 | val loss: 3.90 | perplexity: 49.49 | lr: 7.78e-04 | norm: 0.4292 | dt: 17170.5973ms | tok/sec: 22.9005\n",
      "step 1274 | train loss: 4.11 | val loss: 3.90 | perplexity: 49.47 | lr: 7.77e-04 | norm: 0.3773 | dt: 17571.7080ms | tok/sec: 22.3778\n",
      "step 1275 | train loss: 4.01 | val loss: 3.90 | perplexity: 49.25 | lr: 7.76e-04 | norm: 0.4000 | dt: 17132.9324ms | tok/sec: 22.9509\n",
      "step 1276 | train loss: 4.00 | val loss: 3.89 | perplexity: 49.07 | lr: 7.75e-04 | norm: 0.3954 | dt: 17145.5355ms | tok/sec: 22.9340\n",
      "step 1277 | train loss: 4.26 | val loss: 3.89 | perplexity: 48.95 | lr: 7.75e-04 | norm: 0.4028 | dt: 17135.8235ms | tok/sec: 22.9470\n",
      "step 1278 | train loss: 4.13 | val loss: 3.89 | perplexity: 49.12 | lr: 7.74e-04 | norm: 0.3940 | dt: 17155.9744ms | tok/sec: 22.9201\n",
      "step 1279 | train loss: 3.97 | val loss: 3.89 | perplexity: 48.73 | lr: 7.73e-04 | norm: 0.4979 | dt: 17153.7523ms | tok/sec: 22.9230\n",
      "step 1280 | train loss: 4.31 | val loss: 3.89 | perplexity: 48.76 | lr: 7.73e-04 | norm: 0.4101 | dt: 17107.8417ms | tok/sec: 22.9845\n",
      "step 1281 | train loss: 3.96 | val loss: 3.89 | perplexity: 49.07 | lr: 7.72e-04 | norm: 0.3894 | dt: 17110.4174ms | tok/sec: 22.9811\n",
      "step 1282 | train loss: 4.18 | val loss: 3.89 | perplexity: 49.09 | lr: 7.71e-04 | norm: 0.4426 | dt: 17161.1083ms | tok/sec: 22.9132\n",
      "step 1283 | train loss: 4.06 | val loss: 3.90 | perplexity: 49.25 | lr: 7.70e-04 | norm: 0.4066 | dt: 17108.4957ms | tok/sec: 22.9837\n",
      "step 1284 | train loss: 4.12 | val loss: 3.90 | perplexity: 49.23 | lr: 7.70e-04 | norm: 0.3654 | dt: 17229.0285ms | tok/sec: 22.8229\n",
      "step 1285 | train loss: 4.09 | val loss: 3.89 | perplexity: 48.96 | lr: 7.69e-04 | norm: 0.3793 | dt: 17197.7694ms | tok/sec: 22.8644\n",
      "step 1286 | train loss: 4.03 | val loss: 3.89 | perplexity: 48.91 | lr: 7.68e-04 | norm: 0.4445 | dt: 17131.6504ms | tok/sec: 22.9526\n",
      "step 1287 | train loss: 3.86 | val loss: 3.89 | perplexity: 48.80 | lr: 7.67e-04 | norm: 0.4640 | dt: 17165.1535ms | tok/sec: 22.9078\n",
      "step 1288 | train loss: 3.90 | val loss: 3.88 | perplexity: 48.61 | lr: 7.67e-04 | norm: 0.4959 | dt: 17136.0505ms | tok/sec: 22.9467\n",
      "step 1289 | train loss: 3.99 | val loss: 3.88 | perplexity: 48.46 | lr: 7.66e-04 | norm: 0.4625 | dt: 17172.0755ms | tok/sec: 22.8986\n",
      "step 1290 | train loss: 4.04 | val loss: 3.88 | perplexity: 48.33 | lr: 7.65e-04 | norm: 0.4693 | dt: 17093.7569ms | tok/sec: 23.0035\n",
      "step 1291 | train loss: 4.30 | val loss: 3.88 | perplexity: 48.39 | lr: 7.65e-04 | norm: 0.3663 | dt: 17097.1293ms | tok/sec: 22.9989\n",
      "step 1292 | train loss: 4.13 | val loss: 3.88 | perplexity: 48.30 | lr: 7.64e-04 | norm: 0.4903 | dt: 17102.5350ms | tok/sec: 22.9917\n",
      "step 1293 | train loss: 4.09 | val loss: 3.88 | perplexity: 48.25 | lr: 7.63e-04 | norm: 0.3749 | dt: 17170.7339ms | tok/sec: 22.9004\n",
      "step 1294 | train loss: 4.19 | val loss: 3.89 | perplexity: 48.70 | lr: 7.62e-04 | norm: 0.4300 | dt: 17092.6821ms | tok/sec: 23.0049\n",
      "step 1295 | train loss: 3.99 | val loss: 3.87 | perplexity: 47.99 | lr: 7.62e-04 | norm: 0.5740 | dt: 17128.1188ms | tok/sec: 22.9573\n",
      "step 1296 | train loss: 4.07 | val loss: 3.88 | perplexity: 48.20 | lr: 7.61e-04 | norm: 0.3915 | dt: 17168.8881ms | tok/sec: 22.9028\n",
      "step 1297 | train loss: 3.99 | val loss: 3.87 | perplexity: 47.88 | lr: 7.60e-04 | norm: 0.4971 | dt: 17125.3331ms | tok/sec: 22.9611\n",
      "step 1298 | train loss: 3.90 | val loss: 3.87 | perplexity: 48.14 | lr: 7.59e-04 | norm: 0.4711 | dt: 17157.1243ms | tok/sec: 22.9185\n",
      "step 1299 | train loss: 4.23 | val loss: 3.87 | perplexity: 48.10 | lr: 7.59e-04 | norm: 0.4559 | dt: 17079.1905ms | tok/sec: 23.0231\n",
      "step 1300 | train loss: 3.99 | val loss: 3.86 | perplexity: 47.58 | lr: 7.58e-04 | norm: 0.5652 | dt: 17154.7403ms | tok/sec: 22.9217\n",
      "step 1301 | train loss: 4.00 | val loss: 3.87 | perplexity: 47.79 | lr: 7.57e-04 | norm: 0.4360 | dt: 17104.3241ms | tok/sec: 22.9893\n",
      "step 1302 | train loss: 4.26 | val loss: 3.86 | perplexity: 47.57 | lr: 7.56e-04 | norm: 0.5850 | dt: 17200.6941ms | tok/sec: 22.8605\n",
      "step 1303 | train loss: 3.85 | val loss: 3.86 | perplexity: 47.69 | lr: 7.56e-04 | norm: 0.3806 | dt: 17140.8150ms | tok/sec: 22.9403\n",
      "step 1304 | train loss: 3.79 | val loss: 3.87 | perplexity: 47.77 | lr: 7.55e-04 | norm: 0.4656 | dt: 17121.4275ms | tok/sec: 22.9663\n",
      "step 1305 | train loss: 4.02 | val loss: 3.87 | perplexity: 47.91 | lr: 7.54e-04 | norm: 0.4992 | dt: 17141.0263ms | tok/sec: 22.9401\n",
      "step 1306 | train loss: 4.10 | val loss: 3.87 | perplexity: 48.09 | lr: 7.53e-04 | norm: 0.4597 | dt: 17059.8955ms | tok/sec: 23.0491\n",
      "step 1307 | train loss: 3.98 | val loss: 3.87 | perplexity: 47.99 | lr: 7.53e-04 | norm: 0.4667 | dt: 17143.5547ms | tok/sec: 22.9367\n",
      "step 1308 | train loss: 3.94 | val loss: 3.87 | perplexity: 47.73 | lr: 7.52e-04 | norm: 0.4172 | dt: 17147.8300ms | tok/sec: 22.9309\n",
      "step 1309 | train loss: 4.23 | val loss: 3.87 | perplexity: 47.71 | lr: 7.51e-04 | norm: 0.5002 | dt: 17118.1273ms | tok/sec: 22.9707\n",
      "step 1310 | train loss: 4.13 | val loss: 3.86 | perplexity: 47.49 | lr: 7.50e-04 | norm: 0.5084 | dt: 17097.1184ms | tok/sec: 22.9990\n",
      "step 1311 | train loss: 3.87 | val loss: 3.86 | perplexity: 47.62 | lr: 7.50e-04 | norm: 0.4124 | dt: 17177.0446ms | tok/sec: 22.8919\n",
      "step 1312 | train loss: 4.10 | val loss: 3.86 | perplexity: 47.53 | lr: 7.49e-04 | norm: 0.4668 | dt: 17155.9191ms | tok/sec: 22.9201\n",
      "step 1313 | train loss: 4.08 | val loss: 3.86 | perplexity: 47.66 | lr: 7.48e-04 | norm: 0.4781 | dt: 17124.3386ms | tok/sec: 22.9624\n",
      "step 1314 | train loss: 3.86 | val loss: 3.86 | perplexity: 47.48 | lr: 7.47e-04 | norm: 0.4856 | dt: 17170.1915ms | tok/sec: 22.9011\n",
      "step 1315 | train loss: 3.99 | val loss: 3.86 | perplexity: 47.53 | lr: 7.47e-04 | norm: 0.4108 | dt: 17124.0790ms | tok/sec: 22.9628\n",
      "step 1316 | train loss: 3.98 | val loss: 3.86 | perplexity: 47.54 | lr: 7.46e-04 | norm: 0.4162 | dt: 17155.5910ms | tok/sec: 22.9206\n",
      "step 1317 | train loss: 3.89 | val loss: 3.86 | perplexity: 47.35 | lr: 7.45e-04 | norm: 0.3602 | dt: 17112.2866ms | tok/sec: 22.9786\n",
      "step 1318 | train loss: 3.84 | val loss: 3.85 | perplexity: 47.02 | lr: 7.44e-04 | norm: 0.3709 | dt: 17173.2855ms | tok/sec: 22.8970\n",
      "step 1319 | train loss: 3.84 | val loss: 3.85 | perplexity: 46.97 | lr: 7.44e-04 | norm: 0.3598 | dt: 17136.9057ms | tok/sec: 22.9456\n",
      "step 1320 | train loss: 3.98 | val loss: 3.85 | perplexity: 47.08 | lr: 7.43e-04 | norm: 0.3496 | dt: 17119.1716ms | tok/sec: 22.9693\n",
      "step 1321 | train loss: 4.20 | val loss: 3.85 | perplexity: 47.08 | lr: 7.42e-04 | norm: 0.3952 | dt: 17087.5542ms | tok/sec: 23.0118\n",
      "step 1322 | train loss: 4.37 | val loss: 3.86 | perplexity: 47.29 | lr: 7.41e-04 | norm: 0.4042 | dt: 17134.4850ms | tok/sec: 22.9488\n",
      "step 1323 | train loss: 3.82 | val loss: 3.85 | perplexity: 47.22 | lr: 7.41e-04 | norm: 0.4232 | dt: 17106.6980ms | tok/sec: 22.9861\n",
      "step 1324 | train loss: 4.04 | val loss: 3.85 | perplexity: 46.96 | lr: 7.40e-04 | norm: 0.3818 | dt: 17028.8565ms | tok/sec: 23.0912\n",
      "step 1325 | train loss: 3.88 | val loss: 3.85 | perplexity: 46.86 | lr: 7.39e-04 | norm: 0.3955 | dt: 17182.7958ms | tok/sec: 22.8843\n",
      "step 1326 | train loss: 4.02 | val loss: 3.84 | perplexity: 46.64 | lr: 7.38e-04 | norm: 0.3864 | dt: 17147.7981ms | tok/sec: 22.9310\n",
      "step 1327 | train loss: 3.97 | val loss: 3.85 | perplexity: 46.82 | lr: 7.37e-04 | norm: 0.4097 | dt: 17104.8963ms | tok/sec: 22.9885\n",
      "step 1328 | train loss: 3.99 | val loss: 3.84 | perplexity: 46.71 | lr: 7.37e-04 | norm: 0.4215 | dt: 17113.9302ms | tok/sec: 22.9764\n",
      "step 1329 | train loss: 4.25 | val loss: 3.85 | perplexity: 46.90 | lr: 7.36e-04 | norm: 0.4480 | dt: 17072.5508ms | tok/sec: 23.0321\n",
      "step 1330 | train loss: 3.99 | val loss: 3.85 | perplexity: 47.01 | lr: 7.35e-04 | norm: 0.3960 | dt: 17107.8019ms | tok/sec: 22.9846\n",
      "step 1331 | train loss: 3.71 | val loss: 3.85 | perplexity: 47.04 | lr: 7.34e-04 | norm: 0.4724 | dt: 17053.5705ms | tok/sec: 23.0577\n",
      "step 1332 | train loss: 3.65 | val loss: 3.85 | perplexity: 47.18 | lr: 7.34e-04 | norm: 0.3915 | dt: 17161.8371ms | tok/sec: 22.9122\n",
      "step 1333 | train loss: 3.88 | val loss: 3.85 | perplexity: 47.12 | lr: 7.33e-04 | norm: 0.4794 | dt: 17105.4919ms | tok/sec: 22.9877\n",
      "step 1334 | train loss: 3.74 | val loss: 3.85 | perplexity: 46.87 | lr: 7.32e-04 | norm: 0.4062 | dt: 17257.8759ms | tok/sec: 22.7847\n",
      "step 1335 | train loss: 3.99 | val loss: 3.85 | perplexity: 46.80 | lr: 7.31e-04 | norm: 0.3941 | dt: 17079.0348ms | tok/sec: 23.0233\n",
      "step 1336 | train loss: 3.96 | val loss: 3.85 | perplexity: 47.13 | lr: 7.31e-04 | norm: 0.4808 | dt: 17147.1384ms | tok/sec: 22.9319\n",
      "step 1337 | train loss: 4.00 | val loss: 3.86 | perplexity: 47.40 | lr: 7.30e-04 | norm: 0.3821 | dt: 17157.0332ms | tok/sec: 22.9186\n",
      "step 1338 | train loss: 3.93 | val loss: 3.85 | perplexity: 47.11 | lr: 7.29e-04 | norm: 0.4304 | dt: 17194.8483ms | tok/sec: 22.8682\n",
      "step 1339 | train loss: 4.11 | val loss: 3.86 | perplexity: 47.32 | lr: 7.28e-04 | norm: 0.3894 | dt: 17115.3681ms | tok/sec: 22.9744\n",
      "step 1340 | train loss: 4.14 | val loss: 3.86 | perplexity: 47.23 | lr: 7.28e-04 | norm: 0.4402 | dt: 17081.3124ms | tok/sec: 23.0202\n",
      "step 1341 | train loss: 3.93 | val loss: 3.85 | perplexity: 47.10 | lr: 7.27e-04 | norm: 0.4315 | dt: 17143.0089ms | tok/sec: 22.9374\n",
      "step 1342 | train loss: 3.77 | val loss: 3.85 | perplexity: 46.95 | lr: 7.26e-04 | norm: 0.5070 | dt: 17161.2999ms | tok/sec: 22.9129\n",
      "step 1343 | train loss: 4.01 | val loss: 3.86 | perplexity: 47.23 | lr: 7.25e-04 | norm: 0.3935 | dt: 17154.7594ms | tok/sec: 22.9217\n",
      "step 1344 | train loss: 4.01 | val loss: 3.85 | perplexity: 46.81 | lr: 7.24e-04 | norm: 0.4950 | dt: 17120.4519ms | tok/sec: 22.9676\n",
      "step 1345 | train loss: 3.80 | val loss: 3.84 | perplexity: 46.51 | lr: 7.24e-04 | norm: 0.3635 | dt: 17139.4668ms | tok/sec: 22.9421\n",
      "step 1346 | train loss: 3.64 | val loss: 3.84 | perplexity: 46.41 | lr: 7.23e-04 | norm: 0.4067 | dt: 17162.5879ms | tok/sec: 22.9112\n",
      "step 1347 | train loss: 3.68 | val loss: 3.83 | perplexity: 46.22 | lr: 7.22e-04 | norm: 0.3933 | dt: 17160.4753ms | tok/sec: 22.9141\n",
      "step 1348 | train loss: 3.84 | val loss: 3.83 | perplexity: 46.02 | lr: 7.21e-04 | norm: 0.4380 | dt: 17130.2464ms | tok/sec: 22.9545\n",
      "step 1349 | train loss: 4.14 | val loss: 3.83 | perplexity: 46.24 | lr: 7.21e-04 | norm: 0.3949 | dt: 17172.5907ms | tok/sec: 22.8979\n",
      "step 1350 | train loss: 4.02 | val loss: 3.83 | perplexity: 46.16 | lr: 7.20e-04 | norm: 0.4005 | dt: 17124.9983ms | tok/sec: 22.9615\n",
      "step 1351 | train loss: 4.27 | val loss: 3.84 | perplexity: 46.53 | lr: 7.19e-04 | norm: 0.4857 | dt: 17156.1875ms | tok/sec: 22.9198\n",
      "step 1352 | train loss: 3.64 | val loss: 3.84 | perplexity: 46.71 | lr: 7.18e-04 | norm: 0.5375 | dt: 17172.8597ms | tok/sec: 22.8975\n",
      "step 1353 | train loss: 4.13 | val loss: 3.84 | perplexity: 46.64 | lr: 7.17e-04 | norm: 0.5453 | dt: 17137.0902ms | tok/sec: 22.9453\n",
      "step 1354 | train loss: 4.09 | val loss: 3.84 | perplexity: 46.46 | lr: 7.17e-04 | norm: 0.4640 | dt: 17158.1762ms | tok/sec: 22.9171\n",
      "step 1355 | train loss: 4.13 | val loss: 3.84 | perplexity: 46.48 | lr: 7.16e-04 | norm: 0.5276 | dt: 17203.6345ms | tok/sec: 22.8566\n",
      "step 1356 | train loss: 4.15 | val loss: 3.84 | perplexity: 46.40 | lr: 7.15e-04 | norm: 0.4353 | dt: 17116.8857ms | tok/sec: 22.9724\n",
      "step 1357 | train loss: 4.06 | val loss: 3.84 | perplexity: 46.39 | lr: 7.14e-04 | norm: 0.3658 | dt: 17169.5731ms | tok/sec: 22.9019\n",
      "step 1358 | train loss: 3.95 | val loss: 3.84 | perplexity: 46.38 | lr: 7.14e-04 | norm: 0.3530 | dt: 17105.1972ms | tok/sec: 22.9881\n",
      "step 1359 | train loss: 4.02 | val loss: 3.84 | perplexity: 46.53 | lr: 7.13e-04 | norm: 0.3758 | dt: 17094.5358ms | tok/sec: 23.0024\n",
      "step 1360 | train loss: 4.03 | val loss: 3.84 | perplexity: 46.67 | lr: 7.12e-04 | norm: 0.3758 | dt: 17076.0689ms | tok/sec: 23.0273\n",
      "step 1361 | train loss: 4.20 | val loss: 3.84 | perplexity: 46.59 | lr: 7.11e-04 | norm: 0.4578 | dt: 17122.2684ms | tok/sec: 22.9652\n",
      "step 1362 | train loss: 4.19 | val loss: 3.84 | perplexity: 46.31 | lr: 7.10e-04 | norm: 0.4335 | dt: 17165.7062ms | tok/sec: 22.9071\n",
      "step 1363 | train loss: 3.93 | val loss: 3.83 | perplexity: 46.06 | lr: 7.10e-04 | norm: 0.4570 | dt: 17149.7488ms | tok/sec: 22.9284\n",
      "step 1364 | train loss: 3.98 | val loss: 3.83 | perplexity: 46.07 | lr: 7.09e-04 | norm: 0.3772 | dt: 17162.2436ms | tok/sec: 22.9117\n",
      "step 1365 | train loss: 3.98 | val loss: 3.83 | perplexity: 46.16 | lr: 7.08e-04 | norm: 0.4002 | dt: 17161.2506ms | tok/sec: 22.9130\n",
      "step 1366 | train loss: 4.02 | val loss: 3.84 | perplexity: 46.49 | lr: 7.07e-04 | norm: 0.4335 | dt: 17162.4453ms | tok/sec: 22.9114\n",
      "step 1367 | train loss: 3.81 | val loss: 3.83 | perplexity: 46.22 | lr: 7.06e-04 | norm: 0.4927 | dt: 17154.1185ms | tok/sec: 22.9225\n",
      "step 1368 | train loss: 3.99 | val loss: 3.82 | perplexity: 45.78 | lr: 7.06e-04 | norm: 0.4256 | dt: 17168.5631ms | tok/sec: 22.9033\n",
      "step 1369 | train loss: 3.85 | val loss: 3.82 | perplexity: 45.48 | lr: 7.05e-04 | norm: 0.5035 | dt: 17168.7169ms | tok/sec: 22.9031\n",
      "step 1370 | train loss: 4.56 | val loss: 3.82 | perplexity: 45.70 | lr: 7.04e-04 | norm: 0.4690 | dt: 17227.0839ms | tok/sec: 22.8255\n",
      "step 1371 | train loss: 4.04 | val loss: 3.82 | perplexity: 45.62 | lr: 7.03e-04 | norm: 0.4811 | dt: 17137.6452ms | tok/sec: 22.9446\n",
      "step 1372 | train loss: 3.91 | val loss: 3.82 | perplexity: 45.51 | lr: 7.03e-04 | norm: 0.5018 | dt: 17172.3685ms | tok/sec: 22.8982\n",
      "step 1373 | train loss: 3.71 | val loss: 3.82 | perplexity: 45.38 | lr: 7.02e-04 | norm: 0.4411 | dt: 17103.0762ms | tok/sec: 22.9910\n",
      "step 1374 | train loss: 3.77 | val loss: 3.81 | perplexity: 45.30 | lr: 7.01e-04 | norm: 0.4381 | dt: 17130.2452ms | tok/sec: 22.9545\n",
      "step 1375 | train loss: 4.03 | val loss: 3.83 | perplexity: 46.01 | lr: 7.00e-04 | norm: 0.5864 | dt: 17168.4327ms | tok/sec: 22.9034\n",
      "step 1376 | train loss: 3.89 | val loss: 3.84 | perplexity: 46.75 | lr: 6.99e-04 | norm: 0.5319 | dt: 17099.2830ms | tok/sec: 22.9961\n",
      "step 1377 | train loss: 3.68 | val loss: 3.84 | perplexity: 46.33 | lr: 6.99e-04 | norm: 0.6231 | dt: 17190.7399ms | tok/sec: 22.8737\n",
      "step 1378 | train loss: 3.72 | val loss: 3.84 | perplexity: 46.40 | lr: 6.98e-04 | norm: 0.5068 | dt: 17141.5238ms | tok/sec: 22.9394\n",
      "step 1379 | train loss: 3.61 | val loss: 3.83 | perplexity: 46.09 | lr: 6.97e-04 | norm: 0.5643 | dt: 17139.4105ms | tok/sec: 22.9422\n",
      "step 1380 | train loss: 4.38 | val loss: 3.84 | perplexity: 46.31 | lr: 6.96e-04 | norm: 0.5146 | dt: 17213.6562ms | tok/sec: 22.8433\n",
      "step 1381 | train loss: 4.67 | val loss: 3.85 | perplexity: 47.00 | lr: 6.95e-04 | norm: 0.4870 | dt: 17164.9051ms | tok/sec: 22.9081\n",
      "step 1382 | train loss: 3.75 | val loss: 3.84 | perplexity: 46.66 | lr: 6.95e-04 | norm: 0.5761 | dt: 17200.5141ms | tok/sec: 22.8607\n",
      "step 1383 | train loss: 3.71 | val loss: 3.85 | perplexity: 46.77 | lr: 6.94e-04 | norm: 0.3701 | dt: 17148.7896ms | tok/sec: 22.9297\n",
      "step 1384 | train loss: 3.59 | val loss: 3.84 | perplexity: 46.65 | lr: 6.93e-04 | norm: 0.5460 | dt: 17148.0873ms | tok/sec: 22.9306\n",
      "step 1385 | train loss: 3.77 | val loss: 3.84 | perplexity: 46.73 | lr: 6.92e-04 | norm: 0.4075 | dt: 17107.0507ms | tok/sec: 22.9856\n",
      "step 1386 | train loss: 3.99 | val loss: 3.85 | perplexity: 47.07 | lr: 6.91e-04 | norm: 0.4677 | dt: 17138.8903ms | tok/sec: 22.9429\n",
      "step 1387 | train loss: 4.08 | val loss: 3.86 | perplexity: 47.24 | lr: 6.91e-04 | norm: 0.5627 | dt: 17178.6971ms | tok/sec: 22.8897\n",
      "step 1388 | train loss: 3.79 | val loss: 3.86 | perplexity: 47.28 | lr: 6.90e-04 | norm: 0.5268 | dt: 17186.1780ms | tok/sec: 22.8798\n",
      "step 1389 | train loss: 3.91 | val loss: 3.86 | perplexity: 47.39 | lr: 6.89e-04 | norm: 0.5678 | dt: 17179.3025ms | tok/sec: 22.8889\n",
      "step 1390 | train loss: 3.71 | val loss: 3.86 | perplexity: 47.32 | lr: 6.88e-04 | norm: 0.4602 | dt: 17109.5593ms | tok/sec: 22.9822\n",
      "step 1391 | train loss: 3.87 | val loss: 3.85 | perplexity: 46.93 | lr: 6.87e-04 | norm: 0.4873 | dt: 17137.3978ms | tok/sec: 22.9449\n",
      "step 1392 | train loss: 3.95 | val loss: 3.85 | perplexity: 47.03 | lr: 6.87e-04 | norm: 0.8795 | dt: 17185.9677ms | tok/sec: 22.8801\n",
      "step 1393 | train loss: 3.86 | val loss: 3.85 | perplexity: 46.88 | lr: 6.86e-04 | norm: 0.4541 | dt: 17177.1510ms | tok/sec: 22.8918\n",
      "step 1394 | train loss: 3.88 | val loss: 3.85 | perplexity: 46.89 | lr: 6.85e-04 | norm: 0.4439 | dt: 17152.2884ms | tok/sec: 22.9250\n",
      "step 1395 | train loss: 3.70 | val loss: 3.84 | perplexity: 46.54 | lr: 6.84e-04 | norm: 0.5194 | dt: 17148.7210ms | tok/sec: 22.9298\n",
      "step 1396 | train loss: 3.88 | val loss: 3.84 | perplexity: 46.50 | lr: 6.83e-04 | norm: 0.3680 | dt: 17167.7382ms | tok/sec: 22.9044\n",
      "step 1397 | train loss: 3.89 | val loss: 3.83 | perplexity: 46.28 | lr: 6.83e-04 | norm: 0.4520 | dt: 17154.0492ms | tok/sec: 22.9226\n",
      "step 1398 | train loss: 3.87 | val loss: 3.83 | perplexity: 46.22 | lr: 6.82e-04 | norm: 0.3898 | dt: 17115.7634ms | tok/sec: 22.9739\n",
      "step 1399 | train loss: 4.22 | val loss: 3.83 | perplexity: 46.07 | lr: 6.81e-04 | norm: 0.5167 | dt: 17128.1796ms | tok/sec: 22.9573\n",
      "step 1400 | train loss: 4.06 | val loss: 3.83 | perplexity: 46.02 | lr: 6.80e-04 | norm: 0.4730 | dt: 17144.2881ms | tok/sec: 22.9357\n",
      "step 1401 | train loss: 3.66 | val loss: 3.82 | perplexity: 45.75 | lr: 6.79e-04 | norm: 0.4521 | dt: 17147.3639ms | tok/sec: 22.9316\n",
      "step 1402 | train loss: 3.73 | val loss: 3.82 | perplexity: 45.69 | lr: 6.79e-04 | norm: 0.5258 | dt: 17151.7179ms | tok/sec: 22.9258\n",
      "step 1403 | train loss: 3.53 | val loss: 3.82 | perplexity: 45.71 | lr: 6.78e-04 | norm: 0.4945 | dt: 17143.4813ms | tok/sec: 22.9368\n",
      "step 1404 | train loss: 3.75 | val loss: 3.82 | perplexity: 45.56 | lr: 6.77e-04 | norm: 0.4209 | dt: 17151.6874ms | tok/sec: 22.9258\n",
      "step 1405 | train loss: 3.97 | val loss: 3.82 | perplexity: 45.56 | lr: 6.76e-04 | norm: 0.3729 | dt: 17082.6313ms | tok/sec: 23.0185\n",
      "step 1406 | train loss: 3.76 | val loss: 3.83 | perplexity: 45.85 | lr: 6.75e-04 | norm: 0.4642 | dt: 17173.7385ms | tok/sec: 22.8964\n",
      "step 1407 | train loss: 3.72 | val loss: 3.82 | perplexity: 45.63 | lr: 6.75e-04 | norm: 0.5310 | dt: 17146.1227ms | tok/sec: 22.9332\n",
      "step 1408 | train loss: 3.47 | val loss: 3.82 | perplexity: 45.78 | lr: 6.74e-04 | norm: 0.4597 | dt: 17124.2201ms | tok/sec: 22.9626\n",
      "step 1409 | train loss: 3.67 | val loss: 3.82 | perplexity: 45.70 | lr: 6.73e-04 | norm: 0.4670 | dt: 17123.0850ms | tok/sec: 22.9641\n",
      "step 1410 | train loss: 3.60 | val loss: 3.82 | perplexity: 45.69 | lr: 6.72e-04 | norm: 0.4031 | dt: 17136.1864ms | tok/sec: 22.9465\n",
      "step 1411 | train loss: 3.80 | val loss: 3.82 | perplexity: 45.58 | lr: 6.71e-04 | norm: 0.3867 | dt: 17198.8471ms | tok/sec: 22.8629\n",
      "step 1412 | train loss: 4.29 | val loss: 3.82 | perplexity: 45.70 | lr: 6.71e-04 | norm: 0.5397 | dt: 17147.3534ms | tok/sec: 22.9316\n",
      "step 1413 | train loss: 4.01 | val loss: 3.83 | perplexity: 45.83 | lr: 6.70e-04 | norm: 0.4917 | dt: 17138.1290ms | tok/sec: 22.9439\n",
      "step 1414 | train loss: 3.94 | val loss: 3.82 | perplexity: 45.53 | lr: 6.69e-04 | norm: 0.4607 | dt: 17171.3915ms | tok/sec: 22.8995\n",
      "step 1415 | train loss: 3.97 | val loss: 3.82 | perplexity: 45.51 | lr: 6.68e-04 | norm: 0.3912 | dt: 17133.7316ms | tok/sec: 22.9498\n",
      "step 1416 | train loss: 3.87 | val loss: 3.81 | perplexity: 45.25 | lr: 6.67e-04 | norm: 0.4832 | dt: 17117.9070ms | tok/sec: 22.9710\n",
      "step 1417 | train loss: 3.68 | val loss: 3.82 | perplexity: 45.56 | lr: 6.67e-04 | norm: 0.7252 | dt: 17101.3889ms | tok/sec: 22.9932\n",
      "step 1418 | train loss: 3.63 | val loss: 3.83 | perplexity: 46.05 | lr: 6.66e-04 | norm: 0.5148 | dt: 17179.4257ms | tok/sec: 22.8888\n",
      "step 1419 | train loss: 3.80 | val loss: 3.83 | perplexity: 45.86 | lr: 6.65e-04 | norm: 0.6157 | dt: 17170.0101ms | tok/sec: 22.9013\n",
      "step 1420 | train loss: 4.22 | val loss: 3.83 | perplexity: 45.84 | lr: 6.64e-04 | norm: 0.6220 | dt: 17277.0255ms | tok/sec: 22.7595\n",
      "step 1421 | train loss: 3.73 | val loss: 3.82 | perplexity: 45.51 | lr: 6.63e-04 | norm: 0.5295 | dt: 17146.2243ms | tok/sec: 22.9331\n",
      "step 1422 | train loss: 3.75 | val loss: 3.81 | perplexity: 45.32 | lr: 6.62e-04 | norm: 0.4527 | dt: 17154.1853ms | tok/sec: 22.9225\n",
      "step 1423 | train loss: 3.71 | val loss: 3.81 | perplexity: 45.26 | lr: 6.62e-04 | norm: 0.3925 | dt: 17099.0016ms | tok/sec: 22.9964\n",
      "step 1424 | train loss: 3.76 | val loss: 3.81 | perplexity: 45.15 | lr: 6.61e-04 | norm: 0.3377 | dt: 17114.8460ms | tok/sec: 22.9751\n",
      "step 1425 | train loss: 3.90 | val loss: 3.80 | perplexity: 44.88 | lr: 6.60e-04 | norm: 0.4353 | dt: 17173.1293ms | tok/sec: 22.8972\n",
      "step 1426 | train loss: 3.67 | val loss: 3.81 | perplexity: 44.96 | lr: 6.59e-04 | norm: 0.4606 | dt: 17115.3209ms | tok/sec: 22.9745\n",
      "step 1427 | train loss: 3.81 | val loss: 3.81 | perplexity: 45.00 | lr: 6.58e-04 | norm: 0.4486 | dt: 17108.6740ms | tok/sec: 22.9834\n",
      "step 1428 | train loss: 4.42 | val loss: 3.82 | perplexity: 45.50 | lr: 6.58e-04 | norm: 0.6850 | dt: 17190.9633ms | tok/sec: 22.8734\n",
      "step 1429 | train loss: 4.49 | val loss: 3.84 | perplexity: 46.52 | lr: 6.57e-04 | norm: 0.5030 | dt: 17168.1540ms | tok/sec: 22.9038\n",
      "step 1430 | train loss: 3.88 | val loss: 3.82 | perplexity: 45.55 | lr: 6.56e-04 | norm: 0.6306 | dt: 17155.8847ms | tok/sec: 22.9202\n",
      "step 1431 | train loss: 4.17 | val loss: 3.81 | perplexity: 45.04 | lr: 6.55e-04 | norm: 0.6868 | dt: 17164.7389ms | tok/sec: 22.9084\n",
      "step 1432 | train loss: 3.79 | val loss: 3.81 | perplexity: 45.00 | lr: 6.54e-04 | norm: 0.5182 | dt: 17169.9739ms | tok/sec: 22.9014\n",
      "step 1433 | train loss: 3.68 | val loss: 3.80 | perplexity: 44.84 | lr: 6.54e-04 | norm: 0.4991 | dt: 17133.5926ms | tok/sec: 22.9500\n",
      "step 1434 | train loss: 3.85 | val loss: 3.81 | perplexity: 44.98 | lr: 6.53e-04 | norm: 0.4541 | dt: 17133.2016ms | tok/sec: 22.9505\n",
      "step 1435 | train loss: 3.74 | val loss: 3.80 | perplexity: 44.91 | lr: 6.52e-04 | norm: 0.5029 | dt: 17142.5383ms | tok/sec: 22.9380\n",
      "step 1436 | train loss: 4.03 | val loss: 3.80 | perplexity: 44.64 | lr: 6.51e-04 | norm: 0.4178 | dt: 17106.6835ms | tok/sec: 22.9861\n",
      "step 1437 | train loss: 3.67 | val loss: 3.80 | perplexity: 44.48 | lr: 6.50e-04 | norm: 0.3426 | dt: 17159.9879ms | tok/sec: 22.9147\n",
      "step 1438 | train loss: 3.90 | val loss: 3.80 | perplexity: 44.55 | lr: 6.49e-04 | norm: 0.4214 | dt: 17175.7181ms | tok/sec: 22.8937\n",
      "step 1439 | train loss: 3.93 | val loss: 3.80 | perplexity: 44.56 | lr: 6.49e-04 | norm: 0.4218 | dt: 17184.9465ms | tok/sec: 22.8814\n",
      "step 1440 | train loss: 3.87 | val loss: 3.79 | perplexity: 44.37 | lr: 6.48e-04 | norm: 0.3835 | dt: 17174.3052ms | tok/sec: 22.8956\n",
      "step 1441 | train loss: 3.80 | val loss: 3.79 | perplexity: 44.19 | lr: 6.47e-04 | norm: 0.4459 | dt: 17170.6040ms | tok/sec: 22.9005\n",
      "step 1442 | train loss: 3.99 | val loss: 3.79 | perplexity: 44.18 | lr: 6.46e-04 | norm: 0.3837 | dt: 17102.4761ms | tok/sec: 22.9918\n",
      "step 1443 | train loss: 4.57 | val loss: 3.79 | perplexity: 44.13 | lr: 6.45e-04 | norm: 0.5473 | dt: 17171.8838ms | tok/sec: 22.8988\n",
      "step 1444 | train loss: 3.98 | val loss: 3.79 | perplexity: 44.21 | lr: 6.45e-04 | norm: 0.6465 | dt: 17117.8625ms | tok/sec: 22.9711\n",
      "step 1445 | train loss: 3.78 | val loss: 3.79 | perplexity: 44.30 | lr: 6.44e-04 | norm: 0.3769 | dt: 17146.1654ms | tok/sec: 22.9332\n",
      "step 1446 | train loss: 4.06 | val loss: 3.81 | perplexity: 44.93 | lr: 6.43e-04 | norm: 0.8050 | dt: 17142.0200ms | tok/sec: 22.9387\n",
      "step 1447 | train loss: 4.07 | val loss: 3.81 | perplexity: 44.99 | lr: 6.42e-04 | norm: 0.5194 | dt: 17180.3296ms | tok/sec: 22.8876\n",
      "step 1448 | train loss: 3.70 | val loss: 3.80 | perplexity: 44.72 | lr: 6.41e-04 | norm: 0.4906 | dt: 17089.5522ms | tok/sec: 23.0091\n",
      "step 1449 | train loss: 3.89 | val loss: 3.81 | perplexity: 44.95 | lr: 6.40e-04 | norm: 0.3835 | dt: 17120.9159ms | tok/sec: 22.9670\n",
      "step 1450 | train loss: 4.00 | val loss: 3.80 | perplexity: 44.83 | lr: 6.40e-04 | norm: 0.4645 | dt: 17145.7882ms | tok/sec: 22.9337\n",
      "step 1451 | train loss: 3.77 | val loss: 3.80 | perplexity: 44.78 | lr: 6.39e-04 | norm: 0.5161 | dt: 17116.7774ms | tok/sec: 22.9725\n",
      "step 1452 | train loss: 3.84 | val loss: 3.79 | perplexity: 44.45 | lr: 6.38e-04 | norm: 0.4587 | dt: 17100.2865ms | tok/sec: 22.9947\n",
      "step 1453 | train loss: 3.68 | val loss: 3.79 | perplexity: 44.32 | lr: 6.37e-04 | norm: 0.3460 | dt: 17146.4343ms | tok/sec: 22.9328\n",
      "step 1454 | train loss: 3.97 | val loss: 3.79 | perplexity: 44.17 | lr: 6.36e-04 | norm: 0.4338 | dt: 17119.1144ms | tok/sec: 22.9694\n",
      "step 1455 | train loss: 3.54 | val loss: 3.79 | perplexity: 44.29 | lr: 6.35e-04 | norm: 0.4936 | dt: 17095.4835ms | tok/sec: 23.0012\n",
      "step 1456 | train loss: 3.98 | val loss: 3.80 | perplexity: 44.49 | lr: 6.35e-04 | norm: 0.3758 | dt: 17176.4545ms | tok/sec: 22.8927\n",
      "step 1457 | train loss: 3.78 | val loss: 3.79 | perplexity: 44.20 | lr: 6.34e-04 | norm: 0.4343 | dt: 17083.3719ms | tok/sec: 23.0175\n",
      "step 1458 | train loss: 3.67 | val loss: 3.79 | perplexity: 44.41 | lr: 6.33e-04 | norm: 0.4079 | dt: 17136.7009ms | tok/sec: 22.9458\n",
      "step 1459 | train loss: 3.75 | val loss: 3.80 | perplexity: 44.84 | lr: 6.32e-04 | norm: 0.4186 | dt: 17180.4552ms | tok/sec: 22.8874\n",
      "step 1460 | train loss: 3.74 | val loss: 3.79 | perplexity: 44.43 | lr: 6.31e-04 | norm: 0.4794 | dt: 17174.4421ms | tok/sec: 22.8954\n",
      "step 1461 | train loss: 4.42 | val loss: 3.79 | perplexity: 44.43 | lr: 6.31e-04 | norm: 0.5888 | dt: 17145.0608ms | tok/sec: 22.9347\n",
      "step 1462 | train loss: 3.98 | val loss: 3.80 | perplexity: 44.55 | lr: 6.30e-04 | norm: 0.4465 | dt: 17187.5255ms | tok/sec: 22.8780\n",
      "step 1463 | train loss: 3.91 | val loss: 3.80 | perplexity: 44.50 | lr: 6.29e-04 | norm: 0.5389 | dt: 17226.1415ms | tok/sec: 22.8267\n",
      "step 1464 | train loss: 3.89 | val loss: 3.80 | perplexity: 44.60 | lr: 6.28e-04 | norm: 0.4033 | dt: 17168.4363ms | tok/sec: 22.9034\n",
      "step 1465 | train loss: 3.74 | val loss: 3.79 | perplexity: 44.48 | lr: 6.27e-04 | norm: 0.4613 | dt: 17129.2861ms | tok/sec: 22.9558\n",
      "step 1466 | train loss: 4.06 | val loss: 3.79 | perplexity: 44.43 | lr: 6.26e-04 | norm: 0.4804 | dt: 17105.1347ms | tok/sec: 22.9882\n",
      "step 1467 | train loss: 3.83 | val loss: 3.79 | perplexity: 44.28 | lr: 6.26e-04 | norm: 0.4057 | dt: 17162.3464ms | tok/sec: 22.9116\n",
      "step 1468 | train loss: 3.55 | val loss: 3.79 | perplexity: 44.32 | lr: 6.25e-04 | norm: 0.4036 | dt: 17106.9803ms | tok/sec: 22.9857\n",
      "step 1469 | train loss: 3.77 | val loss: 3.79 | perplexity: 44.33 | lr: 6.24e-04 | norm: 0.4536 | dt: 17149.4439ms | tok/sec: 22.9288\n",
      "step 1470 | train loss: 3.75 | val loss: 3.79 | perplexity: 44.20 | lr: 6.23e-04 | norm: 0.3779 | dt: 17142.2176ms | tok/sec: 22.9385\n",
      "step 1471 | train loss: 4.05 | val loss: 3.79 | perplexity: 44.47 | lr: 6.22e-04 | norm: 0.3924 | dt: 17110.3961ms | tok/sec: 22.9811\n",
      "step 1472 | train loss: 3.63 | val loss: 3.80 | perplexity: 44.59 | lr: 6.21e-04 | norm: 0.4822 | dt: 17169.2762ms | tok/sec: 22.9023\n",
      "step 1473 | train loss: 3.83 | val loss: 3.79 | perplexity: 44.17 | lr: 6.21e-04 | norm: 0.4886 | dt: 17177.6466ms | tok/sec: 22.8911\n",
      "step 1474 | train loss: 3.80 | val loss: 3.79 | perplexity: 44.10 | lr: 6.20e-04 | norm: 0.3823 | dt: 17138.4828ms | tok/sec: 22.9435\n",
      "step 1475 | train loss: 3.84 | val loss: 3.79 | perplexity: 44.10 | lr: 6.19e-04 | norm: 0.3678 | dt: 17103.9634ms | tok/sec: 22.9898\n",
      "step 1476 | train loss: 3.88 | val loss: 3.78 | perplexity: 43.87 | lr: 6.18e-04 | norm: 0.4410 | dt: 17149.2641ms | tok/sec: 22.9290\n",
      "step 1477 | train loss: 3.87 | val loss: 3.78 | perplexity: 43.70 | lr: 6.17e-04 | norm: 0.4625 | dt: 17173.5842ms | tok/sec: 22.8966\n",
      "step 1478 | train loss: 3.81 | val loss: 3.78 | perplexity: 43.84 | lr: 6.16e-04 | norm: 0.3531 | dt: 17133.0473ms | tok/sec: 22.9507\n",
      "step 1479 | train loss: 4.14 | val loss: 3.78 | perplexity: 43.78 | lr: 6.16e-04 | norm: 0.5661 | dt: 17134.1290ms | tok/sec: 22.9493\n",
      "step 1480 | train loss: 3.96 | val loss: 3.77 | perplexity: 43.55 | lr: 6.15e-04 | norm: 0.3880 | dt: 17152.1530ms | tok/sec: 22.9252\n",
      "step 1481 | train loss: 3.63 | val loss: 3.77 | perplexity: 43.39 | lr: 6.14e-04 | norm: 0.3546 | dt: 17188.8242ms | tok/sec: 22.8763\n",
      "step 1482 | train loss: 3.81 | val loss: 3.77 | perplexity: 43.19 | lr: 6.13e-04 | norm: 0.3141 | dt: 17117.3034ms | tok/sec: 22.9718\n",
      "step 1483 | train loss: 4.01 | val loss: 3.76 | perplexity: 43.16 | lr: 6.12e-04 | norm: 0.3816 | dt: 17099.8948ms | tok/sec: 22.9952\n",
      "step 1484 | train loss: 3.78 | val loss: 3.77 | perplexity: 43.28 | lr: 6.11e-04 | norm: 0.3583 | dt: 17090.8957ms | tok/sec: 23.0073\n",
      "step 1485 | train loss: 3.67 | val loss: 3.77 | perplexity: 43.36 | lr: 6.11e-04 | norm: 0.4598 | dt: 17148.5813ms | tok/sec: 22.9299\n",
      "step 1486 | train loss: 3.61 | val loss: 3.77 | perplexity: 43.48 | lr: 6.10e-04 | norm: 0.3783 | dt: 17079.9315ms | tok/sec: 23.0221\n",
      "step 1487 | train loss: 3.68 | val loss: 3.77 | perplexity: 43.46 | lr: 6.09e-04 | norm: 0.4512 | dt: 17110.7986ms | tok/sec: 22.9806\n",
      "step 1488 | train loss: 3.80 | val loss: 3.77 | perplexity: 43.36 | lr: 6.08e-04 | norm: 0.3596 | dt: 17164.5691ms | tok/sec: 22.9086\n",
      "step 1489 | train loss: 3.68 | val loss: 3.77 | perplexity: 43.30 | lr: 6.07e-04 | norm: 0.3844 | dt: 17131.9747ms | tok/sec: 22.9522\n",
      "step 1490 | train loss: 3.76 | val loss: 3.77 | perplexity: 43.47 | lr: 6.06e-04 | norm: 0.3912 | dt: 17148.0687ms | tok/sec: 22.9306\n",
      "step 1491 | train loss: 3.87 | val loss: 3.78 | perplexity: 43.66 | lr: 6.06e-04 | norm: 0.3143 | dt: 17206.8737ms | tok/sec: 22.8523\n",
      "step 1492 | train loss: 3.83 | val loss: 3.78 | perplexity: 43.62 | lr: 6.05e-04 | norm: 0.3651 | dt: 17142.6752ms | tok/sec: 22.9378\n",
      "step 1493 | train loss: 3.78 | val loss: 3.77 | perplexity: 43.45 | lr: 6.04e-04 | norm: 0.4451 | dt: 17129.6465ms | tok/sec: 22.9553\n",
      "step 1494 | train loss: 3.66 | val loss: 3.77 | perplexity: 43.42 | lr: 6.03e-04 | norm: 0.3146 | dt: 17116.5512ms | tok/sec: 22.9729\n",
      "step 1495 | train loss: 3.91 | val loss: 3.77 | perplexity: 43.31 | lr: 6.02e-04 | norm: 0.4443 | dt: 17123.4398ms | tok/sec: 22.9636\n",
      "step 1496 | train loss: 3.80 | val loss: 3.77 | perplexity: 43.24 | lr: 6.01e-04 | norm: 0.3782 | dt: 17170.2330ms | tok/sec: 22.9010\n",
      "step 1497 | train loss: 3.88 | val loss: 3.76 | perplexity: 43.05 | lr: 6.01e-04 | norm: 0.4079 | dt: 17156.1167ms | tok/sec: 22.9199\n",
      "step 1498 | train loss: 3.31 | val loss: 3.76 | perplexity: 42.95 | lr: 6.00e-04 | norm: 0.4513 | dt: 17154.5782ms | tok/sec: 22.9219\n",
      "step 1499 | train loss: 3.93 | val loss: 3.76 | perplexity: 42.97 | lr: 5.99e-04 | norm: 0.4006 | dt: 17148.7949ms | tok/sec: 22.9297\n",
      "step 1500 | train loss: 4.07 | val loss: 3.76 | perplexity: 42.96 | lr: 5.98e-04 | norm: 0.4062 | dt: 17161.0584ms | tok/sec: 22.9133\n",
      "step 1501 | train loss: 4.00 | val loss: 3.77 | perplexity: 43.58 | lr: 5.97e-04 | norm: 0.5183 | dt: 17134.4588ms | tok/sec: 22.9488\n",
      "step 1502 | train loss: 3.87 | val loss: 3.77 | perplexity: 43.41 | lr: 5.96e-04 | norm: 0.5772 | dt: 17104.6805ms | tok/sec: 22.9888\n",
      "step 1503 | train loss: 3.66 | val loss: 3.77 | perplexity: 43.29 | lr: 5.96e-04 | norm: 0.4745 | dt: 17070.3690ms | tok/sec: 23.0350\n",
      "step 1504 | train loss: 3.88 | val loss: 3.77 | perplexity: 43.18 | lr: 5.95e-04 | norm: 0.3899 | dt: 17088.0046ms | tok/sec: 23.0112\n",
      "step 1505 | train loss: 4.01 | val loss: 3.76 | perplexity: 42.96 | lr: 5.94e-04 | norm: 0.4167 | dt: 17115.7675ms | tok/sec: 22.9739\n",
      "step 1506 | train loss: 3.82 | val loss: 3.76 | perplexity: 42.88 | lr: 5.93e-04 | norm: 0.3639 | dt: 17105.0932ms | tok/sec: 22.9882\n",
      "step 1507 | train loss: 3.84 | val loss: 3.75 | perplexity: 42.67 | lr: 5.92e-04 | norm: 0.3971 | dt: 17149.2989ms | tok/sec: 22.9290\n",
      "step 1508 | train loss: 3.55 | val loss: 3.75 | perplexity: 42.59 | lr: 5.91e-04 | norm: 0.4000 | dt: 17097.0757ms | tok/sec: 22.9990\n",
      "step 1509 | train loss: 3.56 | val loss: 3.76 | perplexity: 42.75 | lr: 5.91e-04 | norm: 0.3467 | dt: 17160.7866ms | tok/sec: 22.9136\n",
      "step 1510 | train loss: 4.07 | val loss: 3.76 | perplexity: 42.78 | lr: 5.90e-04 | norm: 0.4385 | dt: 17138.9663ms | tok/sec: 22.9428\n",
      "step 1511 | train loss: 3.78 | val loss: 3.76 | perplexity: 42.75 | lr: 5.89e-04 | norm: 0.3391 | dt: 17113.3564ms | tok/sec: 22.9771\n",
      "step 1512 | train loss: 3.93 | val loss: 3.75 | perplexity: 42.62 | lr: 5.88e-04 | norm: 0.3380 | dt: 17191.0050ms | tok/sec: 22.8734\n",
      "step 1513 | train loss: 3.82 | val loss: 3.75 | perplexity: 42.45 | lr: 5.87e-04 | norm: 0.3740 | dt: 17086.7388ms | tok/sec: 23.0129\n",
      "step 1514 | train loss: 4.02 | val loss: 3.75 | perplexity: 42.37 | lr: 5.86e-04 | norm: 0.3627 | dt: 17155.4966ms | tok/sec: 22.9207\n",
      "step 1515 | train loss: 3.88 | val loss: 3.74 | perplexity: 42.31 | lr: 5.86e-04 | norm: 0.3203 | dt: 17137.2635ms | tok/sec: 22.9451\n",
      "step 1516 | train loss: 4.41 | val loss: 3.75 | perplexity: 42.35 | lr: 5.85e-04 | norm: 0.4395 | dt: 17141.9253ms | tok/sec: 22.9388\n",
      "step 1517 | train loss: 3.66 | val loss: 3.75 | perplexity: 42.39 | lr: 5.84e-04 | norm: 0.3641 | dt: 17170.9430ms | tok/sec: 22.9001\n",
      "step 1518 | train loss: 3.76 | val loss: 3.74 | perplexity: 42.26 | lr: 5.83e-04 | norm: 0.3677 | dt: 17139.0078ms | tok/sec: 22.9428\n",
      "step 1519 | train loss: 3.78 | val loss: 3.74 | perplexity: 42.25 | lr: 5.82e-04 | norm: 0.3596 | dt: 17116.1184ms | tok/sec: 22.9734\n",
      "step 1520 | train loss: 3.99 | val loss: 3.75 | perplexity: 42.32 | lr: 5.81e-04 | norm: 0.4021 | dt: 17145.8445ms | tok/sec: 22.9336\n",
      "step 1521 | train loss: 3.85 | val loss: 3.74 | perplexity: 42.29 | lr: 5.81e-04 | norm: 0.3333 | dt: 17186.7721ms | tok/sec: 22.8790\n",
      "step 1522 | train loss: 4.04 | val loss: 3.74 | perplexity: 42.29 | lr: 5.80e-04 | norm: 0.3775 | dt: 17136.3764ms | tok/sec: 22.9463\n",
      "step 1523 | train loss: 4.21 | val loss: 3.75 | perplexity: 42.44 | lr: 5.79e-04 | norm: 0.3694 | dt: 17143.7221ms | tok/sec: 22.9364\n",
      "step 1524 | train loss: 3.58 | val loss: 3.75 | perplexity: 42.38 | lr: 5.78e-04 | norm: 0.5078 | dt: 17110.7833ms | tok/sec: 22.9806\n",
      "step 1525 | train loss: 3.91 | val loss: 3.74 | perplexity: 42.17 | lr: 5.77e-04 | norm: 0.4030 | dt: 17156.5616ms | tok/sec: 22.9193\n",
      "step 1526 | train loss: 3.85 | val loss: 3.74 | perplexity: 41.93 | lr: 5.76e-04 | norm: 0.3879 | dt: 17130.2817ms | tok/sec: 22.9544\n",
      "step 1527 | train loss: 3.90 | val loss: 3.74 | perplexity: 41.99 | lr: 5.76e-04 | norm: 0.3948 | dt: 17201.8504ms | tok/sec: 22.8589\n",
      "step 1528 | train loss: 3.98 | val loss: 3.74 | perplexity: 41.92 | lr: 5.75e-04 | norm: 0.3969 | dt: 17135.9513ms | tok/sec: 22.9468\n",
      "step 1529 | train loss: 3.70 | val loss: 3.74 | perplexity: 41.92 | lr: 5.74e-04 | norm: 0.4496 | dt: 17141.6798ms | tok/sec: 22.9392\n",
      "step 1530 | train loss: 3.85 | val loss: 3.73 | perplexity: 41.77 | lr: 5.73e-04 | norm: 0.4824 | dt: 17140.6293ms | tok/sec: 22.9406\n",
      "step 1531 | train loss: 4.04 | val loss: 3.73 | perplexity: 41.77 | lr: 5.72e-04 | norm: 0.4318 | dt: 17152.7488ms | tok/sec: 22.9244\n",
      "step 1532 | train loss: 3.59 | val loss: 3.73 | perplexity: 41.63 | lr: 5.71e-04 | norm: 0.4553 | dt: 17977.1564ms | tok/sec: 21.8731\n",
      "step 1533 | train loss: 3.77 | val loss: 3.73 | perplexity: 41.85 | lr: 5.71e-04 | norm: 0.4066 | dt: 23365.5400ms | tok/sec: 16.8289\n",
      "step 1534 | train loss: 3.99 | val loss: 3.73 | perplexity: 41.88 | lr: 5.70e-04 | norm: 0.4054 | dt: 20795.0680ms | tok/sec: 18.9091\n",
      "step 1535 | train loss: 4.15 | val loss: 3.73 | perplexity: 41.85 | lr: 5.69e-04 | norm: 0.4450 | dt: 20834.8331ms | tok/sec: 18.8730\n",
      "step 1536 | train loss: 3.64 | val loss: 3.73 | perplexity: 41.79 | lr: 5.68e-04 | norm: 0.3249 | dt: 20763.9964ms | tok/sec: 18.9374\n",
      "step 1537 | train loss: 3.78 | val loss: 3.73 | perplexity: 41.59 | lr: 5.67e-04 | norm: 0.4265 | dt: 20780.9038ms | tok/sec: 18.9220\n",
      "step 1538 | train loss: 3.77 | val loss: 3.74 | perplexity: 42.06 | lr: 5.66e-04 | norm: 0.4160 | dt: 20837.4887ms | tok/sec: 18.8706\n",
      "step 1539 | train loss: 4.03 | val loss: 3.74 | perplexity: 42.03 | lr: 5.66e-04 | norm: 0.5029 | dt: 20863.7879ms | tok/sec: 18.8468\n",
      "step 1540 | train loss: 3.90 | val loss: 3.73 | perplexity: 41.88 | lr: 5.65e-04 | norm: 0.3790 | dt: 20701.0355ms | tok/sec: 18.9950\n",
      "step 1541 | train loss: 4.03 | val loss: 3.73 | perplexity: 41.77 | lr: 5.64e-04 | norm: 0.3845 | dt: 21097.0273ms | tok/sec: 18.6385\n",
      "step 1542 | train loss: 3.63 | val loss: 3.73 | perplexity: 41.50 | lr: 5.63e-04 | norm: 0.3911 | dt: 20789.9942ms | tok/sec: 18.9137\n",
      "step 1543 | train loss: 3.79 | val loss: 3.73 | perplexity: 41.51 | lr: 5.62e-04 | norm: 0.3716 | dt: 20789.7253ms | tok/sec: 18.9140\n",
      "step 1544 | train loss: 4.17 | val loss: 3.73 | perplexity: 41.54 | lr: 5.61e-04 | norm: 0.4763 | dt: 20652.9047ms | tok/sec: 19.0393\n",
      "step 1545 | train loss: 4.16 | val loss: 3.73 | perplexity: 41.79 | lr: 5.60e-04 | norm: 0.4763 | dt: 20611.5208ms | tok/sec: 19.0775\n",
      "step 1546 | train loss: 3.85 | val loss: 3.73 | perplexity: 41.88 | lr: 5.60e-04 | norm: 0.4402 | dt: 20778.6078ms | tok/sec: 18.9241\n",
      "step 1547 | train loss: 3.86 | val loss: 3.73 | perplexity: 41.79 | lr: 5.59e-04 | norm: 0.4086 | dt: 20867.6369ms | tok/sec: 18.8433\n",
      "step 1548 | train loss: 3.93 | val loss: 3.73 | perplexity: 41.69 | lr: 5.58e-04 | norm: 0.4379 | dt: 20671.3881ms | tok/sec: 19.0222\n",
      "step 1549 | train loss: 3.89 | val loss: 3.73 | perplexity: 41.52 | lr: 5.57e-04 | norm: 0.3799 | dt: 20648.5677ms | tok/sec: 19.0433\n",
      "step 1550 | train loss: 4.00 | val loss: 3.72 | perplexity: 41.40 | lr: 5.56e-04 | norm: 0.3562 | dt: 20816.7560ms | tok/sec: 18.8894\n",
      "step 1551 | train loss: 3.83 | val loss: 3.72 | perplexity: 41.30 | lr: 5.55e-04 | norm: 0.4041 | dt: 20705.7352ms | tok/sec: 18.9907\n",
      "step 1552 | train loss: 4.15 | val loss: 3.73 | perplexity: 41.82 | lr: 5.55e-04 | norm: 0.6524 | dt: 20767.2768ms | tok/sec: 18.9344\n",
      "step 1553 | train loss: 3.96 | val loss: 3.74 | perplexity: 42.16 | lr: 5.54e-04 | norm: 0.4603 | dt: 20680.1193ms | tok/sec: 19.0142\n",
      "step 1554 | train loss: 3.97 | val loss: 3.73 | perplexity: 41.79 | lr: 5.53e-04 | norm: 0.5113 | dt: 20747.7899ms | tok/sec: 18.9522\n",
      "step 1555 | train loss: 4.05 | val loss: 3.73 | perplexity: 41.69 | lr: 5.52e-04 | norm: 0.4586 | dt: 20712.4801ms | tok/sec: 18.9845\n",
      "step 1556 | train loss: 4.04 | val loss: 3.73 | perplexity: 41.84 | lr: 5.51e-04 | norm: 0.4372 | dt: 20757.7322ms | tok/sec: 18.9431\n",
      "step 1557 | train loss: 3.84 | val loss: 3.73 | perplexity: 41.86 | lr: 5.50e-04 | norm: 0.4194 | dt: 20749.6769ms | tok/sec: 18.9505\n",
      "step 1558 | train loss: 3.94 | val loss: 3.74 | perplexity: 41.91 | lr: 5.50e-04 | norm: 0.4661 | dt: 20786.6313ms | tok/sec: 18.9168\n",
      "step 1559 | train loss: 4.07 | val loss: 3.73 | perplexity: 41.70 | lr: 5.49e-04 | norm: 0.4251 | dt: 20693.4912ms | tok/sec: 19.0019\n",
      "step 1560 | train loss: 3.93 | val loss: 3.73 | perplexity: 41.55 | lr: 5.48e-04 | norm: 0.4016 | dt: 20755.4629ms | tok/sec: 18.9452\n",
      "step 1561 | train loss: 3.94 | val loss: 3.73 | perplexity: 41.49 | lr: 5.47e-04 | norm: 0.4030 | dt: 20895.5166ms | tok/sec: 18.8182\n",
      "step 1562 | train loss: 3.99 | val loss: 3.72 | perplexity: 41.40 | lr: 5.46e-04 | norm: 0.3650 | dt: 20859.7093ms | tok/sec: 18.8505\n",
      "step 1563 | train loss: 4.84 | val loss: 3.72 | perplexity: 41.46 | lr: 5.45e-04 | norm: 0.8312 | dt: 20787.5266ms | tok/sec: 18.9160\n",
      "step 1564 | train loss: 4.03 | val loss: 3.73 | perplexity: 41.51 | lr: 5.45e-04 | norm: 0.3684 | dt: 20688.7767ms | tok/sec: 19.0062\n",
      "step 1565 | train loss: 3.92 | val loss: 3.73 | perplexity: 41.64 | lr: 5.44e-04 | norm: 0.3807 | dt: 20776.1495ms | tok/sec: 18.9263\n",
      "step 1566 | train loss: 3.93 | val loss: 3.73 | perplexity: 41.63 | lr: 5.43e-04 | norm: 0.3501 | dt: 20814.3201ms | tok/sec: 18.8916\n",
      "step 1567 | train loss: 3.83 | val loss: 3.73 | perplexity: 41.53 | lr: 5.42e-04 | norm: 0.3293 | dt: 20653.8804ms | tok/sec: 19.0384\n",
      "step 1568 | train loss: 3.69 | val loss: 3.72 | perplexity: 41.46 | lr: 5.41e-04 | norm: 0.3507 | dt: 20677.5873ms | tok/sec: 19.0165\n",
      "step 1569 | train loss: 3.86 | val loss: 3.72 | perplexity: 41.20 | lr: 5.40e-04 | norm: 0.3776 | dt: 20726.0447ms | tok/sec: 18.9721\n",
      "step 1570 | train loss: 3.55 | val loss: 3.72 | perplexity: 41.07 | lr: 5.40e-04 | norm: 0.3618 | dt: 20669.8720ms | tok/sec: 19.0236\n",
      "step 1571 | train loss: 3.57 | val loss: 3.71 | perplexity: 41.05 | lr: 5.39e-04 | norm: 0.3135 | dt: 20629.1032ms | tok/sec: 19.0612\n",
      "step 1572 | train loss: 4.05 | val loss: 3.71 | perplexity: 40.96 | lr: 5.38e-04 | norm: 0.4454 | dt: 20857.2478ms | tok/sec: 18.8527\n",
      "step 1573 | train loss: 3.62 | val loss: 3.71 | perplexity: 40.87 | lr: 5.37e-04 | norm: 0.3795 | dt: 20653.9071ms | tok/sec: 19.0383\n",
      "step 1574 | train loss: 3.61 | val loss: 3.71 | perplexity: 40.96 | lr: 5.36e-04 | norm: 0.3283 | dt: 20783.1538ms | tok/sec: 18.9199\n",
      "step 1575 | train loss: 3.93 | val loss: 3.72 | perplexity: 41.29 | lr: 5.35e-04 | norm: 0.4112 | dt: 20785.3308ms | tok/sec: 18.9180\n",
      "step 1576 | train loss: 4.32 | val loss: 3.74 | perplexity: 41.95 | lr: 5.34e-04 | norm: 0.4720 | dt: 20790.5760ms | tok/sec: 18.9132\n",
      "step 1577 | train loss: 3.42 | val loss: 3.74 | perplexity: 42.07 | lr: 5.34e-04 | norm: 0.4889 | dt: 20867.3372ms | tok/sec: 18.8436\n",
      "step 1578 | train loss: 4.06 | val loss: 3.73 | perplexity: 41.60 | lr: 5.33e-04 | norm: 0.7378 | dt: 20795.0413ms | tok/sec: 18.9091\n",
      "step 1579 | train loss: 3.84 | val loss: 3.73 | perplexity: 41.87 | lr: 5.32e-04 | norm: 0.6509 | dt: 20796.7300ms | tok/sec: 18.9076\n",
      "step 1580 | train loss: 3.81 | val loss: 3.73 | perplexity: 41.77 | lr: 5.31e-04 | norm: 0.4946 | dt: 20807.4715ms | tok/sec: 18.8978\n",
      "step 1581 | train loss: 3.94 | val loss: 3.73 | perplexity: 41.54 | lr: 5.30e-04 | norm: 0.4029 | dt: 20832.2644ms | tok/sec: 18.8753\n",
      "step 1582 | train loss: 4.04 | val loss: 3.73 | perplexity: 41.72 | lr: 5.29e-04 | norm: 0.4333 | dt: 20887.6851ms | tok/sec: 18.8253\n",
      "step 1583 | train loss: 3.79 | val loss: 3.73 | perplexity: 41.83 | lr: 5.29e-04 | norm: 0.4383 | dt: 20779.3000ms | tok/sec: 18.9234\n",
      "step 1584 | train loss: 4.31 | val loss: 3.73 | perplexity: 41.84 | lr: 5.28e-04 | norm: 0.5424 | dt: 20820.9252ms | tok/sec: 18.8856\n",
      "step 1585 | train loss: 4.05 | val loss: 3.73 | perplexity: 41.78 | lr: 5.27e-04 | norm: 0.4156 | dt: 20851.9483ms | tok/sec: 18.8575\n",
      "step 1586 | train loss: 4.26 | val loss: 3.74 | perplexity: 42.12 | lr: 5.26e-04 | norm: 1.5805 | dt: 20674.5074ms | tok/sec: 19.0194\n",
      "step 1587 | train loss: 3.97 | val loss: 3.75 | perplexity: 42.43 | lr: 5.25e-04 | norm: 0.6807 | dt: 20800.3740ms | tok/sec: 18.9043\n",
      "step 1588 | train loss: 3.89 | val loss: 3.75 | perplexity: 42.34 | lr: 5.24e-04 | norm: 0.5905 | dt: 20794.8003ms | tok/sec: 18.9093\n",
      "step 1589 | train loss: 3.66 | val loss: 3.75 | perplexity: 42.56 | lr: 5.24e-04 | norm: 0.3708 | dt: 20642.6868ms | tok/sec: 19.0487\n",
      "step 1590 | train loss: 3.67 | val loss: 3.75 | perplexity: 42.33 | lr: 5.23e-04 | norm: 0.4190 | dt: 20972.1162ms | tok/sec: 18.7495\n",
      "step 1591 | train loss: 4.22 | val loss: 3.73 | perplexity: 41.87 | lr: 5.22e-04 | norm: 0.4901 | dt: 20703.1190ms | tok/sec: 18.9931\n",
      "step 1592 | train loss: 3.80 | val loss: 3.73 | perplexity: 41.81 | lr: 5.21e-04 | norm: 0.4225 | dt: 20947.0589ms | tok/sec: 18.7719\n",
      "step 1593 | train loss: 3.83 | val loss: 3.73 | perplexity: 41.67 | lr: 5.20e-04 | norm: 0.3923 | dt: 20910.2008ms | tok/sec: 18.8050\n",
      "step 1594 | train loss: 3.81 | val loss: 3.72 | perplexity: 41.35 | lr: 5.19e-04 | norm: 0.4115 | dt: 20754.3535ms | tok/sec: 18.9462\n",
      "step 1595 | train loss: 3.98 | val loss: 3.72 | perplexity: 41.28 | lr: 5.19e-04 | norm: 0.4415 | dt: 20672.5814ms | tok/sec: 19.0211\n",
      "step 1596 | train loss: 3.78 | val loss: 3.72 | perplexity: 41.26 | lr: 5.18e-04 | norm: 0.3814 | dt: 20693.7466ms | tok/sec: 19.0017\n",
      "step 1597 | train loss: 4.04 | val loss: 3.72 | perplexity: 41.27 | lr: 5.17e-04 | norm: 0.4110 | dt: 20739.2032ms | tok/sec: 18.9600\n",
      "step 1598 | train loss: 3.91 | val loss: 3.72 | perplexity: 41.24 | lr: 5.16e-04 | norm: 0.3628 | dt: 20694.2241ms | tok/sec: 19.0012\n",
      "step 1599 | train loss: 3.99 | val loss: 3.72 | perplexity: 41.15 | lr: 5.15e-04 | norm: 0.3584 | dt: 20656.8193ms | tok/sec: 19.0357\n",
      "step 1600 | train loss: 3.88 | val loss: 3.71 | perplexity: 41.01 | lr: 5.14e-04 | norm: 0.3872 | dt: 20669.2178ms | tok/sec: 19.0242\n",
      "step 1601 | train loss: 4.11 | val loss: 3.71 | perplexity: 41.03 | lr: 5.14e-04 | norm: 0.3624 | dt: 20764.0278ms | tok/sec: 18.9374\n",
      "step 1602 | train loss: 3.89 | val loss: 3.71 | perplexity: 40.97 | lr: 5.13e-04 | norm: 0.3913 | dt: 20788.4247ms | tok/sec: 18.9151\n",
      "step 1603 | train loss: 4.05 | val loss: 3.72 | perplexity: 41.09 | lr: 5.12e-04 | norm: 0.3481 | dt: 20723.7408ms | tok/sec: 18.9742\n",
      "step 1604 | train loss: 3.99 | val loss: 3.72 | perplexity: 41.09 | lr: 5.11e-04 | norm: 0.4297 | dt: 20863.5700ms | tok/sec: 18.8470\n",
      "step 1605 | train loss: 3.82 | val loss: 3.71 | perplexity: 41.06 | lr: 5.10e-04 | norm: 0.4401 | dt: 20631.0091ms | tok/sec: 19.0595\n",
      "step 1606 | train loss: 3.76 | val loss: 3.72 | perplexity: 41.20 | lr: 5.09e-04 | norm: 0.4316 | dt: 20851.7392ms | tok/sec: 18.8577\n",
      "step 1607 | train loss: 3.92 | val loss: 3.72 | perplexity: 41.24 | lr: 5.09e-04 | norm: 0.4089 | dt: 20673.5466ms | tok/sec: 19.0202\n",
      "step 1608 | train loss: 3.85 | val loss: 3.71 | perplexity: 41.03 | lr: 5.08e-04 | norm: 0.4466 | dt: 20759.3608ms | tok/sec: 18.9416\n",
      "step 1609 | train loss: 3.86 | val loss: 3.71 | perplexity: 40.98 | lr: 5.07e-04 | norm: 0.4025 | dt: 20774.6112ms | tok/sec: 18.9277\n",
      "step 1610 | train loss: 4.05 | val loss: 3.71 | perplexity: 40.86 | lr: 5.06e-04 | norm: 0.5229 | dt: 20855.9010ms | tok/sec: 18.8539\n",
      "step 1611 | train loss: 3.89 | val loss: 3.72 | perplexity: 41.19 | lr: 5.05e-04 | norm: 0.4091 | dt: 20713.2957ms | tok/sec: 18.9837\n",
      "step 1612 | train loss: 3.94 | val loss: 3.71 | perplexity: 40.94 | lr: 5.04e-04 | norm: 0.5037 | dt: 20701.1545ms | tok/sec: 18.9949\n",
      "step 1613 | train loss: 3.84 | val loss: 3.71 | perplexity: 40.73 | lr: 5.04e-04 | norm: 0.4129 | dt: 20650.2044ms | tok/sec: 19.0417\n",
      "step 1614 | train loss: 4.14 | val loss: 3.71 | perplexity: 40.88 | lr: 5.03e-04 | norm: 0.3706 | dt: 20709.6384ms | tok/sec: 18.9871\n",
      "step 1615 | train loss: 3.66 | val loss: 3.71 | perplexity: 40.75 | lr: 5.02e-04 | norm: 0.4015 | dt: 20700.8793ms | tok/sec: 18.9951\n",
      "step 1616 | train loss: 3.68 | val loss: 3.71 | perplexity: 40.74 | lr: 5.01e-04 | norm: 0.3788 | dt: 20718.4107ms | tok/sec: 18.9791\n",
      "step 1617 | train loss: 5.03 | val loss: 3.73 | perplexity: 41.56 | lr: 5.00e-04 | norm: 1.5035 | dt: 20718.0648ms | tok/sec: 18.9794\n",
      "step 1618 | train loss: 3.97 | val loss: 3.72 | perplexity: 41.32 | lr: 4.99e-04 | norm: 0.5156 | dt: 20782.2452ms | tok/sec: 18.9208\n",
      "step 1619 | train loss: 3.89 | val loss: 3.71 | perplexity: 40.87 | lr: 4.99e-04 | norm: 0.3985 | dt: 20621.2163ms | tok/sec: 19.0685\n",
      "step 1620 | train loss: 3.79 | val loss: 3.71 | perplexity: 40.76 | lr: 4.98e-04 | norm: 0.3686 | dt: 20750.3946ms | tok/sec: 18.9498\n",
      "step 1621 | train loss: 3.75 | val loss: 3.71 | perplexity: 40.93 | lr: 4.97e-04 | norm: 0.3590 | dt: 20645.1828ms | tok/sec: 19.0464\n",
      "step 1622 | train loss: 3.86 | val loss: 3.71 | perplexity: 40.92 | lr: 4.96e-04 | norm: 0.3992 | dt: 20846.6675ms | tok/sec: 18.8623\n",
      "step 1623 | train loss: 4.00 | val loss: 3.71 | perplexity: 40.79 | lr: 4.95e-04 | norm: 0.4229 | dt: 20835.5615ms | tok/sec: 18.8723\n",
      "step 1624 | train loss: 4.06 | val loss: 3.70 | perplexity: 40.58 | lr: 4.94e-04 | norm: 0.3950 | dt: 20712.1403ms | tok/sec: 18.9848\n",
      "step 1625 | train loss: 4.04 | val loss: 3.71 | perplexity: 40.74 | lr: 4.94e-04 | norm: 0.7935 | dt: 20808.2030ms | tok/sec: 18.8972\n",
      "step 1626 | train loss: 3.85 | val loss: 3.71 | perplexity: 40.87 | lr: 4.93e-04 | norm: 0.3869 | dt: 20791.5416ms | tok/sec: 18.9123\n",
      "step 1627 | train loss: 4.00 | val loss: 3.71 | perplexity: 40.70 | lr: 4.92e-04 | norm: 0.6076 | dt: 20626.7080ms | tok/sec: 19.0634\n",
      "step 1628 | train loss: 3.62 | val loss: 3.71 | perplexity: 40.76 | lr: 4.91e-04 | norm: 0.4413 | dt: 20694.7024ms | tok/sec: 19.0008\n",
      "step 1629 | train loss: 3.87 | val loss: 3.71 | perplexity: 40.78 | lr: 4.90e-04 | norm: 0.4521 | dt: 20633.2328ms | tok/sec: 19.0574\n",
      "step 1630 | train loss: 4.08 | val loss: 3.71 | perplexity: 40.71 | lr: 4.89e-04 | norm: 0.4302 | dt: 20706.7699ms | tok/sec: 18.9897\n",
      "step 1631 | train loss: 4.05 | val loss: 3.71 | perplexity: 40.95 | lr: 4.89e-04 | norm: 0.4343 | dt: 20594.5497ms | tok/sec: 19.0932\n",
      "step 1632 | train loss: 4.01 | val loss: 3.71 | perplexity: 40.98 | lr: 4.88e-04 | norm: 0.4614 | dt: 20625.4303ms | tok/sec: 19.0646\n",
      "step 1633 | train loss: 4.22 | val loss: 3.71 | perplexity: 41.04 | lr: 4.87e-04 | norm: 0.4422 | dt: 20469.8918ms | tok/sec: 19.2095\n",
      "step 1634 | train loss: 3.75 | val loss: 3.71 | perplexity: 40.84 | lr: 4.86e-04 | norm: 0.4490 | dt: 20627.4219ms | tok/sec: 19.0628\n",
      "step 1635 | train loss: 3.63 | val loss: 3.70 | perplexity: 40.50 | lr: 4.85e-04 | norm: 0.3942 | dt: 20522.5449ms | tok/sec: 19.1602\n",
      "step 1636 | train loss: 4.00 | val loss: 3.70 | perplexity: 40.41 | lr: 4.84e-04 | norm: 0.3723 | dt: 20603.9498ms | tok/sec: 19.0845\n",
      "step 1637 | train loss: 4.17 | val loss: 3.71 | perplexity: 40.67 | lr: 4.84e-04 | norm: 0.4242 | dt: 20644.3093ms | tok/sec: 19.0472\n",
      "step 1638 | train loss: 4.18 | val loss: 3.71 | perplexity: 40.69 | lr: 4.83e-04 | norm: 0.4532 | dt: 20626.3161ms | tok/sec: 19.0638\n",
      "step 1639 | train loss: 3.79 | val loss: 3.70 | perplexity: 40.47 | lr: 4.82e-04 | norm: 0.4388 | dt: 20469.1525ms | tok/sec: 19.2102\n",
      "step 1640 | train loss: 3.80 | val loss: 3.70 | perplexity: 40.27 | lr: 4.81e-04 | norm: 0.3949 | dt: 20599.2789ms | tok/sec: 19.0888\n",
      "step 1641 | train loss: 3.86 | val loss: 3.69 | perplexity: 40.17 | lr: 4.80e-04 | norm: 0.4125 | dt: 20585.2482ms | tok/sec: 19.1018\n",
      "step 1642 | train loss: 4.14 | val loss: 3.69 | perplexity: 40.09 | lr: 4.79e-04 | norm: 0.4191 | dt: 20755.4164ms | tok/sec: 18.9452\n",
      "step 1643 | train loss: 3.85 | val loss: 3.69 | perplexity: 40.02 | lr: 4.79e-04 | norm: 0.3586 | dt: 20708.4436ms | tok/sec: 18.9882\n",
      "step 1644 | train loss: 3.64 | val loss: 3.69 | perplexity: 39.94 | lr: 4.78e-04 | norm: 0.4228 | dt: 20621.2420ms | tok/sec: 19.0685\n",
      "step 1645 | train loss: 3.78 | val loss: 3.69 | perplexity: 39.88 | lr: 4.77e-04 | norm: 0.3430 | dt: 20647.0501ms | tok/sec: 19.0447\n",
      "step 1646 | train loss: 3.80 | val loss: 3.68 | perplexity: 39.83 | lr: 4.76e-04 | norm: 0.3427 | dt: 20634.0477ms | tok/sec: 19.0567\n",
      "step 1647 | train loss: 3.75 | val loss: 3.69 | perplexity: 39.85 | lr: 4.75e-04 | norm: 0.3736 | dt: 20666.9436ms | tok/sec: 19.0263\n",
      "step 1648 | train loss: 3.82 | val loss: 3.69 | perplexity: 39.86 | lr: 4.74e-04 | norm: 0.3323 | dt: 20599.6366ms | tok/sec: 19.0885\n",
      "step 1649 | train loss: 3.58 | val loss: 3.68 | perplexity: 39.75 | lr: 4.74e-04 | norm: 0.4116 | dt: 20567.0817ms | tok/sec: 19.1187\n",
      "step 1650 | train loss: 3.99 | val loss: 3.69 | perplexity: 39.93 | lr: 4.73e-04 | norm: 0.4157 | dt: 20613.7319ms | tok/sec: 19.0754\n",
      "step 1651 | train loss: 3.77 | val loss: 3.69 | perplexity: 39.96 | lr: 4.72e-04 | norm: 0.3810 | dt: 20732.8024ms | tok/sec: 18.9659\n",
      "step 1652 | train loss: 3.58 | val loss: 3.68 | perplexity: 39.82 | lr: 4.71e-04 | norm: 0.3461 | dt: 20851.7296ms | tok/sec: 18.8577\n",
      "step 1653 | train loss: 3.98 | val loss: 3.69 | perplexity: 39.86 | lr: 4.70e-04 | norm: 0.4259 | dt: 20653.0917ms | tok/sec: 19.0391\n",
      "step 1654 | train loss: 3.53 | val loss: 3.69 | perplexity: 39.85 | lr: 4.69e-04 | norm: 0.3896 | dt: 20544.5089ms | tok/sec: 19.1397\n",
      "step 1655 | train loss: 3.70 | val loss: 3.68 | perplexity: 39.81 | lr: 4.69e-04 | norm: 0.3853 | dt: 20719.9261ms | tok/sec: 18.9777\n",
      "step 1656 | train loss: 3.76 | val loss: 3.68 | perplexity: 39.84 | lr: 4.68e-04 | norm: 0.4586 | dt: 20580.0841ms | tok/sec: 19.1066\n",
      "step 1657 | train loss: 3.78 | val loss: 3.68 | perplexity: 39.69 | lr: 4.67e-04 | norm: 0.3555 | dt: 20821.0835ms | tok/sec: 18.8855\n",
      "step 1658 | train loss: 3.86 | val loss: 3.68 | perplexity: 39.64 | lr: 4.66e-04 | norm: 0.4473 | dt: 20567.2925ms | tok/sec: 19.1185\n",
      "step 1659 | train loss: 3.81 | val loss: 3.68 | perplexity: 39.71 | lr: 4.65e-04 | norm: 0.3471 | dt: 20759.1217ms | tok/sec: 18.9418\n",
      "step 1660 | train loss: 3.67 | val loss: 3.69 | perplexity: 39.86 | lr: 4.65e-04 | norm: 0.4114 | dt: 20611.1536ms | tok/sec: 19.0778\n",
      "step 1661 | train loss: 3.80 | val loss: 3.69 | perplexity: 40.01 | lr: 4.64e-04 | norm: 0.3450 | dt: 20884.0203ms | tok/sec: 18.8286\n",
      "step 1662 | train loss: 3.88 | val loss: 3.69 | perplexity: 39.94 | lr: 4.63e-04 | norm: 0.3836 | dt: 20714.1497ms | tok/sec: 18.9830\n",
      "step 1663 | train loss: 3.57 | val loss: 3.68 | perplexity: 39.82 | lr: 4.62e-04 | norm: 0.3472 | dt: 20827.1651ms | tok/sec: 18.8800\n",
      "step 1664 | train loss: 3.80 | val loss: 3.68 | perplexity: 39.68 | lr: 4.61e-04 | norm: 0.3095 | dt: 20594.9626ms | tok/sec: 19.0928\n",
      "step 1665 | train loss: 3.74 | val loss: 3.68 | perplexity: 39.53 | lr: 4.60e-04 | norm: 0.3549 | dt: 20662.4799ms | tok/sec: 19.0304\n",
      "step 1666 | train loss: 3.58 | val loss: 3.67 | perplexity: 39.39 | lr: 4.60e-04 | norm: 0.3866 | dt: 20780.1380ms | tok/sec: 18.9227\n",
      "step 1667 | train loss: 3.56 | val loss: 3.68 | perplexity: 39.55 | lr: 4.59e-04 | norm: 0.3296 | dt: 20813.8785ms | tok/sec: 18.8920\n",
      "step 1668 | train loss: 3.86 | val loss: 3.68 | perplexity: 39.54 | lr: 4.58e-04 | norm: 0.4120 | dt: 20901.5806ms | tok/sec: 18.8127\n",
      "step 1669 | train loss: 3.77 | val loss: 3.68 | perplexity: 39.53 | lr: 4.57e-04 | norm: 0.3902 | dt: 20669.1093ms | tok/sec: 19.0243\n",
      "step 1670 | train loss: 3.68 | val loss: 3.68 | perplexity: 39.66 | lr: 4.56e-04 | norm: 0.3393 | dt: 20885.1635ms | tok/sec: 18.8275\n",
      "step 1671 | train loss: 3.72 | val loss: 3.68 | perplexity: 39.56 | lr: 4.55e-04 | norm: 0.3446 | dt: 20876.9305ms | tok/sec: 18.8350\n",
      "step 1672 | train loss: 3.82 | val loss: 3.67 | perplexity: 39.44 | lr: 4.55e-04 | norm: 0.3609 | dt: 20768.5077ms | tok/sec: 18.9333\n",
      "step 1673 | train loss: 3.55 | val loss: 3.67 | perplexity: 39.44 | lr: 4.54e-04 | norm: 0.4356 | dt: 20892.5881ms | tok/sec: 18.8208\n",
      "step 1674 | train loss: 3.76 | val loss: 3.67 | perplexity: 39.26 | lr: 4.53e-04 | norm: 0.4252 | dt: 20740.5374ms | tok/sec: 18.9588\n",
      "step 1675 | train loss: 3.94 | val loss: 3.67 | perplexity: 39.25 | lr: 4.52e-04 | norm: 0.3816 | dt: 20714.0639ms | tok/sec: 18.9830\n",
      "step 1676 | train loss: 3.95 | val loss: 3.67 | perplexity: 39.44 | lr: 4.51e-04 | norm: 0.3553 | dt: 20853.0748ms | tok/sec: 18.8565\n",
      "step 1677 | train loss: 4.11 | val loss: 3.68 | perplexity: 39.49 | lr: 4.51e-04 | norm: 0.4450 | dt: 20679.7538ms | tok/sec: 19.0145\n",
      "step 1678 | train loss: 3.73 | val loss: 3.67 | perplexity: 39.35 | lr: 4.50e-04 | norm: 0.4292 | dt: 20661.8602ms | tok/sec: 19.0310\n",
      "step 1679 | train loss: 3.67 | val loss: 3.67 | perplexity: 39.30 | lr: 4.49e-04 | norm: 0.3416 | dt: 20851.5742ms | tok/sec: 18.8579\n",
      "step 1680 | train loss: 3.59 | val loss: 3.67 | perplexity: 39.32 | lr: 4.48e-04 | norm: 0.3807 | dt: 20907.8097ms | tok/sec: 18.8071\n",
      "step 1681 | train loss: 3.71 | val loss: 3.67 | perplexity: 39.32 | lr: 4.47e-04 | norm: 0.4699 | dt: 20848.8750ms | tok/sec: 18.8603\n",
      "step 1682 | train loss: 3.48 | val loss: 3.67 | perplexity: 39.22 | lr: 4.46e-04 | norm: 0.3818 | dt: 20747.7238ms | tok/sec: 18.9522\n",
      "step 1683 | train loss: 3.87 | val loss: 3.67 | perplexity: 39.14 | lr: 4.46e-04 | norm: 0.3612 | dt: 20834.5454ms | tok/sec: 18.8733\n",
      "step 1684 | train loss: 4.16 | val loss: 3.67 | perplexity: 39.25 | lr: 4.45e-04 | norm: 0.4077 | dt: 20841.8505ms | tok/sec: 18.8667\n",
      "step 1685 | train loss: 3.93 | val loss: 3.67 | perplexity: 39.32 | lr: 4.44e-04 | norm: 0.4110 | dt: 20954.7420ms | tok/sec: 18.7650\n",
      "step 1686 | train loss: 3.94 | val loss: 3.67 | perplexity: 39.31 | lr: 4.43e-04 | norm: 0.4166 | dt: 20776.0544ms | tok/sec: 18.9264\n",
      "step 1687 | train loss: 3.77 | val loss: 3.67 | perplexity: 39.23 | lr: 4.42e-04 | norm: 0.3969 | dt: 20870.8658ms | tok/sec: 18.8404\n",
      "step 1688 | train loss: 3.90 | val loss: 3.67 | perplexity: 39.28 | lr: 4.42e-04 | norm: 0.9061 | dt: 18532.5341ms | tok/sec: 21.2176\n",
      "step 1689 | train loss: 3.83 | val loss: 3.67 | perplexity: 39.25 | lr: 4.41e-04 | norm: 0.3901 | dt: 18106.0421ms | tok/sec: 21.7174\n",
      "step 1690 | train loss: 3.93 | val loss: 3.67 | perplexity: 39.16 | lr: 4.40e-04 | norm: 0.3752 | dt: 17938.7982ms | tok/sec: 21.9199\n",
      "step 1691 | train loss: 3.95 | val loss: 3.67 | perplexity: 39.23 | lr: 4.39e-04 | norm: 0.4511 | dt: 17967.7556ms | tok/sec: 21.8845\n",
      "step 1692 | train loss: 3.99 | val loss: 3.67 | perplexity: 39.32 | lr: 4.38e-04 | norm: 0.3678 | dt: 18009.8326ms | tok/sec: 21.8334\n",
      "step 1693 | train loss: 3.76 | val loss: 3.67 | perplexity: 39.39 | lr: 4.38e-04 | norm: 0.3774 | dt: 17990.4177ms | tok/sec: 21.8570\n",
      "step 1694 | train loss: 3.73 | val loss: 3.67 | perplexity: 39.31 | lr: 4.37e-04 | norm: 0.3865 | dt: 17949.1231ms | tok/sec: 21.9073\n",
      "step 1695 | train loss: 4.07 | val loss: 3.67 | perplexity: 39.23 | lr: 4.36e-04 | norm: 0.3530 | dt: 17942.0741ms | tok/sec: 21.9159\n",
      "step 1696 | train loss: 3.61 | val loss: 3.67 | perplexity: 39.08 | lr: 4.35e-04 | norm: 0.3292 | dt: 17967.2668ms | tok/sec: 21.8851\n",
      "step 1697 | train loss: 3.63 | val loss: 3.66 | perplexity: 38.96 | lr: 4.34e-04 | norm: 0.3440 | dt: 18185.7722ms | tok/sec: 21.6222\n",
      "step 1698 | train loss: 3.62 | val loss: 3.66 | perplexity: 38.83 | lr: 4.33e-04 | norm: 0.3571 | dt: 17931.2847ms | tok/sec: 21.9290\n",
      "step 1699 | train loss: 3.69 | val loss: 3.66 | perplexity: 38.87 | lr: 4.33e-04 | norm: 0.3380 | dt: 18018.2860ms | tok/sec: 21.8232\n",
      "step 1700 | train loss: 3.81 | val loss: 3.66 | perplexity: 38.85 | lr: 4.32e-04 | norm: 0.4249 | dt: 18086.6063ms | tok/sec: 21.7407\n",
      "step 1701 | train loss: 3.62 | val loss: 3.66 | perplexity: 38.75 | lr: 4.31e-04 | norm: 0.3277 | dt: 17892.1611ms | tok/sec: 21.9770\n",
      "step 1702 | train loss: 3.55 | val loss: 3.66 | perplexity: 38.69 | lr: 4.30e-04 | norm: 0.2968 | dt: 17888.4795ms | tok/sec: 21.9815\n",
      "step 1703 | train loss: 3.78 | val loss: 3.66 | perplexity: 38.67 | lr: 4.29e-04 | norm: 0.3457 | dt: 17952.1697ms | tok/sec: 21.9035\n",
      "step 1704 | train loss: 3.67 | val loss: 3.66 | perplexity: 38.77 | lr: 4.29e-04 | norm: 0.3503 | dt: 17936.2524ms | tok/sec: 21.9230\n",
      "step 1705 | train loss: 3.59 | val loss: 3.66 | perplexity: 38.77 | lr: 4.28e-04 | norm: 0.3258 | dt: 18104.2941ms | tok/sec: 21.7195\n",
      "step 1706 | train loss: 3.85 | val loss: 3.66 | perplexity: 38.70 | lr: 4.27e-04 | norm: 0.4740 | dt: 17968.7657ms | tok/sec: 21.8833\n",
      "step 1707 | train loss: 3.60 | val loss: 3.65 | perplexity: 38.57 | lr: 4.26e-04 | norm: 0.3337 | dt: 17932.6539ms | tok/sec: 21.9274\n",
      "step 1708 | train loss: 4.11 | val loss: 3.65 | perplexity: 38.60 | lr: 4.25e-04 | norm: 0.3240 | dt: 18016.3853ms | tok/sec: 21.8255\n",
      "step 1709 | train loss: 3.84 | val loss: 3.65 | perplexity: 38.58 | lr: 4.25e-04 | norm: 0.3779 | dt: 17973.0978ms | tok/sec: 21.8780\n",
      "step 1710 | train loss: 3.78 | val loss: 3.65 | perplexity: 38.40 | lr: 4.24e-04 | norm: 0.3851 | dt: 17947.5260ms | tok/sec: 21.9092\n",
      "step 1711 | train loss: 3.81 | val loss: 3.65 | perplexity: 38.48 | lr: 4.23e-04 | norm: 0.3402 | dt: 17924.5765ms | tok/sec: 21.9373\n",
      "step 1712 | train loss: 3.88 | val loss: 3.65 | perplexity: 38.55 | lr: 4.22e-04 | norm: 0.4722 | dt: 17929.0164ms | tok/sec: 21.9318\n",
      "step 1713 | train loss: 3.68 | val loss: 3.65 | perplexity: 38.60 | lr: 4.21e-04 | norm: 0.3260 | dt: 17990.8261ms | tok/sec: 21.8565\n",
      "step 1714 | train loss: 3.59 | val loss: 3.65 | perplexity: 38.52 | lr: 4.21e-04 | norm: 0.3630 | dt: 18000.4764ms | tok/sec: 21.8448\n",
      "step 1715 | train loss: 3.93 | val loss: 3.65 | perplexity: 38.46 | lr: 4.20e-04 | norm: 0.4205 | dt: 18000.1974ms | tok/sec: 21.8451\n",
      "step 1716 | train loss: 3.62 | val loss: 3.65 | perplexity: 38.50 | lr: 4.19e-04 | norm: 0.3397 | dt: 17947.6445ms | tok/sec: 21.9091\n",
      "step 1717 | train loss: 3.71 | val loss: 3.65 | perplexity: 38.53 | lr: 4.18e-04 | norm: 0.3402 | dt: 18013.8776ms | tok/sec: 21.8285\n",
      "step 1718 | train loss: 3.80 | val loss: 3.65 | perplexity: 38.47 | lr: 4.17e-04 | norm: 0.3230 | dt: 17971.1237ms | tok/sec: 21.8804\n",
      "step 1719 | train loss: 3.75 | val loss: 3.65 | perplexity: 38.38 | lr: 4.17e-04 | norm: 0.3503 | dt: 17876.7917ms | tok/sec: 21.9959\n",
      "step 1720 | train loss: 3.77 | val loss: 3.65 | perplexity: 38.31 | lr: 4.16e-04 | norm: 0.3138 | dt: 17992.2831ms | tok/sec: 21.8547\n",
      "step 1721 | train loss: 3.59 | val loss: 3.65 | perplexity: 38.35 | lr: 4.15e-04 | norm: 0.3225 | dt: 18022.3429ms | tok/sec: 21.8183\n",
      "step 1722 | train loss: 3.59 | val loss: 3.65 | perplexity: 38.34 | lr: 4.14e-04 | norm: 0.3214 | dt: 18118.6380ms | tok/sec: 21.7023\n",
      "step 1723 | train loss: 4.00 | val loss: 3.65 | perplexity: 38.31 | lr: 4.13e-04 | norm: 0.3339 | dt: 17964.0756ms | tok/sec: 21.8890\n",
      "step 1724 | train loss: 3.70 | val loss: 3.65 | perplexity: 38.40 | lr: 4.13e-04 | norm: 0.3517 | dt: 17971.9284ms | tok/sec: 21.8795\n",
      "step 1725 | train loss: 3.77 | val loss: 3.65 | perplexity: 38.41 | lr: 4.12e-04 | norm: 0.3847 | dt: 17965.3761ms | tok/sec: 21.8874\n",
      "step 1726 | train loss: 3.58 | val loss: 3.65 | perplexity: 38.32 | lr: 4.11e-04 | norm: 0.3312 | dt: 18166.9028ms | tok/sec: 21.6446\n",
      "step 1727 | train loss: 3.71 | val loss: 3.64 | perplexity: 38.21 | lr: 4.10e-04 | norm: 0.3731 | dt: 17959.2314ms | tok/sec: 21.8949\n",
      "step 1728 | train loss: 3.89 | val loss: 3.65 | perplexity: 38.37 | lr: 4.09e-04 | norm: 0.3867 | dt: 17875.5393ms | tok/sec: 21.9974\n",
      "step 1729 | train loss: 3.75 | val loss: 3.65 | perplexity: 38.40 | lr: 4.09e-04 | norm: 0.3989 | dt: 17992.6052ms | tok/sec: 21.8543\n",
      "step 1730 | train loss: 3.82 | val loss: 3.65 | perplexity: 38.30 | lr: 4.08e-04 | norm: 0.3936 | dt: 17929.1346ms | tok/sec: 21.9317\n",
      "step 1731 | train loss: 3.58 | val loss: 3.64 | perplexity: 38.20 | lr: 4.07e-04 | norm: 0.3137 | dt: 17943.1326ms | tok/sec: 21.9146\n",
      "step 1732 | train loss: 3.75 | val loss: 3.64 | perplexity: 38.24 | lr: 4.06e-04 | norm: 0.3627 | dt: 18081.5451ms | tok/sec: 21.7468\n",
      "step 1733 | train loss: 3.71 | val loss: 3.64 | perplexity: 38.23 | lr: 4.05e-04 | norm: 0.3587 | dt: 17897.0308ms | tok/sec: 21.9710\n",
      "step 1734 | train loss: 3.43 | val loss: 3.64 | perplexity: 38.27 | lr: 4.05e-04 | norm: 0.3250 | dt: 17940.4902ms | tok/sec: 21.9178\n",
      "step 1735 | train loss: 3.66 | val loss: 3.65 | perplexity: 38.29 | lr: 4.04e-04 | norm: 0.3793 | dt: 17933.0678ms | tok/sec: 21.9269\n",
      "step 1736 | train loss: 3.87 | val loss: 3.65 | perplexity: 38.40 | lr: 4.03e-04 | norm: 0.2868 | dt: 17950.0463ms | tok/sec: 21.9061\n",
      "step 1737 | train loss: 3.85 | val loss: 3.65 | perplexity: 38.34 | lr: 4.02e-04 | norm: 0.3545 | dt: 17942.0996ms | tok/sec: 21.9158\n",
      "step 1738 | train loss: 3.59 | val loss: 3.65 | perplexity: 38.33 | lr: 4.01e-04 | norm: 0.4312 | dt: 17944.4876ms | tok/sec: 21.9129\n",
      "step 1739 | train loss: 3.79 | val loss: 3.65 | perplexity: 38.42 | lr: 4.01e-04 | norm: 0.3282 | dt: 17922.8420ms | tok/sec: 21.9394\n",
      "step 1740 | train loss: 3.91 | val loss: 3.65 | perplexity: 38.30 | lr: 4.00e-04 | norm: 0.4413 | dt: 17991.1103ms | tok/sec: 21.8561\n",
      "step 1741 | train loss: 4.19 | val loss: 3.65 | perplexity: 38.40 | lr: 3.99e-04 | norm: 0.4107 | dt: 17955.7450ms | tok/sec: 21.8992\n",
      "step 1742 | train loss: 3.87 | val loss: 3.65 | perplexity: 38.52 | lr: 3.98e-04 | norm: 0.3303 | dt: 17937.2449ms | tok/sec: 21.9218\n",
      "step 1743 | train loss: 3.46 | val loss: 3.65 | perplexity: 38.32 | lr: 3.97e-04 | norm: 0.3916 | dt: 18056.3929ms | tok/sec: 21.7771\n",
      "step 1744 | train loss: 3.94 | val loss: 3.65 | perplexity: 38.29 | lr: 3.97e-04 | norm: 0.3854 | dt: 17923.7809ms | tok/sec: 21.9382\n",
      "step 1745 | train loss: 3.87 | val loss: 3.64 | perplexity: 38.17 | lr: 3.96e-04 | norm: 0.4557 | dt: 17935.3826ms | tok/sec: 21.9240\n",
      "step 1746 | train loss: 3.86 | val loss: 3.64 | perplexity: 38.17 | lr: 3.95e-04 | norm: 0.3403 | dt: 17798.8107ms | tok/sec: 22.0923\n",
      "step 1747 | train loss: 3.70 | val loss: 3.64 | perplexity: 38.15 | lr: 3.94e-04 | norm: 0.3599 | dt: 18243.4275ms | tok/sec: 21.5538\n",
      "step 1748 | train loss: 3.84 | val loss: 3.64 | perplexity: 38.17 | lr: 3.94e-04 | norm: 0.3845 | dt: 17921.1917ms | tok/sec: 21.9414\n",
      "step 1749 | train loss: 3.88 | val loss: 3.64 | perplexity: 38.18 | lr: 3.93e-04 | norm: 0.3578 | dt: 17854.7885ms | tok/sec: 22.0230\n",
      "step 1750 | train loss: 3.84 | val loss: 3.64 | perplexity: 38.14 | lr: 3.92e-04 | norm: 0.3993 | dt: 17932.7033ms | tok/sec: 21.9273\n",
      "step 1751 | train loss: 3.79 | val loss: 3.64 | perplexity: 38.04 | lr: 3.91e-04 | norm: 0.3508 | dt: 17953.9073ms | tok/sec: 21.9014\n",
      "step 1752 | train loss: 3.77 | val loss: 3.64 | perplexity: 38.01 | lr: 3.90e-04 | norm: 0.2966 | dt: 17915.3409ms | tok/sec: 21.9486\n",
      "step 1753 | train loss: 3.60 | val loss: 3.64 | perplexity: 38.08 | lr: 3.90e-04 | norm: 0.3303 | dt: 17906.8549ms | tok/sec: 21.9590\n",
      "step 1754 | train loss: 3.70 | val loss: 3.64 | perplexity: 38.15 | lr: 3.89e-04 | norm: 0.3544 | dt: 18042.0511ms | tok/sec: 21.7944\n",
      "step 1755 | train loss: 3.56 | val loss: 3.64 | perplexity: 38.08 | lr: 3.88e-04 | norm: 0.3793 | dt: 17881.9404ms | tok/sec: 21.9896\n",
      "step 1756 | train loss: 3.86 | val loss: 3.64 | perplexity: 37.98 | lr: 3.87e-04 | norm: 0.3881 | dt: 17921.1717ms | tok/sec: 21.9414\n",
      "step 1757 | train loss: 3.76 | val loss: 3.64 | perplexity: 37.96 | lr: 3.86e-04 | norm: 0.3330 | dt: 17896.2421ms | tok/sec: 21.9720\n",
      "step 1758 | train loss: 3.80 | val loss: 3.64 | perplexity: 37.92 | lr: 3.86e-04 | norm: 0.3281 | dt: 17944.0279ms | tok/sec: 21.9135\n",
      "step 1759 | train loss: 3.91 | val loss: 3.64 | perplexity: 38.00 | lr: 3.85e-04 | norm: 0.3293 | dt: 17902.3957ms | tok/sec: 21.9644\n",
      "step 1760 | train loss: 3.85 | val loss: 3.64 | perplexity: 38.16 | lr: 3.84e-04 | norm: 0.3408 | dt: 17951.8368ms | tok/sec: 21.9039\n",
      "step 1761 | train loss: 3.68 | val loss: 3.64 | perplexity: 38.03 | lr: 3.83e-04 | norm: 0.3646 | dt: 17926.0187ms | tok/sec: 21.9355\n",
      "step 1762 | train loss: 3.75 | val loss: 3.63 | perplexity: 37.90 | lr: 3.83e-04 | norm: 0.3292 | dt: 17918.8089ms | tok/sec: 21.9443\n",
      "step 1763 | train loss: 3.59 | val loss: 3.63 | perplexity: 37.80 | lr: 3.82e-04 | norm: 0.3693 | dt: 17871.5930ms | tok/sec: 22.0023\n",
      "step 1764 | train loss: 3.72 | val loss: 3.63 | perplexity: 37.78 | lr: 3.81e-04 | norm: 0.3258 | dt: 17885.1728ms | tok/sec: 21.9856\n",
      "step 1765 | train loss: 3.93 | val loss: 3.63 | perplexity: 37.77 | lr: 3.80e-04 | norm: 0.4086 | dt: 17914.9089ms | tok/sec: 21.9491\n",
      "step 1766 | train loss: 3.81 | val loss: 3.63 | perplexity: 37.85 | lr: 3.79e-04 | norm: 0.4001 | dt: 17917.6772ms | tok/sec: 21.9457\n",
      "step 1767 | train loss: 3.76 | val loss: 3.63 | perplexity: 37.89 | lr: 3.79e-04 | norm: 0.3833 | dt: 18024.7333ms | tok/sec: 21.8154\n",
      "step 1768 | train loss: 3.65 | val loss: 3.63 | perplexity: 37.83 | lr: 3.78e-04 | norm: 0.3793 | dt: 17876.1699ms | tok/sec: 21.9967\n",
      "step 1769 | train loss: 3.94 | val loss: 3.63 | perplexity: 37.85 | lr: 3.77e-04 | norm: 0.3168 | dt: 17929.7025ms | tok/sec: 21.9310\n",
      "step 1770 | train loss: 3.76 | val loss: 3.63 | perplexity: 37.85 | lr: 3.76e-04 | norm: 0.3688 | dt: 17958.7824ms | tok/sec: 21.8955\n",
      "step 1771 | train loss: 3.44 | val loss: 3.63 | perplexity: 37.82 | lr: 3.76e-04 | norm: 0.3753 | dt: 17902.8256ms | tok/sec: 21.9639\n",
      "step 1772 | train loss: 3.72 | val loss: 3.63 | perplexity: 37.79 | lr: 3.75e-04 | norm: 0.3683 | dt: 17898.8907ms | tok/sec: 21.9687\n",
      "step 1773 | train loss: 3.92 | val loss: 3.63 | perplexity: 37.84 | lr: 3.74e-04 | norm: 0.3508 | dt: 17790.6699ms | tok/sec: 22.1024\n",
      "step 1774 | train loss: 3.91 | val loss: 3.63 | perplexity: 37.80 | lr: 3.73e-04 | norm: 0.3294 | dt: 17885.9882ms | tok/sec: 21.9846\n",
      "step 1775 | train loss: 3.88 | val loss: 3.63 | perplexity: 37.70 | lr: 3.72e-04 | norm: 0.3369 | dt: 17851.4621ms | tok/sec: 22.0271\n",
      "step 1776 | train loss: 3.47 | val loss: 3.63 | perplexity: 37.68 | lr: 3.72e-04 | norm: 0.3734 | dt: 17874.5732ms | tok/sec: 21.9986\n",
      "step 1777 | train loss: 3.96 | val loss: 3.63 | perplexity: 37.67 | lr: 3.71e-04 | norm: 0.3504 | dt: 17902.3445ms | tok/sec: 21.9645\n",
      "step 1778 | train loss: 3.83 | val loss: 3.63 | perplexity: 37.73 | lr: 3.70e-04 | norm: 0.3186 | dt: 17907.5480ms | tok/sec: 21.9581\n",
      "step 1779 | train loss: 3.54 | val loss: 3.63 | perplexity: 37.57 | lr: 3.69e-04 | norm: 0.3690 | dt: 17833.2717ms | tok/sec: 22.0496\n",
      "step 1780 | train loss: 3.86 | val loss: 3.63 | perplexity: 37.56 | lr: 3.69e-04 | norm: 0.3229 | dt: 17919.5545ms | tok/sec: 21.9434\n",
      "step 1781 | train loss: 3.46 | val loss: 3.62 | perplexity: 37.51 | lr: 3.68e-04 | norm: 0.3517 | dt: 17880.2195ms | tok/sec: 21.9917\n",
      "step 1782 | train loss: 3.64 | val loss: 3.62 | perplexity: 37.45 | lr: 3.67e-04 | norm: 0.3240 | dt: 18047.2038ms | tok/sec: 21.7882\n",
      "step 1783 | train loss: 4.05 | val loss: 3.62 | perplexity: 37.52 | lr: 3.66e-04 | norm: 0.3359 | dt: 17854.4624ms | tok/sec: 22.0234\n",
      "step 1784 | train loss: 3.79 | val loss: 3.62 | perplexity: 37.49 | lr: 3.66e-04 | norm: 0.3233 | dt: 17876.3030ms | tok/sec: 21.9965\n",
      "step 1785 | train loss: 3.68 | val loss: 3.62 | perplexity: 37.45 | lr: 3.65e-04 | norm: 0.3590 | dt: 18079.6766ms | tok/sec: 21.7491\n",
      "step 1786 | train loss: 3.84 | val loss: 3.62 | perplexity: 37.42 | lr: 3.64e-04 | norm: 0.3272 | dt: 17930.7847ms | tok/sec: 21.9297\n",
      "step 1787 | train loss: 3.81 | val loss: 3.62 | perplexity: 37.40 | lr: 3.63e-04 | norm: 0.3003 | dt: 17920.3327ms | tok/sec: 21.9424\n",
      "step 1788 | train loss: 3.52 | val loss: 3.62 | perplexity: 37.35 | lr: 3.63e-04 | norm: 0.3045 | dt: 17878.7448ms | tok/sec: 21.9935\n",
      "step 1789 | train loss: 4.05 | val loss: 3.62 | perplexity: 37.45 | lr: 3.62e-04 | norm: 0.4543 | dt: 17867.6612ms | tok/sec: 22.0071\n",
      "step 1790 | train loss: 3.91 | val loss: 3.63 | perplexity: 37.55 | lr: 3.61e-04 | norm: 0.2915 | dt: 17873.8499ms | tok/sec: 21.9995\n",
      "step 1791 | train loss: 3.70 | val loss: 3.63 | perplexity: 37.60 | lr: 3.60e-04 | norm: 0.3161 | dt: 17858.2799ms | tok/sec: 22.0187\n",
      "step 1792 | train loss: 3.74 | val loss: 3.62 | perplexity: 37.52 | lr: 3.59e-04 | norm: 0.3835 | dt: 17798.9628ms | tok/sec: 22.0921\n",
      "step 1793 | train loss: 3.70 | val loss: 3.62 | perplexity: 37.50 | lr: 3.59e-04 | norm: 0.3582 | dt: 17985.5795ms | tok/sec: 21.8628\n",
      "step 1794 | train loss: 3.79 | val loss: 3.62 | perplexity: 37.51 | lr: 3.58e-04 | norm: 0.3612 | dt: 17932.8709ms | tok/sec: 21.9271\n",
      "step 1795 | train loss: 3.80 | val loss: 3.62 | perplexity: 37.51 | lr: 3.57e-04 | norm: 0.3648 | dt: 17959.3871ms | tok/sec: 21.8947\n",
      "step 1796 | train loss: 3.73 | val loss: 3.62 | perplexity: 37.47 | lr: 3.56e-04 | norm: 0.3305 | dt: 17869.9844ms | tok/sec: 22.0043\n",
      "step 1797 | train loss: 3.55 | val loss: 3.62 | perplexity: 37.36 | lr: 3.56e-04 | norm: 0.3394 | dt: 18059.6256ms | tok/sec: 21.7732\n",
      "step 1798 | train loss: 3.42 | val loss: 3.62 | perplexity: 37.41 | lr: 3.55e-04 | norm: 0.4070 | dt: 17860.2614ms | tok/sec: 22.0163\n",
      "step 1799 | train loss: 3.84 | val loss: 3.62 | perplexity: 37.52 | lr: 3.54e-04 | norm: 0.3428 | dt: 18105.3693ms | tok/sec: 21.7182\n",
      "step 1800 | train loss: 3.68 | val loss: 3.62 | perplexity: 37.48 | lr: 3.53e-04 | norm: 0.3651 | dt: 17925.0987ms | tok/sec: 21.9366\n",
      "step 1801 | train loss: 3.86 | val loss: 3.62 | perplexity: 37.39 | lr: 3.53e-04 | norm: 0.4455 | dt: 17969.2326ms | tok/sec: 21.8827\n",
      "step 1802 | train loss: 3.56 | val loss: 3.62 | perplexity: 37.50 | lr: 3.52e-04 | norm: 0.3126 | dt: 17861.4268ms | tok/sec: 22.0148\n",
      "step 1803 | train loss: 3.47 | val loss: 3.62 | perplexity: 37.46 | lr: 3.51e-04 | norm: 0.3877 | dt: 17913.9121ms | tok/sec: 21.9503\n",
      "step 1804 | train loss: 3.71 | val loss: 3.62 | perplexity: 37.27 | lr: 3.50e-04 | norm: 0.3886 | dt: 17858.7272ms | tok/sec: 22.0181\n",
      "step 1805 | train loss: 3.83 | val loss: 3.62 | perplexity: 37.30 | lr: 3.50e-04 | norm: 0.4022 | dt: 17850.0795ms | tok/sec: 22.0288\n",
      "step 1806 | train loss: 3.63 | val loss: 3.62 | perplexity: 37.40 | lr: 3.49e-04 | norm: 0.3942 | dt: 17841.8508ms | tok/sec: 22.0390\n",
      "step 1807 | train loss: 3.46 | val loss: 3.62 | perplexity: 37.41 | lr: 3.48e-04 | norm: 0.3765 | dt: 17813.3991ms | tok/sec: 22.0742\n",
      "step 1808 | train loss: 3.57 | val loss: 3.62 | perplexity: 37.39 | lr: 3.47e-04 | norm: 0.3297 | dt: 17858.6059ms | tok/sec: 22.0183\n",
      "step 1809 | train loss: 3.85 | val loss: 3.62 | perplexity: 37.30 | lr: 3.47e-04 | norm: 0.3951 | dt: 17872.7887ms | tok/sec: 22.0008\n",
      "step 1810 | train loss: 3.50 | val loss: 3.62 | perplexity: 37.31 | lr: 3.46e-04 | norm: 0.4015 | dt: 17947.5515ms | tok/sec: 21.9092\n",
      "step 1811 | train loss: 3.62 | val loss: 3.62 | perplexity: 37.40 | lr: 3.45e-04 | norm: 0.3395 | dt: 17976.2311ms | tok/sec: 21.8742\n",
      "step 1812 | train loss: 3.68 | val loss: 3.62 | perplexity: 37.38 | lr: 3.44e-04 | norm: 0.4042 | dt: 17918.5090ms | tok/sec: 21.9447\n",
      "step 1813 | train loss: 3.59 | val loss: 3.62 | perplexity: 37.29 | lr: 3.44e-04 | norm: 0.3463 | dt: 18136.7955ms | tok/sec: 21.6806\n",
      "step 1814 | train loss: 3.66 | val loss: 3.62 | perplexity: 37.25 | lr: 3.43e-04 | norm: 0.2800 | dt: 17886.4048ms | tok/sec: 21.9841\n",
      "step 1815 | train loss: 4.01 | val loss: 3.62 | perplexity: 37.22 | lr: 3.42e-04 | norm: 0.3606 | dt: 17946.7506ms | tok/sec: 21.9102\n",
      "step 1816 | train loss: 3.92 | val loss: 3.61 | perplexity: 37.14 | lr: 3.41e-04 | norm: 0.2908 | dt: 17988.8730ms | tok/sec: 21.8588\n",
      "step 1817 | train loss: 3.45 | val loss: 3.61 | perplexity: 37.09 | lr: 3.41e-04 | norm: 0.3495 | dt: 17906.2467ms | tok/sec: 21.9597\n",
      "step 1818 | train loss: 3.69 | val loss: 3.61 | perplexity: 37.03 | lr: 3.40e-04 | norm: 0.3220 | dt: 18217.6299ms | tok/sec: 21.5844\n",
      "step 1819 | train loss: 3.86 | val loss: 3.61 | perplexity: 36.97 | lr: 3.39e-04 | norm: 0.4193 | dt: 17832.8674ms | tok/sec: 22.0501\n",
      "step 1820 | train loss: 3.52 | val loss: 3.61 | perplexity: 37.04 | lr: 3.38e-04 | norm: 0.3148 | dt: 17895.7858ms | tok/sec: 21.9725\n",
      "step 1821 | train loss: 3.50 | val loss: 3.61 | perplexity: 37.05 | lr: 3.38e-04 | norm: 0.2940 | dt: 17939.3842ms | tok/sec: 21.9191\n",
      "step 1822 | train loss: 3.90 | val loss: 3.61 | perplexity: 37.12 | lr: 3.37e-04 | norm: 0.4186 | dt: 17939.0857ms | tok/sec: 21.9195\n",
      "step 1823 | train loss: 3.55 | val loss: 3.61 | perplexity: 37.13 | lr: 3.36e-04 | norm: 0.3669 | dt: 18025.4967ms | tok/sec: 21.8144\n",
      "step 1824 | train loss: 3.76 | val loss: 3.61 | perplexity: 36.96 | lr: 3.35e-04 | norm: 0.3789 | dt: 17935.9353ms | tok/sec: 21.9234\n",
      "step 1825 | train loss: 3.88 | val loss: 3.61 | perplexity: 36.94 | lr: 3.35e-04 | norm: 0.3342 | dt: 17885.1049ms | tok/sec: 21.9857\n",
      "step 1826 | train loss: 3.58 | val loss: 3.61 | perplexity: 36.93 | lr: 3.34e-04 | norm: 0.3451 | dt: 17948.5524ms | tok/sec: 21.9080\n",
      "step 1827 | train loss: 3.42 | val loss: 3.61 | perplexity: 36.88 | lr: 3.33e-04 | norm: 0.3484 | dt: 17989.6271ms | tok/sec: 21.8579\n",
      "step 1828 | train loss: 3.80 | val loss: 3.61 | perplexity: 36.90 | lr: 3.33e-04 | norm: 0.2770 | dt: 17917.8071ms | tok/sec: 21.9455\n",
      "step 1829 | train loss: 3.53 | val loss: 3.61 | perplexity: 36.98 | lr: 3.32e-04 | norm: 0.3093 | dt: 17996.3963ms | tok/sec: 21.8497\n",
      "step 1830 | train loss: 3.62 | val loss: 3.61 | perplexity: 37.02 | lr: 3.31e-04 | norm: 0.3174 | dt: 17892.7073ms | tok/sec: 21.9763\n",
      "step 1831 | train loss: 3.75 | val loss: 3.61 | perplexity: 36.97 | lr: 3.30e-04 | norm: 0.3442 | dt: 17827.0147ms | tok/sec: 22.0573\n",
      "step 1832 | train loss: 3.83 | val loss: 3.61 | perplexity: 36.96 | lr: 3.30e-04 | norm: 0.3408 | dt: 17907.4099ms | tok/sec: 21.9583\n",
      "step 1833 | train loss: 3.68 | val loss: 3.61 | perplexity: 36.99 | lr: 3.29e-04 | norm: 0.2967 | dt: 17943.6009ms | tok/sec: 21.9140\n",
      "step 1834 | train loss: 3.68 | val loss: 3.61 | perplexity: 36.97 | lr: 3.28e-04 | norm: 0.3089 | dt: 17908.7250ms | tok/sec: 21.9567\n",
      "step 1835 | train loss: 3.51 | val loss: 3.61 | perplexity: 36.94 | lr: 3.27e-04 | norm: 0.3191 | dt: 18033.4666ms | tok/sec: 21.8048\n",
      "step 1836 | train loss: 3.59 | val loss: 3.61 | perplexity: 36.94 | lr: 3.27e-04 | norm: 0.3638 | dt: 18008.8351ms | tok/sec: 21.8346\n",
      "step 1837 | train loss: 3.72 | val loss: 3.61 | perplexity: 36.92 | lr: 3.26e-04 | norm: 0.3345 | dt: 18001.9863ms | tok/sec: 21.8429\n",
      "step 1838 | train loss: 3.61 | val loss: 3.61 | perplexity: 36.97 | lr: 3.25e-04 | norm: 0.4201 | dt: 17978.6227ms | tok/sec: 21.8713\n",
      "step 1839 | train loss: 3.67 | val loss: 3.61 | perplexity: 36.96 | lr: 3.25e-04 | norm: 0.3023 | dt: 17978.0877ms | tok/sec: 21.8720\n",
      "step 1840 | train loss: 3.60 | val loss: 3.61 | perplexity: 36.85 | lr: 3.24e-04 | norm: 0.3352 | dt: 17850.9479ms | tok/sec: 22.0277\n",
      "step 1841 | train loss: 3.92 | val loss: 3.61 | perplexity: 36.81 | lr: 3.23e-04 | norm: 0.3373 | dt: 17951.4420ms | tok/sec: 21.9044\n",
      "step 1842 | train loss: 3.75 | val loss: 3.61 | perplexity: 36.86 | lr: 3.22e-04 | norm: 0.2999 | dt: 17950.6221ms | tok/sec: 21.9054\n",
      "step 1843 | train loss: 3.69 | val loss: 3.61 | perplexity: 36.84 | lr: 3.22e-04 | norm: 0.3706 | dt: 18021.2369ms | tok/sec: 21.8196\n",
      "step 1844 | train loss: 3.78 | val loss: 3.60 | perplexity: 36.73 | lr: 3.21e-04 | norm: 0.3184 | dt: 17937.0697ms | tok/sec: 21.9220\n",
      "step 1845 | train loss: 3.78 | val loss: 3.60 | perplexity: 36.69 | lr: 3.20e-04 | norm: 0.3169 | dt: 17953.6443ms | tok/sec: 21.9017\n",
      "step 1846 | train loss: 3.67 | val loss: 3.60 | perplexity: 36.68 | lr: 3.19e-04 | norm: 0.3103 | dt: 17931.9787ms | tok/sec: 21.9282\n",
      "step 1847 | train loss: 3.96 | val loss: 3.60 | perplexity: 36.65 | lr: 3.19e-04 | norm: 0.4126 | dt: 18070.2076ms | tok/sec: 21.7605\n",
      "step 1848 | train loss: 3.63 | val loss: 3.60 | perplexity: 36.66 | lr: 3.18e-04 | norm: 0.3110 | dt: 18073.4689ms | tok/sec: 21.7565\n",
      "step 1849 | train loss: 3.57 | val loss: 3.60 | perplexity: 36.61 | lr: 3.17e-04 | norm: 0.3244 | dt: 17849.3817ms | tok/sec: 22.0297\n",
      "step 1850 | train loss: 3.49 | val loss: 3.60 | perplexity: 36.58 | lr: 3.17e-04 | norm: 0.3413 | dt: 17908.9689ms | tok/sec: 21.9564\n",
      "step 1851 | train loss: 3.65 | val loss: 3.60 | perplexity: 36.59 | lr: 3.16e-04 | norm: 0.3810 | dt: 17965.1446ms | tok/sec: 21.8877\n",
      "step 1852 | train loss: 3.44 | val loss: 3.60 | perplexity: 36.63 | lr: 3.15e-04 | norm: 0.2928 | dt: 17964.3054ms | tok/sec: 21.8887\n",
      "step 1853 | train loss: 3.76 | val loss: 3.60 | perplexity: 36.69 | lr: 3.14e-04 | norm: 0.3524 | dt: 18181.4499ms | tok/sec: 21.6273\n",
      "step 1854 | train loss: 3.48 | val loss: 3.60 | perplexity: 36.63 | lr: 3.14e-04 | norm: 0.3346 | dt: 18046.7770ms | tok/sec: 21.7887\n",
      "step 1855 | train loss: 3.70 | val loss: 3.60 | perplexity: 36.58 | lr: 3.13e-04 | norm: 0.3392 | dt: 17885.5767ms | tok/sec: 21.9851\n",
      "step 1856 | train loss: 3.88 | val loss: 3.60 | perplexity: 36.56 | lr: 3.12e-04 | norm: 0.3698 | dt: 17976.7416ms | tok/sec: 21.8736\n",
      "step 1857 | train loss: 3.43 | val loss: 3.60 | perplexity: 36.58 | lr: 3.12e-04 | norm: 0.3236 | dt: 18115.4261ms | tok/sec: 21.7061\n",
      "step 1858 | train loss: 3.65 | val loss: 3.60 | perplexity: 36.47 | lr: 3.11e-04 | norm: 0.3377 | dt: 17894.3455ms | tok/sec: 21.9743\n",
      "step 1859 | train loss: 3.89 | val loss: 3.59 | perplexity: 36.39 | lr: 3.10e-04 | norm: 0.3482 | dt: 17903.5416ms | tok/sec: 21.9630\n",
      "step 1860 | train loss: 3.91 | val loss: 3.59 | perplexity: 36.40 | lr: 3.09e-04 | norm: 0.2919 | dt: 17951.1280ms | tok/sec: 21.9048\n",
      "step 1861 | train loss: 4.05 | val loss: 3.60 | perplexity: 36.44 | lr: 3.09e-04 | norm: 0.4431 | dt: 17906.5604ms | tok/sec: 21.9593\n",
      "step 1862 | train loss: 4.27 | val loss: 3.60 | perplexity: 36.58 | lr: 3.08e-04 | norm: 0.8637 | dt: 17911.7913ms | tok/sec: 21.9529\n",
      "step 1863 | train loss: 3.91 | val loss: 3.60 | perplexity: 36.59 | lr: 3.07e-04 | norm: 0.3473 | dt: 17903.2829ms | tok/sec: 21.9633\n",
      "step 1864 | train loss: 3.57 | val loss: 3.60 | perplexity: 36.61 | lr: 3.07e-04 | norm: 0.3511 | dt: 18109.9217ms | tok/sec: 21.7127\n",
      "step 1865 | train loss: 3.77 | val loss: 3.60 | perplexity: 36.66 | lr: 3.06e-04 | norm: 0.3620 | dt: 17942.4725ms | tok/sec: 21.9154\n",
      "step 1866 | train loss: 3.61 | val loss: 3.60 | perplexity: 36.50 | lr: 3.05e-04 | norm: 0.3775 | dt: 17871.5670ms | tok/sec: 22.0023\n",
      "step 1867 | train loss: 3.61 | val loss: 3.59 | perplexity: 36.41 | lr: 3.05e-04 | norm: 0.3736 | dt: 17861.0096ms | tok/sec: 22.0153\n",
      "step 1868 | train loss: 3.39 | val loss: 3.60 | perplexity: 36.49 | lr: 3.04e-04 | norm: 0.3234 | dt: 17852.9253ms | tok/sec: 22.0253\n",
      "step 1869 | train loss: 4.04 | val loss: 3.60 | perplexity: 36.50 | lr: 3.03e-04 | norm: 0.5809 | dt: 17808.6085ms | tok/sec: 22.0801\n",
      "step 1870 | train loss: 3.64 | val loss: 3.60 | perplexity: 36.49 | lr: 3.02e-04 | norm: 0.3637 | dt: 17854.8133ms | tok/sec: 22.0230\n",
      "step 1871 | train loss: 3.74 | val loss: 3.60 | perplexity: 36.47 | lr: 3.02e-04 | norm: 0.3429 | dt: 17950.5630ms | tok/sec: 21.9055\n",
      "step 1872 | train loss: 3.71 | val loss: 3.60 | perplexity: 36.42 | lr: 3.01e-04 | norm: 0.3422 | dt: 18079.8402ms | tok/sec: 21.7489\n",
      "step 1873 | train loss: 3.68 | val loss: 3.59 | perplexity: 36.37 | lr: 3.00e-04 | norm: 0.3633 | dt: 17904.0351ms | tok/sec: 21.9624\n",
      "step 1874 | train loss: 3.51 | val loss: 3.59 | perplexity: 36.36 | lr: 3.00e-04 | norm: 0.3517 | dt: 17849.7951ms | tok/sec: 22.0292\n",
      "step 1875 | train loss: 3.65 | val loss: 3.59 | perplexity: 36.39 | lr: 2.99e-04 | norm: 0.3033 | dt: 17918.6547ms | tok/sec: 21.9445\n",
      "step 1876 | train loss: 3.62 | val loss: 3.59 | perplexity: 36.36 | lr: 2.98e-04 | norm: 0.3945 | dt: 17913.2679ms | tok/sec: 21.9511\n",
      "step 1877 | train loss: 3.76 | val loss: 3.60 | perplexity: 36.44 | lr: 2.98e-04 | norm: 0.4100 | dt: 17919.2302ms | tok/sec: 21.9438\n",
      "step 1878 | train loss: 3.90 | val loss: 3.60 | perplexity: 36.46 | lr: 2.97e-04 | norm: 0.3880 | dt: 18204.5774ms | tok/sec: 21.5998\n",
      "step 1879 | train loss: 3.42 | val loss: 3.60 | perplexity: 36.42 | lr: 2.96e-04 | norm: 0.3359 | dt: 17840.0683ms | tok/sec: 22.0412\n",
      "step 1880 | train loss: 3.73 | val loss: 3.60 | perplexity: 36.43 | lr: 2.95e-04 | norm: 0.3890 | dt: 17888.9380ms | tok/sec: 21.9810\n",
      "step 1881 | train loss: 3.65 | val loss: 3.60 | perplexity: 36.43 | lr: 2.95e-04 | norm: 0.3084 | dt: 17835.3910ms | tok/sec: 22.0470\n",
      "step 1882 | train loss: 4.09 | val loss: 3.59 | perplexity: 36.41 | lr: 2.94e-04 | norm: 0.3439 | dt: 17881.0284ms | tok/sec: 21.9907\n",
      "step 1883 | train loss: 3.59 | val loss: 3.59 | perplexity: 36.36 | lr: 2.93e-04 | norm: 0.3654 | dt: 17926.6303ms | tok/sec: 21.9347\n",
      "step 1884 | train loss: 3.72 | val loss: 3.59 | perplexity: 36.35 | lr: 2.93e-04 | norm: 0.3532 | dt: 17978.4262ms | tok/sec: 21.8715\n",
      "step 1885 | train loss: 3.70 | val loss: 3.59 | perplexity: 36.29 | lr: 2.92e-04 | norm: 0.3322 | dt: 18195.2624ms | tok/sec: 21.6109\n",
      "step 1886 | train loss: 3.55 | val loss: 3.59 | perplexity: 36.20 | lr: 2.91e-04 | norm: 0.3168 | dt: 17899.9498ms | tok/sec: 21.9674\n",
      "step 1887 | train loss: 3.49 | val loss: 3.59 | perplexity: 36.18 | lr: 2.91e-04 | norm: 0.3003 | dt: 17942.3463ms | tok/sec: 21.9155\n",
      "step 1888 | train loss: 3.73 | val loss: 3.59 | perplexity: 36.22 | lr: 2.90e-04 | norm: 0.3346 | dt: 18037.1904ms | tok/sec: 21.8003\n",
      "step 1889 | train loss: 3.53 | val loss: 3.59 | perplexity: 36.21 | lr: 2.89e-04 | norm: 0.3067 | dt: 18119.0295ms | tok/sec: 21.7018\n",
      "step 1890 | train loss: 3.51 | val loss: 3.59 | perplexity: 36.16 | lr: 2.89e-04 | norm: 0.3241 | dt: 18129.1261ms | tok/sec: 21.6897\n",
      "step 1891 | train loss: 3.75 | val loss: 3.59 | perplexity: 36.16 | lr: 2.88e-04 | norm: 0.3225 | dt: 17986.4855ms | tok/sec: 21.8617\n",
      "step 1892 | train loss: 3.73 | val loss: 3.59 | perplexity: 36.17 | lr: 2.87e-04 | norm: 0.3184 | dt: 17864.9681ms | tok/sec: 22.0105\n",
      "step 1893 | train loss: 3.66 | val loss: 3.59 | perplexity: 36.11 | lr: 2.87e-04 | norm: 0.3190 | dt: 17922.7288ms | tok/sec: 21.9395\n",
      "step 1894 | train loss: 3.88 | val loss: 3.59 | perplexity: 36.06 | lr: 2.86e-04 | norm: 0.3061 | dt: 17877.5992ms | tok/sec: 21.9949\n",
      "step 1895 | train loss: 3.74 | val loss: 3.58 | perplexity: 36.05 | lr: 2.85e-04 | norm: 0.2686 | dt: 17899.5595ms | tok/sec: 21.9679\n",
      "step 1896 | train loss: 3.89 | val loss: 3.58 | perplexity: 36.00 | lr: 2.84e-04 | norm: 0.3112 | dt: 17911.8624ms | tok/sec: 21.9528\n",
      "step 1897 | train loss: 3.80 | val loss: 3.58 | perplexity: 35.96 | lr: 2.84e-04 | norm: 0.3239 | dt: 17889.7645ms | tok/sec: 21.9799\n",
      "step 1898 | train loss: 3.73 | val loss: 3.58 | perplexity: 35.97 | lr: 2.83e-04 | norm: 0.2470 | dt: 17860.7664ms | tok/sec: 22.0156\n",
      "step 1899 | train loss: 3.66 | val loss: 3.58 | perplexity: 35.95 | lr: 2.82e-04 | norm: 0.3797 | dt: 17977.4227ms | tok/sec: 21.8728\n",
      "step 1900 | train loss: 3.64 | val loss: 3.58 | perplexity: 35.94 | lr: 2.82e-04 | norm: 0.2907 | dt: 17991.9357ms | tok/sec: 21.8551\n",
      "step 1901 | train loss: 3.35 | val loss: 3.58 | perplexity: 35.90 | lr: 2.81e-04 | norm: 0.2710 | dt: 17934.7579ms | tok/sec: 21.9248\n",
      "step 1902 | train loss: 3.89 | val loss: 3.58 | perplexity: 35.89 | lr: 2.80e-04 | norm: 0.3239 | dt: 17937.5095ms | tok/sec: 21.9214\n",
      "step 1903 | train loss: 3.92 | val loss: 3.58 | perplexity: 35.94 | lr: 2.80e-04 | norm: 0.2973 | dt: 17923.8746ms | tok/sec: 21.9381\n",
      "step 1904 | train loss: 3.55 | val loss: 3.58 | perplexity: 35.94 | lr: 2.79e-04 | norm: 0.2865 | dt: 17870.7612ms | tok/sec: 22.0033\n",
      "step 1905 | train loss: 3.65 | val loss: 3.58 | perplexity: 35.97 | lr: 2.78e-04 | norm: 0.3154 | dt: 17923.5456ms | tok/sec: 21.9385\n",
      "step 1906 | train loss: 3.80 | val loss: 3.58 | perplexity: 35.99 | lr: 2.78e-04 | norm: 0.4152 | dt: 18135.5822ms | tok/sec: 21.6820\n",
      "step 1907 | train loss: 3.80 | val loss: 3.59 | perplexity: 36.06 | lr: 2.77e-04 | norm: 0.3410 | dt: 17940.6841ms | tok/sec: 21.9176\n",
      "step 1908 | train loss: 3.67 | val loss: 3.58 | perplexity: 36.03 | lr: 2.76e-04 | norm: 0.3393 | dt: 17870.2586ms | tok/sec: 22.0039\n",
      "step 1909 | train loss: 3.71 | val loss: 3.58 | perplexity: 35.97 | lr: 2.76e-04 | norm: 0.3554 | dt: 18590.8937ms | tok/sec: 21.1510\n",
      "step 1910 | train loss: 3.92 | val loss: 3.58 | perplexity: 35.95 | lr: 2.75e-04 | norm: 0.3956 | dt: 22308.1663ms | tok/sec: 17.6265\n",
      "step 1911 | train loss: 3.62 | val loss: 3.58 | perplexity: 35.93 | lr: 2.74e-04 | norm: 0.3052 | dt: 20926.7654ms | tok/sec: 18.7901\n",
      "step 1912 | train loss: 3.55 | val loss: 3.58 | perplexity: 35.90 | lr: 2.74e-04 | norm: 0.3716 | dt: 20831.9857ms | tok/sec: 18.8756\n",
      "step 1913 | train loss: 3.40 | val loss: 3.58 | perplexity: 36.00 | lr: 2.73e-04 | norm: 0.2698 | dt: 20918.3559ms | tok/sec: 18.7977\n",
      "step 1914 | train loss: 3.40 | val loss: 3.58 | perplexity: 36.04 | lr: 2.72e-04 | norm: 0.3767 | dt: 21108.0108ms | tok/sec: 18.6288\n",
      "step 1915 | train loss: 3.72 | val loss: 3.58 | perplexity: 35.96 | lr: 2.72e-04 | norm: 0.3447 | dt: 21052.5858ms | tok/sec: 18.6778\n",
      "step 1916 | train loss: 3.45 | val loss: 3.58 | perplexity: 35.93 | lr: 2.71e-04 | norm: 0.2905 | dt: 20788.0733ms | tok/sec: 18.9155\n",
      "step 1917 | train loss: 3.71 | val loss: 3.58 | perplexity: 35.91 | lr: 2.70e-04 | norm: 0.3019 | dt: 21148.1872ms | tok/sec: 18.5934\n",
      "step 1918 | train loss: 3.79 | val loss: 3.58 | perplexity: 35.85 | lr: 2.70e-04 | norm: 0.3218 | dt: 21063.4832ms | tok/sec: 18.6681\n",
      "step 1919 | train loss: 3.54 | val loss: 3.58 | perplexity: 35.75 | lr: 2.69e-04 | norm: 0.3413 | dt: 20779.0284ms | tok/sec: 18.9237\n",
      "step 1920 | train loss: 3.92 | val loss: 3.58 | perplexity: 35.72 | lr: 2.68e-04 | norm: 0.3600 | dt: 20952.9700ms | tok/sec: 18.7666\n",
      "step 1921 | train loss: 3.71 | val loss: 3.58 | perplexity: 35.77 | lr: 2.68e-04 | norm: 0.3442 | dt: 20973.7086ms | tok/sec: 18.7480\n",
      "step 1922 | train loss: 3.64 | val loss: 3.58 | perplexity: 35.85 | lr: 2.67e-04 | norm: 0.4035 | dt: 21159.0037ms | tok/sec: 18.5839\n",
      "step 1923 | train loss: 3.60 | val loss: 3.58 | perplexity: 35.85 | lr: 2.67e-04 | norm: 0.2935 | dt: 20883.6899ms | tok/sec: 18.8289\n",
      "step 1924 | train loss: 3.77 | val loss: 3.58 | perplexity: 35.87 | lr: 2.66e-04 | norm: 0.3012 | dt: 21081.8281ms | tok/sec: 18.6519\n",
      "step 1925 | train loss: 3.77 | val loss: 3.58 | perplexity: 35.89 | lr: 2.65e-04 | norm: 0.3238 | dt: 21021.9061ms | tok/sec: 18.7051\n",
      "step 1926 | train loss: 3.60 | val loss: 3.58 | perplexity: 35.82 | lr: 2.65e-04 | norm: 0.3434 | dt: 20912.6079ms | tok/sec: 18.8028\n",
      "step 1927 | train loss: 3.84 | val loss: 3.58 | perplexity: 35.80 | lr: 2.64e-04 | norm: 0.3692 | dt: 21134.3048ms | tok/sec: 18.6056\n",
      "step 1928 | train loss: 3.72 | val loss: 3.58 | perplexity: 35.80 | lr: 2.63e-04 | norm: 0.2873 | dt: 20862.8874ms | tok/sec: 18.8476\n",
      "step 1929 | train loss: 3.96 | val loss: 3.58 | perplexity: 35.81 | lr: 2.63e-04 | norm: 0.3538 | dt: 20884.6335ms | tok/sec: 18.8280\n",
      "step 1930 | train loss: 3.64 | val loss: 3.58 | perplexity: 35.77 | lr: 2.62e-04 | norm: 0.3490 | dt: 20941.2920ms | tok/sec: 18.7771\n",
      "step 1931 | train loss: 3.85 | val loss: 3.58 | perplexity: 35.72 | lr: 2.61e-04 | norm: 0.3251 | dt: 21183.8732ms | tok/sec: 18.5620\n",
      "step 1932 | train loss: 3.54 | val loss: 3.57 | perplexity: 35.66 | lr: 2.61e-04 | norm: 0.2892 | dt: 20963.3210ms | tok/sec: 18.7573\n",
      "step 1933 | train loss: 3.62 | val loss: 3.57 | perplexity: 35.66 | lr: 2.60e-04 | norm: 0.3209 | dt: 21220.3043ms | tok/sec: 18.5302\n",
      "step 1934 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.69 | lr: 2.59e-04 | norm: 0.2887 | dt: 21005.1460ms | tok/sec: 18.7200\n",
      "step 1935 | train loss: 3.66 | val loss: 3.57 | perplexity: 35.66 | lr: 2.59e-04 | norm: 0.3138 | dt: 20910.8000ms | tok/sec: 18.8044\n",
      "step 1936 | train loss: 3.93 | val loss: 3.57 | perplexity: 35.63 | lr: 2.58e-04 | norm: 0.3147 | dt: 21008.7733ms | tok/sec: 18.7168\n",
      "step 1937 | train loss: 3.80 | val loss: 3.57 | perplexity: 35.61 | lr: 2.58e-04 | norm: 0.2834 | dt: 21101.3789ms | tok/sec: 18.6346\n",
      "step 1938 | train loss: 3.86 | val loss: 3.57 | perplexity: 35.61 | lr: 2.57e-04 | norm: 0.3627 | dt: 20866.4320ms | tok/sec: 18.8444\n",
      "step 1939 | train loss: 3.74 | val loss: 3.57 | perplexity: 35.61 | lr: 2.56e-04 | norm: 0.3030 | dt: 21129.6461ms | tok/sec: 18.6097\n",
      "step 1940 | train loss: 4.13 | val loss: 3.57 | perplexity: 35.64 | lr: 2.56e-04 | norm: 0.4032 | dt: 20981.4196ms | tok/sec: 18.7412\n",
      "step 1941 | train loss: 3.53 | val loss: 3.57 | perplexity: 35.63 | lr: 2.55e-04 | norm: 0.3586 | dt: 20930.9256ms | tok/sec: 18.7864\n",
      "step 1942 | train loss: 3.87 | val loss: 3.57 | perplexity: 35.65 | lr: 2.54e-04 | norm: 0.3529 | dt: 20868.2995ms | tok/sec: 18.8427\n",
      "step 1943 | train loss: 3.85 | val loss: 3.57 | perplexity: 35.61 | lr: 2.54e-04 | norm: 0.3223 | dt: 21046.9487ms | tok/sec: 18.6828\n",
      "step 1944 | train loss: 3.75 | val loss: 3.57 | perplexity: 35.57 | lr: 2.53e-04 | norm: 0.3121 | dt: 21034.7281ms | tok/sec: 18.6937\n",
      "step 1945 | train loss: 3.63 | val loss: 3.57 | perplexity: 35.58 | lr: 2.52e-04 | norm: 0.4196 | dt: 21101.1717ms | tok/sec: 18.6348\n",
      "step 1946 | train loss: 3.86 | val loss: 3.57 | perplexity: 35.58 | lr: 2.52e-04 | norm: 0.3231 | dt: 21007.8721ms | tok/sec: 18.7176\n",
      "step 1947 | train loss: 3.51 | val loss: 3.57 | perplexity: 35.61 | lr: 2.51e-04 | norm: 0.3054 | dt: 20944.9604ms | tok/sec: 18.7738\n",
      "step 1948 | train loss: 3.66 | val loss: 3.57 | perplexity: 35.59 | lr: 2.51e-04 | norm: 0.3465 | dt: 21123.8737ms | tok/sec: 18.6148\n",
      "step 1949 | train loss: 3.66 | val loss: 3.57 | perplexity: 35.53 | lr: 2.50e-04 | norm: 0.3089 | dt: 20846.3640ms | tok/sec: 18.8626\n",
      "step 1950 | train loss: 3.23 | val loss: 3.57 | perplexity: 35.45 | lr: 2.49e-04 | norm: 0.3512 | dt: 21006.1953ms | tok/sec: 18.7190\n",
      "step 1951 | train loss: 3.55 | val loss: 3.57 | perplexity: 35.46 | lr: 2.49e-04 | norm: 0.3078 | dt: 21011.7550ms | tok/sec: 18.7141\n",
      "step 1952 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.47 | lr: 2.48e-04 | norm: 0.3015 | dt: 21120.9328ms | tok/sec: 18.6174\n",
      "step 1953 | train loss: 3.63 | val loss: 3.57 | perplexity: 35.41 | lr: 2.47e-04 | norm: 0.3295 | dt: 20940.6621ms | tok/sec: 18.7776\n",
      "step 1954 | train loss: 3.56 | val loss: 3.57 | perplexity: 35.41 | lr: 2.47e-04 | norm: 0.2850 | dt: 21036.3894ms | tok/sec: 18.6922\n",
      "step 1955 | train loss: 3.90 | val loss: 3.57 | perplexity: 35.43 | lr: 2.46e-04 | norm: 0.3017 | dt: 20998.9412ms | tok/sec: 18.7255\n",
      "step 1956 | train loss: 3.69 | val loss: 3.57 | perplexity: 35.44 | lr: 2.46e-04 | norm: 0.2892 | dt: 21035.5778ms | tok/sec: 18.6929\n",
      "step 1957 | train loss: 3.64 | val loss: 3.57 | perplexity: 35.50 | lr: 2.45e-04 | norm: 0.3142 | dt: 21082.2902ms | tok/sec: 18.6515\n",
      "step 1958 | train loss: 3.56 | val loss: 3.57 | perplexity: 35.51 | lr: 2.44e-04 | norm: 0.3069 | dt: 21093.2209ms | tok/sec: 18.6418\n",
      "step 1959 | train loss: 3.53 | val loss: 3.57 | perplexity: 35.48 | lr: 2.44e-04 | norm: 0.2961 | dt: 21098.5162ms | tok/sec: 18.6371\n",
      "step 1960 | train loss: 3.68 | val loss: 3.57 | perplexity: 35.39 | lr: 2.43e-04 | norm: 0.3181 | dt: 21122.4086ms | tok/sec: 18.6161\n",
      "step 1961 | train loss: 3.55 | val loss: 3.57 | perplexity: 35.38 | lr: 2.43e-04 | norm: 0.3605 | dt: 23480.9270ms | tok/sec: 16.7462\n",
      "step 1962 | train loss: 3.84 | val loss: 3.57 | perplexity: 35.38 | lr: 2.42e-04 | norm: 0.3154 | dt: 20961.7972ms | tok/sec: 18.7587\n",
      "step 1963 | train loss: 3.73 | val loss: 3.57 | perplexity: 35.35 | lr: 2.41e-04 | norm: 0.3071 | dt: 21045.3928ms | tok/sec: 18.6842\n",
      "step 1964 | train loss: 3.58 | val loss: 3.56 | perplexity: 35.34 | lr: 2.41e-04 | norm: 0.3541 | dt: 21024.2994ms | tok/sec: 18.7029\n",
      "step 1965 | train loss: 3.92 | val loss: 3.57 | perplexity: 35.34 | lr: 2.40e-04 | norm: 0.3365 | dt: 20991.5745ms | tok/sec: 18.7321\n",
      "step 1966 | train loss: 3.56 | val loss: 3.56 | perplexity: 35.32 | lr: 2.39e-04 | norm: 0.3280 | dt: 20857.5952ms | tok/sec: 18.8524\n",
      "step 1967 | train loss: 3.83 | val loss: 3.56 | perplexity: 35.28 | lr: 2.39e-04 | norm: 0.3145 | dt: 21142.1673ms | tok/sec: 18.5987\n",
      "step 1968 | train loss: 3.71 | val loss: 3.56 | perplexity: 35.27 | lr: 2.38e-04 | norm: 0.3501 | dt: 20950.6660ms | tok/sec: 18.7687\n",
      "step 1969 | train loss: 3.66 | val loss: 3.56 | perplexity: 35.23 | lr: 2.38e-04 | norm: 0.3052 | dt: 20803.8013ms | tok/sec: 18.9012\n",
      "step 1970 | train loss: 3.73 | val loss: 3.56 | perplexity: 35.23 | lr: 2.37e-04 | norm: 0.2584 | dt: 21030.7498ms | tok/sec: 18.6972\n",
      "step 1971 | train loss: 3.71 | val loss: 3.56 | perplexity: 35.18 | lr: 2.36e-04 | norm: 0.3036 | dt: 20912.6894ms | tok/sec: 18.8027\n",
      "step 1972 | train loss: 3.53 | val loss: 3.56 | perplexity: 35.13 | lr: 2.36e-04 | norm: 0.3063 | dt: 21000.3431ms | tok/sec: 18.7243\n",
      "step 1973 | train loss: 3.44 | val loss: 3.56 | perplexity: 35.06 | lr: 2.35e-04 | norm: 0.2927 | dt: 20940.3064ms | tok/sec: 18.7779\n",
      "step 1974 | train loss: 3.64 | val loss: 3.56 | perplexity: 34.99 | lr: 2.35e-04 | norm: 0.2846 | dt: 20891.3627ms | tok/sec: 18.8219\n",
      "step 1975 | train loss: 3.66 | val loss: 3.55 | perplexity: 34.97 | lr: 2.34e-04 | norm: 0.2752 | dt: 20996.5465ms | tok/sec: 18.7277\n",
      "step 1976 | train loss: 3.83 | val loss: 3.56 | perplexity: 35.01 | lr: 2.33e-04 | norm: 0.2870 | dt: 20923.9037ms | tok/sec: 18.7927\n",
      "step 1977 | train loss: 3.77 | val loss: 3.56 | perplexity: 35.06 | lr: 2.33e-04 | norm: 0.2839 | dt: 20832.7498ms | tok/sec: 18.8749\n",
      "step 1978 | train loss: 3.65 | val loss: 3.56 | perplexity: 35.05 | lr: 2.32e-04 | norm: 0.2893 | dt: 20932.2200ms | tok/sec: 18.7852\n",
      "step 1979 | train loss: 3.90 | val loss: 3.56 | perplexity: 35.02 | lr: 2.32e-04 | norm: 0.3385 | dt: 21033.7727ms | tok/sec: 18.6945\n",
      "step 1980 | train loss: 3.59 | val loss: 3.55 | perplexity: 34.98 | lr: 2.31e-04 | norm: 0.3068 | dt: 21052.8283ms | tok/sec: 18.6776\n",
      "step 1981 | train loss: 3.64 | val loss: 3.55 | perplexity: 34.94 | lr: 2.30e-04 | norm: 0.2999 | dt: 21117.9824ms | tok/sec: 18.6200\n",
      "step 1982 | train loss: 3.70 | val loss: 3.55 | perplexity: 34.91 | lr: 2.30e-04 | norm: 0.2802 | dt: 21024.0550ms | tok/sec: 18.7031\n",
      "step 1983 | train loss: 3.39 | val loss: 3.55 | perplexity: 34.84 | lr: 2.29e-04 | norm: 0.2948 | dt: 21062.3000ms | tok/sec: 18.6692\n",
      "step 1984 | train loss: 3.80 | val loss: 3.55 | perplexity: 34.79 | lr: 2.29e-04 | norm: 0.2971 | dt: 20990.1946ms | tok/sec: 18.7333\n",
      "step 1985 | train loss: 3.72 | val loss: 3.55 | perplexity: 34.75 | lr: 2.28e-04 | norm: 0.3495 | dt: 20944.0069ms | tok/sec: 18.7746\n",
      "step 1986 | train loss: 3.60 | val loss: 3.55 | perplexity: 34.74 | lr: 2.28e-04 | norm: 0.2928 | dt: 19616.9674ms | tok/sec: 20.0447\n",
      "step 1987 | train loss: 3.76 | val loss: 3.55 | perplexity: 34.76 | lr: 2.27e-04 | norm: 0.2702 | dt: 19679.7547ms | tok/sec: 19.9807\n",
      "step 1988 | train loss: 3.65 | val loss: 3.55 | perplexity: 34.78 | lr: 2.26e-04 | norm: 0.2632 | dt: 19495.6107ms | tok/sec: 20.1695\n",
      "step 1989 | train loss: 3.38 | val loss: 3.55 | perplexity: 34.77 | lr: 2.26e-04 | norm: 0.2934 | dt: 19522.1124ms | tok/sec: 20.1421\n",
      "step 1990 | train loss: 3.56 | val loss: 3.55 | perplexity: 34.74 | lr: 2.25e-04 | norm: 0.3358 | dt: 19735.9855ms | tok/sec: 19.9238\n",
      "step 1991 | train loss: 3.62 | val loss: 3.55 | perplexity: 34.73 | lr: 2.25e-04 | norm: 0.2967 | dt: 19592.4344ms | tok/sec: 20.0698\n",
      "step 1992 | train loss: 3.66 | val loss: 3.55 | perplexity: 34.75 | lr: 2.24e-04 | norm: 0.3428 | dt: 19707.7377ms | tok/sec: 19.9524\n",
      "step 1993 | train loss: 3.60 | val loss: 3.55 | perplexity: 34.74 | lr: 2.23e-04 | norm: 0.3036 | dt: 19559.4587ms | tok/sec: 20.1036\n",
      "step 1994 | train loss: 3.82 | val loss: 3.55 | perplexity: 34.76 | lr: 2.23e-04 | norm: 0.2864 | dt: 19800.7350ms | tok/sec: 19.8587\n",
      "step 1995 | train loss: 3.70 | val loss: 3.55 | perplexity: 34.77 | lr: 2.22e-04 | norm: 0.3127 | dt: 19772.4020ms | tok/sec: 19.8871\n",
      "step 1996 | train loss: 3.49 | val loss: 3.55 | perplexity: 34.81 | lr: 2.22e-04 | norm: 0.2874 | dt: 19552.0985ms | tok/sec: 20.1112\n",
      "step 1997 | train loss: 3.69 | val loss: 3.55 | perplexity: 34.81 | lr: 2.21e-04 | norm: 0.3090 | dt: 19701.1883ms | tok/sec: 19.9590\n",
      "step 1998 | train loss: 3.76 | val loss: 3.55 | perplexity: 34.79 | lr: 2.21e-04 | norm: 0.2850 | dt: 19664.4645ms | tok/sec: 19.9963\n",
      "step 1999 | train loss: 3.40 | val loss: 3.55 | perplexity: 34.76 | lr: 2.20e-04 | norm: 0.3295 | dt: 19641.2606ms | tok/sec: 20.0199\n",
      "step 2000 | train loss: 3.65 | val loss: 3.55 | perplexity: 34.74 | lr: 2.19e-04 | norm: 0.3003 | dt: 19520.7434ms | tok/sec: 20.1435\n",
      "step 2001 | train loss: 3.65 | val loss: 3.55 | perplexity: 34.74 | lr: 2.19e-04 | norm: 0.3022 | dt: 19716.2313ms | tok/sec: 19.9438\n",
      "step 2002 | train loss: 3.49 | val loss: 3.55 | perplexity: 34.74 | lr: 2.18e-04 | norm: 0.2736 | dt: 19723.0523ms | tok/sec: 19.9369\n",
      "step 2003 | train loss: 3.46 | val loss: 3.55 | perplexity: 34.71 | lr: 2.18e-04 | norm: 0.2717 | dt: 19748.3869ms | tok/sec: 19.9113\n",
      "step 2004 | train loss: 3.54 | val loss: 3.55 | perplexity: 34.65 | lr: 2.17e-04 | norm: 0.3213 | dt: 19638.8891ms | tok/sec: 20.0223\n",
      "step 2005 | train loss: 3.49 | val loss: 3.54 | perplexity: 34.63 | lr: 2.17e-04 | norm: 0.2764 | dt: 19777.4699ms | tok/sec: 19.8820\n",
      "step 2006 | train loss: 3.82 | val loss: 3.54 | perplexity: 34.64 | lr: 2.16e-04 | norm: 0.2852 | dt: 19652.8411ms | tok/sec: 20.0081\n",
      "step 2007 | train loss: 4.04 | val loss: 3.55 | perplexity: 34.64 | lr: 2.15e-04 | norm: 0.3491 | dt: 19766.2992ms | tok/sec: 19.8933\n",
      "step 2008 | train loss: 3.44 | val loss: 3.55 | perplexity: 34.65 | lr: 2.15e-04 | norm: 0.2856 | dt: 19635.9775ms | tok/sec: 20.0253\n",
      "step 2009 | train loss: 3.70 | val loss: 3.55 | perplexity: 34.67 | lr: 2.14e-04 | norm: 0.2927 | dt: 19675.4301ms | tok/sec: 19.9851\n",
      "step 2010 | train loss: 3.55 | val loss: 3.55 | perplexity: 34.66 | lr: 2.14e-04 | norm: 0.3332 | dt: 19841.3553ms | tok/sec: 19.8180\n",
      "step 2011 | train loss: 3.62 | val loss: 3.54 | perplexity: 34.64 | lr: 2.13e-04 | norm: 0.2775 | dt: 19719.8899ms | tok/sec: 19.9401\n",
      "step 2012 | train loss: 3.47 | val loss: 3.54 | perplexity: 34.60 | lr: 2.13e-04 | norm: 0.2822 | dt: 19575.4275ms | tok/sec: 20.0872\n",
      "step 2013 | train loss: 3.70 | val loss: 3.54 | perplexity: 34.58 | lr: 2.12e-04 | norm: 0.2914 | dt: 19567.9708ms | tok/sec: 20.0949\n",
      "step 2014 | train loss: 3.84 | val loss: 3.54 | perplexity: 34.57 | lr: 2.12e-04 | norm: 0.3066 | dt: 19528.8966ms | tok/sec: 20.1351\n",
      "step 2015 | train loss: 3.72 | val loss: 3.55 | perplexity: 34.64 | lr: 2.11e-04 | norm: 0.2890 | dt: 19656.8508ms | tok/sec: 20.0040\n",
      "step 2016 | train loss: 3.43 | val loss: 3.55 | perplexity: 34.66 | lr: 2.10e-04 | norm: 0.3134 | dt: 19707.3383ms | tok/sec: 19.9528\n",
      "step 2017 | train loss: 3.29 | val loss: 3.54 | perplexity: 34.64 | lr: 2.10e-04 | norm: 0.3809 | dt: 21722.5013ms | tok/sec: 18.1018\n",
      "step 2018 | train loss: 3.50 | val loss: 3.55 | perplexity: 34.66 | lr: 2.09e-04 | norm: 0.3376 | dt: 20380.2929ms | tok/sec: 19.2939\n",
      "step 2019 | train loss: 3.21 | val loss: 3.55 | perplexity: 34.75 | lr: 2.09e-04 | norm: 0.3042 | dt: 20312.2144ms | tok/sec: 19.3586\n",
      "step 2020 | train loss: 3.75 | val loss: 3.55 | perplexity: 34.79 | lr: 2.08e-04 | norm: 0.2842 | dt: 20504.7500ms | tok/sec: 19.1768\n",
      "step 2021 | train loss: 3.51 | val loss: 3.55 | perplexity: 34.76 | lr: 2.08e-04 | norm: 0.3293 | dt: 20504.3693ms | tok/sec: 19.1772\n",
      "step 2022 | train loss: 3.76 | val loss: 3.55 | perplexity: 34.76 | lr: 2.07e-04 | norm: 0.3082 | dt: 20448.4324ms | tok/sec: 19.2296\n",
      "step 2023 | train loss: 3.52 | val loss: 3.55 | perplexity: 34.83 | lr: 2.07e-04 | norm: 0.2867 | dt: 20302.6381ms | tok/sec: 19.3677\n",
      "step 2024 | train loss: 3.60 | val loss: 3.55 | perplexity: 34.79 | lr: 2.06e-04 | norm: 0.3496 | dt: 20548.2764ms | tok/sec: 19.1362\n",
      "step 2025 | train loss: 3.88 | val loss: 3.55 | perplexity: 34.76 | lr: 2.06e-04 | norm: 0.3277 | dt: 20357.1095ms | tok/sec: 19.3159\n",
      "step 2026 | train loss: 3.60 | val loss: 3.55 | perplexity: 34.68 | lr: 2.05e-04 | norm: 0.3196 | dt: 20407.7337ms | tok/sec: 19.2680\n",
      "step 2027 | train loss: 3.50 | val loss: 3.54 | perplexity: 34.63 | lr: 2.05e-04 | norm: 0.3126 | dt: 20446.6183ms | tok/sec: 19.2313\n",
      "step 2028 | train loss: 3.59 | val loss: 3.54 | perplexity: 34.64 | lr: 2.04e-04 | norm: 0.2913 | dt: 20207.2966ms | tok/sec: 19.4591\n",
      "step 2029 | train loss: 3.69 | val loss: 3.54 | perplexity: 34.62 | lr: 2.03e-04 | norm: 0.2790 | dt: 20471.7753ms | tok/sec: 19.2077\n",
      "step 2030 | train loss: 3.53 | val loss: 3.54 | perplexity: 34.55 | lr: 2.03e-04 | norm: 0.2922 | dt: 20297.0109ms | tok/sec: 19.3731\n",
      "step 2031 | train loss: 3.29 | val loss: 3.54 | perplexity: 34.51 | lr: 2.02e-04 | norm: 0.2773 | dt: 20301.2114ms | tok/sec: 19.3691\n",
      "step 2032 | train loss: 3.35 | val loss: 3.54 | perplexity: 34.49 | lr: 2.02e-04 | norm: 0.3274 | dt: 20469.2273ms | tok/sec: 19.2101\n",
      "step 2033 | train loss: 3.46 | val loss: 3.54 | perplexity: 34.52 | lr: 2.01e-04 | norm: 0.3125 | dt: 20340.1318ms | tok/sec: 19.3320\n",
      "step 2034 | train loss: 3.69 | val loss: 3.54 | perplexity: 34.50 | lr: 2.01e-04 | norm: 0.3194 | dt: 20409.6091ms | tok/sec: 19.2662\n",
      "step 2035 | train loss: 3.78 | val loss: 3.54 | perplexity: 34.47 | lr: 2.00e-04 | norm: 0.2940 | dt: 20383.1508ms | tok/sec: 19.2912\n",
      "step 2036 | train loss: 3.86 | val loss: 3.54 | perplexity: 34.49 | lr: 2.00e-04 | norm: 0.3458 | dt: 20299.6931ms | tok/sec: 19.3705\n",
      "step 2037 | train loss: 3.32 | val loss: 3.54 | perplexity: 34.52 | lr: 1.99e-04 | norm: 0.3206 | dt: 20587.5266ms | tok/sec: 19.0997\n",
      "step 2038 | train loss: 3.64 | val loss: 3.54 | perplexity: 34.53 | lr: 1.99e-04 | norm: 0.3184 | dt: 20443.4128ms | tok/sec: 19.2344\n",
      "step 2039 | train loss: 3.66 | val loss: 3.54 | perplexity: 34.52 | lr: 1.98e-04 | norm: 0.2852 | dt: 20399.7419ms | tok/sec: 19.2755\n",
      "step 2040 | train loss: 3.65 | val loss: 3.54 | perplexity: 34.52 | lr: 1.98e-04 | norm: 0.2989 | dt: 20273.4489ms | tok/sec: 19.3956\n",
      "step 2041 | train loss: 3.89 | val loss: 3.54 | perplexity: 34.54 | lr: 1.97e-04 | norm: 0.3032 | dt: 20366.1108ms | tok/sec: 19.3074\n",
      "step 2042 | train loss: 3.72 | val loss: 3.54 | perplexity: 34.55 | lr: 1.97e-04 | norm: 0.3120 | dt: 20405.8793ms | tok/sec: 19.2697\n",
      "step 2043 | train loss: 3.61 | val loss: 3.54 | perplexity: 34.56 | lr: 1.96e-04 | norm: 0.2927 | dt: 20368.3078ms | tok/sec: 19.3053\n",
      "step 2044 | train loss: 3.66 | val loss: 3.54 | perplexity: 34.54 | lr: 1.96e-04 | norm: 0.3275 | dt: 20313.7753ms | tok/sec: 19.3571\n",
      "step 2045 | train loss: 3.71 | val loss: 3.54 | perplexity: 34.54 | lr: 1.95e-04 | norm: 0.2789 | dt: 20330.9963ms | tok/sec: 19.3407\n",
      "step 2046 | train loss: 3.73 | val loss: 3.54 | perplexity: 34.52 | lr: 1.95e-04 | norm: 0.3008 | dt: 20409.9154ms | tok/sec: 19.2659\n",
      "step 2047 | train loss: 3.81 | val loss: 3.54 | perplexity: 34.49 | lr: 1.94e-04 | norm: 0.2802 | dt: 20216.4607ms | tok/sec: 19.4503\n",
      "step 2048 | train loss: 3.88 | val loss: 3.54 | perplexity: 34.46 | lr: 1.93e-04 | norm: 0.3399 | dt: 20387.0814ms | tok/sec: 19.2875\n",
      "step 2049 | train loss: 3.52 | val loss: 3.54 | perplexity: 34.50 | lr: 1.93e-04 | norm: 0.2541 | dt: 20389.5447ms | tok/sec: 19.2852\n",
      "step 2050 | train loss: 3.62 | val loss: 3.54 | perplexity: 34.48 | lr: 1.92e-04 | norm: 0.3056 | dt: 20534.2109ms | tok/sec: 19.1493\n",
      "step 2051 | train loss: 3.67 | val loss: 3.54 | perplexity: 34.41 | lr: 1.92e-04 | norm: 0.4180 | dt: 20282.7845ms | tok/sec: 19.3867\n",
      "step 2052 | train loss: 3.52 | val loss: 3.54 | perplexity: 34.41 | lr: 1.91e-04 | norm: 0.3214 | dt: 20426.7216ms | tok/sec: 19.2501\n",
      "step 2053 | train loss: 3.62 | val loss: 3.54 | perplexity: 34.41 | lr: 1.91e-04 | norm: 0.2776 | dt: 20436.4207ms | tok/sec: 19.2409\n",
      "step 2054 | train loss: 3.51 | val loss: 3.54 | perplexity: 34.39 | lr: 1.90e-04 | norm: 0.3365 | dt: 20479.3155ms | tok/sec: 19.2006\n",
      "step 2055 | train loss: 4.02 | val loss: 3.54 | perplexity: 34.40 | lr: 1.90e-04 | norm: 0.4242 | dt: 20538.2531ms | tok/sec: 19.1455\n",
      "step 2056 | train loss: 3.91 | val loss: 3.54 | perplexity: 34.41 | lr: 1.89e-04 | norm: 0.4011 | dt: 20257.9389ms | tok/sec: 19.4105\n",
      "step 2057 | train loss: 3.44 | val loss: 3.54 | perplexity: 34.37 | lr: 1.89e-04 | norm: 0.3897 | dt: 20569.0739ms | tok/sec: 19.1169\n",
      "step 2058 | train loss: 3.51 | val loss: 3.54 | perplexity: 34.35 | lr: 1.88e-04 | norm: 0.2973 | dt: 20666.7252ms | tok/sec: 19.0265\n",
      "step 2059 | train loss: 3.31 | val loss: 3.54 | perplexity: 34.35 | lr: 1.88e-04 | norm: 0.2997 | dt: 20470.8376ms | tok/sec: 19.2086\n",
      "step 2060 | train loss: 3.63 | val loss: 3.54 | perplexity: 34.31 | lr: 1.87e-04 | norm: 0.3636 | dt: 20536.3765ms | tok/sec: 19.1473\n",
      "step 2061 | train loss: 3.64 | val loss: 3.54 | perplexity: 34.34 | lr: 1.87e-04 | norm: 0.4061 | dt: 20691.7541ms | tok/sec: 19.0035\n",
      "step 2062 | train loss: 3.39 | val loss: 3.54 | perplexity: 34.38 | lr: 1.86e-04 | norm: 0.2859 | dt: 20338.8114ms | tok/sec: 19.3333\n",
      "step 2063 | train loss: 3.38 | val loss: 3.54 | perplexity: 34.39 | lr: 1.86e-04 | norm: 0.3696 | dt: 20423.5578ms | tok/sec: 19.2531\n",
      "step 2064 | train loss: 3.21 | val loss: 3.54 | perplexity: 34.44 | lr: 1.85e-04 | norm: 0.3784 | dt: 20490.4652ms | tok/sec: 19.1902\n",
      "step 2065 | train loss: 3.71 | val loss: 3.54 | perplexity: 34.49 | lr: 1.85e-04 | norm: 0.3411 | dt: 20554.4159ms | tok/sec: 19.1305\n",
      "step 2066 | train loss: 4.63 | val loss: 3.54 | perplexity: 34.59 | lr: 1.84e-04 | norm: 0.5189 | dt: 20444.8681ms | tok/sec: 19.2330\n",
      "step 2067 | train loss: 3.43 | val loss: 3.55 | perplexity: 34.67 | lr: 1.84e-04 | norm: 0.3261 | dt: 20435.9612ms | tok/sec: 19.2414\n",
      "step 2068 | train loss: 3.42 | val loss: 3.55 | perplexity: 34.66 | lr: 1.83e-04 | norm: 0.3530 | dt: 20519.0537ms | tok/sec: 19.1635\n",
      "step 2069 | train loss: 3.26 | val loss: 3.54 | perplexity: 34.64 | lr: 1.83e-04 | norm: 0.3624 | dt: 20427.8140ms | tok/sec: 19.2490\n",
      "step 2070 | train loss: 3.38 | val loss: 3.55 | perplexity: 34.72 | lr: 1.83e-04 | norm: 0.2824 | dt: 20607.8637ms | tok/sec: 19.0809\n",
      "step 2071 | train loss: 3.60 | val loss: 3.55 | perplexity: 34.80 | lr: 1.82e-04 | norm: 0.4302 | dt: 20352.2680ms | tok/sec: 19.3205\n",
      "step 2072 | train loss: 3.66 | val loss: 3.55 | perplexity: 34.90 | lr: 1.82e-04 | norm: 0.3802 | dt: 20377.5117ms | tok/sec: 19.2966\n",
      "step 2073 | train loss: 3.57 | val loss: 3.55 | perplexity: 34.90 | lr: 1.81e-04 | norm: 0.4578 | dt: 20395.3340ms | tok/sec: 19.2797\n",
      "step 2074 | train loss: 3.51 | val loss: 3.55 | perplexity: 34.85 | lr: 1.81e-04 | norm: 0.4481 | dt: 20390.4579ms | tok/sec: 19.2843\n",
      "step 2075 | train loss: 3.44 | val loss: 3.55 | perplexity: 34.86 | lr: 1.80e-04 | norm: 0.4033 | dt: 20396.6160ms | tok/sec: 19.2785\n",
      "step 2076 | train loss: 3.45 | val loss: 3.55 | perplexity: 34.86 | lr: 1.80e-04 | norm: 0.3632 | dt: 20289.9191ms | tok/sec: 19.3799\n",
      "step 2077 | train loss: 3.51 | val loss: 3.55 | perplexity: 34.83 | lr: 1.79e-04 | norm: 0.6244 | dt: 20356.8423ms | tok/sec: 19.3162\n",
      "step 2078 | train loss: 3.54 | val loss: 3.55 | perplexity: 34.76 | lr: 1.79e-04 | norm: 0.3858 | dt: 20230.9649ms | tok/sec: 19.4363\n",
      "step 2079 | train loss: 3.48 | val loss: 3.55 | perplexity: 34.76 | lr: 1.78e-04 | norm: 0.3623 | dt: 20351.1765ms | tok/sec: 19.3215\n",
      "step 2080 | train loss: 3.37 | val loss: 3.55 | perplexity: 34.73 | lr: 1.78e-04 | norm: 0.3781 | dt: 20430.7933ms | tok/sec: 19.2462\n",
      "step 2081 | train loss: 3.51 | val loss: 3.55 | perplexity: 34.69 | lr: 1.77e-04 | norm: 0.3031 | dt: 20574.4684ms | tok/sec: 19.1118\n",
      "step 2082 | train loss: 3.55 | val loss: 3.55 | perplexity: 34.64 | lr: 1.77e-04 | norm: 0.3036 | dt: 20475.0509ms | tok/sec: 19.2046\n",
      "step 2083 | train loss: 3.56 | val loss: 3.54 | perplexity: 34.60 | lr: 1.76e-04 | norm: 0.3066 | dt: 20559.8993ms | tok/sec: 19.1254\n",
      "step 2084 | train loss: 3.78 | val loss: 3.54 | perplexity: 34.56 | lr: 1.76e-04 | norm: 0.3737 | dt: 20507.8285ms | tok/sec: 19.1739\n",
      "step 2085 | train loss: 3.85 | val loss: 3.54 | perplexity: 34.58 | lr: 1.75e-04 | norm: 0.3458 | dt: 20371.5801ms | tok/sec: 19.3022\n",
      "step 2086 | train loss: 3.36 | val loss: 3.54 | perplexity: 34.57 | lr: 1.75e-04 | norm: 0.3364 | dt: 20559.9382ms | tok/sec: 19.1253\n",
      "step 2087 | train loss: 3.38 | val loss: 3.54 | perplexity: 34.50 | lr: 1.74e-04 | norm: 0.4264 | dt: 20374.2588ms | tok/sec: 19.2996\n",
      "step 2088 | train loss: 3.23 | val loss: 3.54 | perplexity: 34.46 | lr: 1.74e-04 | norm: 0.2790 | dt: 20458.9920ms | tok/sec: 19.2197\n",
      "step 2089 | train loss: 3.31 | val loss: 3.54 | perplexity: 34.45 | lr: 1.74e-04 | norm: 0.2695 | dt: 20535.1105ms | tok/sec: 19.1485\n",
      "step 2090 | train loss: 3.68 | val loss: 3.54 | perplexity: 34.38 | lr: 1.73e-04 | norm: 0.3211 | dt: 20400.3563ms | tok/sec: 19.2750\n",
      "step 2091 | train loss: 3.47 | val loss: 3.54 | perplexity: 34.33 | lr: 1.73e-04 | norm: 0.3660 | dt: 20476.6815ms | tok/sec: 19.2031\n",
      "step 2092 | train loss: 3.44 | val loss: 3.54 | perplexity: 34.33 | lr: 1.72e-04 | norm: 0.2661 | dt: 20457.0954ms | tok/sec: 19.2215\n",
      "step 2093 | train loss: 3.17 | val loss: 3.54 | perplexity: 34.31 | lr: 1.72e-04 | norm: 0.3237 | dt: 20337.4190ms | tok/sec: 19.3346\n",
      "step 2094 | train loss: 3.19 | val loss: 3.54 | perplexity: 34.31 | lr: 1.71e-04 | norm: 0.3705 | dt: 20544.4744ms | tok/sec: 19.1397\n",
      "step 2095 | train loss: 3.34 | val loss: 3.54 | perplexity: 34.37 | lr: 1.71e-04 | norm: 0.3428 | dt: 20533.1123ms | tok/sec: 19.1503\n",
      "step 2096 | train loss: 3.42 | val loss: 3.54 | perplexity: 34.42 | lr: 1.70e-04 | norm: 0.3021 | dt: 20507.1938ms | tok/sec: 19.1745\n",
      "step 2097 | train loss: 3.92 | val loss: 3.54 | perplexity: 34.43 | lr: 1.70e-04 | norm: 0.3541 | dt: 20560.6954ms | tok/sec: 19.1246\n",
      "step 2098 | train loss: 3.72 | val loss: 3.54 | perplexity: 34.41 | lr: 1.69e-04 | norm: 0.4124 | dt: 20496.9115ms | tok/sec: 19.1842\n",
      "step 2099 | train loss: 3.50 | val loss: 3.54 | perplexity: 34.35 | lr: 1.69e-04 | norm: 0.3386 | dt: 20439.2257ms | tok/sec: 19.2383\n",
      "step 2100 | train loss: 3.69 | val loss: 3.53 | perplexity: 34.28 | lr: 1.69e-04 | norm: 0.2866 | dt: 20508.1029ms | tok/sec: 19.1737\n",
      "step 2101 | train loss: 3.55 | val loss: 3.53 | perplexity: 34.22 | lr: 1.68e-04 | norm: 0.3191 | dt: 20413.8892ms | tok/sec: 19.2622\n",
      "step 2102 | train loss: 3.40 | val loss: 3.53 | perplexity: 34.17 | lr: 1.68e-04 | norm: 0.4423 | dt: 20372.6168ms | tok/sec: 19.3012\n",
      "step 2103 | train loss: 3.29 | val loss: 3.53 | perplexity: 34.20 | lr: 1.67e-04 | norm: 0.3919 | dt: 20442.2052ms | tok/sec: 19.2355\n",
      "step 2104 | train loss: 3.51 | val loss: 3.53 | perplexity: 34.23 | lr: 1.67e-04 | norm: 0.3105 | dt: 20535.7163ms | tok/sec: 19.1479\n",
      "step 2105 | train loss: 3.65 | val loss: 3.53 | perplexity: 34.26 | lr: 1.66e-04 | norm: 0.3614 | dt: 20542.8698ms | tok/sec: 19.1412\n",
      "step 2106 | train loss: 3.53 | val loss: 3.53 | perplexity: 34.27 | lr: 1.66e-04 | norm: 0.3503 | dt: 20463.3760ms | tok/sec: 19.2156\n",
      "step 2107 | train loss: 3.48 | val loss: 3.53 | perplexity: 34.27 | lr: 1.65e-04 | norm: 0.3328 | dt: 20542.7182ms | tok/sec: 19.1414\n",
      "step 2108 | train loss: 3.34 | val loss: 3.53 | perplexity: 34.26 | lr: 1.65e-04 | norm: 0.3050 | dt: 20549.9930ms | tok/sec: 19.1346\n",
      "step 2109 | train loss: 3.34 | val loss: 3.53 | perplexity: 34.23 | lr: 1.65e-04 | norm: 0.2839 | dt: 20434.2048ms | tok/sec: 19.2430\n",
      "step 2110 | train loss: 3.57 | val loss: 3.53 | perplexity: 34.22 | lr: 1.64e-04 | norm: 0.2857 | dt: 20404.9246ms | tok/sec: 19.2706\n",
      "step 2111 | train loss: 3.44 | val loss: 3.53 | perplexity: 34.22 | lr: 1.64e-04 | norm: 0.3121 | dt: 20459.1873ms | tok/sec: 19.2195\n",
      "step 2112 | train loss: 3.41 | val loss: 3.53 | perplexity: 34.23 | lr: 1.63e-04 | norm: 0.2654 | dt: 20556.8609ms | tok/sec: 19.1282\n",
      "step 2113 | train loss: 3.83 | val loss: 3.53 | perplexity: 34.23 | lr: 1.63e-04 | norm: 0.5800 | dt: 20423.2047ms | tok/sec: 19.2534\n",
      "step 2114 | train loss: 4.26 | val loss: 3.54 | perplexity: 34.30 | lr: 1.62e-04 | norm: 0.5446 | dt: 20674.3653ms | tok/sec: 19.0195\n",
      "step 2115 | train loss: 3.54 | val loss: 3.54 | perplexity: 34.32 | lr: 1.62e-04 | norm: 0.3197 | dt: 20386.1487ms | tok/sec: 19.2884\n",
      "step 2116 | train loss: 3.84 | val loss: 3.53 | perplexity: 34.28 | lr: 1.62e-04 | norm: 0.3590 | dt: 20501.4417ms | tok/sec: 19.1799\n",
      "step 2117 | train loss: 3.51 | val loss: 3.53 | perplexity: 34.17 | lr: 1.61e-04 | norm: 0.4189 | dt: 20564.7824ms | tok/sec: 19.1208\n",
      "step 2118 | train loss: 3.38 | val loss: 3.53 | perplexity: 34.07 | lr: 1.61e-04 | norm: 0.3136 | dt: 20510.9179ms | tok/sec: 19.1711\n",
      "step 2119 | train loss: 3.51 | val loss: 3.53 | perplexity: 34.05 | lr: 1.60e-04 | norm: 0.2795 | dt: 20507.3490ms | tok/sec: 19.1744\n",
      "step 2120 | train loss: 3.42 | val loss: 3.53 | perplexity: 34.05 | lr: 1.60e-04 | norm: 0.3036 | dt: 20533.3517ms | tok/sec: 19.1501\n",
      "step 2121 | train loss: 3.61 | val loss: 3.53 | perplexity: 34.07 | lr: 1.60e-04 | norm: 0.2899 | dt: 20518.4791ms | tok/sec: 19.1640\n",
      "step 2122 | train loss: 3.50 | val loss: 3.53 | perplexity: 34.09 | lr: 1.59e-04 | norm: 0.2970 | dt: 20494.4348ms | tok/sec: 19.1865\n",
      "step 2123 | train loss: 3.40 | val loss: 3.53 | perplexity: 34.10 | lr: 1.59e-04 | norm: 0.3642 | dt: 22341.2964ms | tok/sec: 17.6004\n",
      "step 2124 | train loss: 3.56 | val loss: 3.53 | perplexity: 34.11 | lr: 1.58e-04 | norm: 0.3182 | dt: 20723.8708ms | tok/sec: 18.9741\n",
      "step 2125 | train loss: 3.71 | val loss: 3.53 | perplexity: 34.07 | lr: 1.58e-04 | norm: 0.3356 | dt: 20658.4918ms | tok/sec: 19.0341\n",
      "step 2126 | train loss: 3.44 | val loss: 3.53 | perplexity: 34.01 | lr: 1.57e-04 | norm: 0.3530 | dt: 20689.0347ms | tok/sec: 19.0060\n",
      "step 2127 | train loss: 3.62 | val loss: 3.53 | perplexity: 33.96 | lr: 1.57e-04 | norm: 0.2919 | dt: 20667.4762ms | tok/sec: 19.0258\n",
      "step 2128 | train loss: 4.02 | val loss: 3.52 | perplexity: 33.94 | lr: 1.57e-04 | norm: 0.3451 | dt: 20775.3260ms | tok/sec: 18.9271\n",
      "step 2129 | train loss: 3.87 | val loss: 3.53 | perplexity: 33.96 | lr: 1.56e-04 | norm: 0.4882 | dt: 20684.7870ms | tok/sec: 19.0099\n",
      "step 2130 | train loss: 3.33 | val loss: 3.53 | perplexity: 33.96 | lr: 1.56e-04 | norm: 0.2961 | dt: 20629.2958ms | tok/sec: 19.0610\n",
      "step 2131 | train loss: 3.51 | val loss: 3.53 | perplexity: 34.01 | lr: 1.55e-04 | norm: 0.5796 | dt: 20749.2778ms | tok/sec: 18.9508\n",
      "step 2132 | train loss: 3.85 | val loss: 3.53 | perplexity: 34.04 | lr: 1.55e-04 | norm: 0.3163 | dt: 21041.9858ms | tok/sec: 18.6872\n",
      "step 2133 | train loss: 3.40 | val loss: 3.53 | perplexity: 34.01 | lr: 1.55e-04 | norm: 0.3354 | dt: 20769.7139ms | tok/sec: 18.9322\n",
      "step 2134 | train loss: 3.52 | val loss: 3.53 | perplexity: 33.97 | lr: 1.54e-04 | norm: 0.3097 | dt: 20659.2057ms | tok/sec: 19.0335\n",
      "step 2135 | train loss: 3.68 | val loss: 3.53 | perplexity: 33.96 | lr: 1.54e-04 | norm: 0.2820 | dt: 20659.3080ms | tok/sec: 19.0334\n",
      "step 2136 | train loss: 3.44 | val loss: 3.53 | perplexity: 33.98 | lr: 1.53e-04 | norm: 0.3640 | dt: 20646.1458ms | tok/sec: 19.0455\n",
      "step 2137 | train loss: 3.55 | val loss: 3.53 | perplexity: 33.98 | lr: 1.53e-04 | norm: 0.3341 | dt: 20743.4127ms | tok/sec: 18.9562\n",
      "step 2138 | train loss: 3.37 | val loss: 3.52 | perplexity: 33.94 | lr: 1.53e-04 | norm: 0.3225 | dt: 20651.0541ms | tok/sec: 19.0410\n",
      "step 2139 | train loss: 3.66 | val loss: 3.52 | perplexity: 33.88 | lr: 1.52e-04 | norm: 0.3539 | dt: 20734.9520ms | tok/sec: 18.9639\n",
      "step 2140 | train loss: 3.26 | val loss: 3.52 | perplexity: 33.88 | lr: 1.52e-04 | norm: 0.2829 | dt: 20731.0753ms | tok/sec: 18.9675\n",
      "step 2141 | train loss: 3.38 | val loss: 3.52 | perplexity: 33.94 | lr: 1.51e-04 | norm: 0.2904 | dt: 20910.5356ms | tok/sec: 18.8047\n",
      "step 2142 | train loss: 3.66 | val loss: 3.53 | perplexity: 34.00 | lr: 1.51e-04 | norm: 0.2892 | dt: 20717.9034ms | tok/sec: 18.9795\n",
      "step 2143 | train loss: 3.36 | val loss: 3.53 | perplexity: 34.05 | lr: 1.51e-04 | norm: 0.3542 | dt: 20710.4490ms | tok/sec: 18.9864\n",
      "step 2144 | train loss: 3.50 | val loss: 3.53 | perplexity: 34.14 | lr: 1.50e-04 | norm: 0.4125 | dt: 20597.6560ms | tok/sec: 19.0903\n",
      "step 2145 | train loss: 3.43 | val loss: 3.53 | perplexity: 34.12 | lr: 1.50e-04 | norm: 0.3603 | dt: 20612.3657ms | tok/sec: 19.0767\n",
      "step 2146 | train loss: 3.81 | val loss: 3.53 | perplexity: 34.02 | lr: 1.50e-04 | norm: 0.4150 | dt: 20614.3246ms | tok/sec: 19.0749\n",
      "step 2147 | train loss: 3.77 | val loss: 3.53 | perplexity: 33.96 | lr: 1.49e-04 | norm: 0.3120 | dt: 20746.6903ms | tok/sec: 18.9532\n",
      "step 2148 | train loss: 3.65 | val loss: 3.53 | perplexity: 33.99 | lr: 1.49e-04 | norm: 0.2724 | dt: 20610.4174ms | tok/sec: 19.0785\n",
      "step 2149 | train loss: 3.56 | val loss: 3.53 | perplexity: 34.02 | lr: 1.48e-04 | norm: 0.3147 | dt: 20711.9639ms | tok/sec: 18.9850\n",
      "step 2150 | train loss: 3.48 | val loss: 3.53 | perplexity: 34.00 | lr: 1.48e-04 | norm: 0.2839 | dt: 20543.8893ms | tok/sec: 19.1403\n",
      "step 2151 | train loss: 3.53 | val loss: 3.52 | perplexity: 33.95 | lr: 1.48e-04 | norm: 0.3185 | dt: 20608.3071ms | tok/sec: 19.0805\n",
      "step 2152 | train loss: 3.72 | val loss: 3.52 | perplexity: 33.91 | lr: 1.47e-04 | norm: 0.3733 | dt: 20727.3378ms | tok/sec: 18.9709\n",
      "step 2153 | train loss: 3.28 | val loss: 3.52 | perplexity: 33.93 | lr: 1.47e-04 | norm: 0.3383 | dt: 20691.8228ms | tok/sec: 19.0034\n",
      "step 2154 | train loss: 3.44 | val loss: 3.53 | perplexity: 33.96 | lr: 1.47e-04 | norm: 0.2963 | dt: 20741.3695ms | tok/sec: 18.9581\n",
      "step 2155 | train loss: 3.37 | val loss: 3.53 | perplexity: 33.97 | lr: 1.46e-04 | norm: 0.3273 | dt: 20660.0499ms | tok/sec: 19.0327\n",
      "step 2156 | train loss: 3.76 | val loss: 3.53 | perplexity: 33.96 | lr: 1.46e-04 | norm: 0.3328 | dt: 20677.2652ms | tok/sec: 19.0168\n",
      "step 2157 | train loss: 3.33 | val loss: 3.53 | perplexity: 33.99 | lr: 1.45e-04 | norm: 0.4482 | dt: 20580.7467ms | tok/sec: 19.1060\n",
      "step 2158 | train loss: 3.51 | val loss: 3.53 | perplexity: 34.00 | lr: 1.45e-04 | norm: 0.2934 | dt: 20649.8301ms | tok/sec: 19.0421\n",
      "step 2159 | train loss: 3.41 | val loss: 3.52 | perplexity: 33.95 | lr: 1.45e-04 | norm: 0.3322 | dt: 20655.7031ms | tok/sec: 19.0367\n",
      "step 2160 | train loss: 3.60 | val loss: 3.52 | perplexity: 33.90 | lr: 1.44e-04 | norm: 0.3140 | dt: 20719.1496ms | tok/sec: 18.9784\n",
      "step 2161 | train loss: 3.57 | val loss: 3.52 | perplexity: 33.90 | lr: 1.44e-04 | norm: 0.2876 | dt: 20589.0234ms | tok/sec: 19.0983\n",
      "step 2162 | train loss: 3.48 | val loss: 3.52 | perplexity: 33.93 | lr: 1.44e-04 | norm: 0.3255 | dt: 20810.2570ms | tok/sec: 18.8953\n",
      "step 2163 | train loss: 3.52 | val loss: 3.52 | perplexity: 33.94 | lr: 1.43e-04 | norm: 0.3476 | dt: 20716.7709ms | tok/sec: 18.9806\n",
      "step 2164 | train loss: 3.66 | val loss: 3.53 | perplexity: 33.96 | lr: 1.43e-04 | norm: 0.3772 | dt: 20704.4902ms | tok/sec: 18.9918\n",
      "step 2165 | train loss: 3.77 | val loss: 3.53 | perplexity: 33.96 | lr: 1.43e-04 | norm: 0.3223 | dt: 20761.6880ms | tok/sec: 18.9395\n",
      "step 2166 | train loss: 3.42 | val loss: 3.52 | perplexity: 33.94 | lr: 1.42e-04 | norm: 0.3030 | dt: 20734.1833ms | tok/sec: 18.9646\n",
      "step 2167 | train loss: 3.37 | val loss: 3.52 | perplexity: 33.91 | lr: 1.42e-04 | norm: 0.2971 | dt: 20818.8446ms | tok/sec: 18.8875\n",
      "step 2168 | train loss: 3.70 | val loss: 3.52 | perplexity: 33.85 | lr: 1.41e-04 | norm: 0.3775 | dt: 20668.4718ms | tok/sec: 19.0249\n",
      "step 2169 | train loss: 3.52 | val loss: 3.52 | perplexity: 33.80 | lr: 1.41e-04 | norm: 0.2774 | dt: 20693.2266ms | tok/sec: 19.0022\n",
      "step 2170 | train loss: 3.45 | val loss: 3.52 | perplexity: 33.79 | lr: 1.41e-04 | norm: 0.3298 | dt: 20789.0675ms | tok/sec: 18.9146\n",
      "step 2171 | train loss: 3.34 | val loss: 3.52 | perplexity: 33.80 | lr: 1.40e-04 | norm: 0.3171 | dt: 20742.2500ms | tok/sec: 18.9572\n",
      "step 2172 | train loss: 3.32 | val loss: 3.52 | perplexity: 33.80 | lr: 1.40e-04 | norm: 0.3086 | dt: 20595.1326ms | tok/sec: 19.0927\n",
      "step 2173 | train loss: 3.42 | val loss: 3.52 | perplexity: 33.80 | lr: 1.40e-04 | norm: 0.2835 | dt: 20773.7062ms | tok/sec: 18.9285\n",
      "step 2174 | train loss: 3.49 | val loss: 3.52 | perplexity: 33.78 | lr: 1.39e-04 | norm: 0.2801 | dt: 20685.5824ms | tok/sec: 19.0092\n",
      "step 2175 | train loss: 3.32 | val loss: 3.52 | perplexity: 33.74 | lr: 1.39e-04 | norm: 0.3875 | dt: 20672.0183ms | tok/sec: 19.0217\n",
      "step 2176 | train loss: 3.66 | val loss: 3.52 | perplexity: 33.77 | lr: 1.39e-04 | norm: 0.2867 | dt: 20730.3391ms | tok/sec: 18.9681\n",
      "step 2177 | train loss: 3.55 | val loss: 3.52 | perplexity: 33.88 | lr: 1.38e-04 | norm: 0.2651 | dt: 20688.1137ms | tok/sec: 19.0069\n",
      "step 2178 | train loss: 3.47 | val loss: 3.53 | perplexity: 33.96 | lr: 1.38e-04 | norm: 0.3166 | dt: 20670.0981ms | tok/sec: 19.0234\n",
      "step 2179 | train loss: 3.45 | val loss: 3.52 | perplexity: 33.94 | lr: 1.38e-04 | norm: 0.3270 | dt: 20621.3396ms | tok/sec: 19.0684\n",
      "step 2180 | train loss: 3.53 | val loss: 3.52 | perplexity: 33.88 | lr: 1.37e-04 | norm: 0.3423 | dt: 20759.6729ms | tok/sec: 18.9413\n",
      "step 2181 | train loss: 3.58 | val loss: 3.52 | perplexity: 33.84 | lr: 1.37e-04 | norm: 0.2807 | dt: 20735.3501ms | tok/sec: 18.9636\n",
      "step 2182 | train loss: 3.52 | val loss: 3.52 | perplexity: 33.82 | lr: 1.37e-04 | norm: 0.2566 | dt: 20765.7795ms | tok/sec: 18.9358\n",
      "step 2183 | train loss: 3.11 | val loss: 3.52 | perplexity: 33.79 | lr: 1.36e-04 | norm: 0.3083 | dt: 20586.7653ms | tok/sec: 19.1004\n",
      "step 2184 | train loss: 3.58 | val loss: 3.52 | perplexity: 33.74 | lr: 1.36e-04 | norm: 0.3276 | dt: 20802.4766ms | tok/sec: 18.9024\n",
      "step 2185 | train loss: 3.71 | val loss: 3.52 | perplexity: 33.69 | lr: 1.36e-04 | norm: 0.2888 | dt: 20547.2634ms | tok/sec: 19.1371\n",
      "step 2186 | train loss: 3.75 | val loss: 3.52 | perplexity: 33.73 | lr: 1.35e-04 | norm: 0.4170 | dt: 20849.1759ms | tok/sec: 18.8600\n",
      "step 2187 | train loss: 3.58 | val loss: 3.52 | perplexity: 33.79 | lr: 1.35e-04 | norm: 0.3017 | dt: 20548.7614ms | tok/sec: 19.1358\n",
      "step 2188 | train loss: 3.47 | val loss: 3.52 | perplexity: 33.81 | lr: 1.35e-04 | norm: 0.3331 | dt: 20520.3416ms | tok/sec: 19.1623\n",
      "step 2189 | train loss: 3.40 | val loss: 3.52 | perplexity: 33.80 | lr: 1.34e-04 | norm: 0.3610 | dt: 20687.8221ms | tok/sec: 19.0071\n",
      "step 2190 | train loss: 3.84 | val loss: 3.52 | perplexity: 33.76 | lr: 1.34e-04 | norm: 0.2985 | dt: 20562.0344ms | tok/sec: 19.1234\n",
      "step 2191 | train loss: 3.61 | val loss: 3.52 | perplexity: 33.73 | lr: 1.34e-04 | norm: 0.3010 | dt: 20786.9854ms | tok/sec: 18.9165\n",
      "step 2192 | train loss: 3.52 | val loss: 3.52 | perplexity: 33.70 | lr: 1.33e-04 | norm: 0.2614 | dt: 20610.1067ms | tok/sec: 19.0788\n",
      "step 2193 | train loss: 3.28 | val loss: 3.52 | perplexity: 33.68 | lr: 1.33e-04 | norm: 0.3196 | dt: 20864.4731ms | tok/sec: 18.8462\n",
      "step 2194 | train loss: 3.35 | val loss: 3.52 | perplexity: 33.68 | lr: 1.33e-04 | norm: 0.2632 | dt: 20699.7592ms | tok/sec: 18.9962\n",
      "step 2195 | train loss: 3.61 | val loss: 3.52 | perplexity: 33.67 | lr: 1.32e-04 | norm: 0.2781 | dt: 20713.3007ms | tok/sec: 18.9837\n",
      "step 2196 | train loss: 3.55 | val loss: 3.52 | perplexity: 33.63 | lr: 1.32e-04 | norm: 0.2581 | dt: 20768.7042ms | tok/sec: 18.9331\n",
      "step 2197 | train loss: 3.58 | val loss: 3.51 | perplexity: 33.59 | lr: 1.32e-04 | norm: 0.2631 | dt: 20651.2935ms | tok/sec: 19.0407\n",
      "step 2198 | train loss: 3.49 | val loss: 3.51 | perplexity: 33.55 | lr: 1.32e-04 | norm: 0.2571 | dt: 20734.5014ms | tok/sec: 18.9643\n",
      "step 2199 | train loss: 3.79 | val loss: 3.51 | perplexity: 33.54 | lr: 1.31e-04 | norm: 0.2366 | dt: 20703.2385ms | tok/sec: 18.9930\n",
      "step 2200 | train loss: 3.50 | val loss: 3.51 | perplexity: 33.52 | lr: 1.31e-04 | norm: 0.2493 | dt: 20595.4449ms | tok/sec: 19.0924\n",
      "step 2201 | train loss: 4.10 | val loss: 3.51 | perplexity: 33.54 | lr: 1.31e-04 | norm: 0.3517 | dt: 20734.7951ms | tok/sec: 18.9641\n",
      "step 2202 | train loss: 3.59 | val loss: 3.51 | perplexity: 33.56 | lr: 1.30e-04 | norm: 0.2436 | dt: 20693.5394ms | tok/sec: 19.0019\n",
      "step 2203 | train loss: 3.38 | val loss: 3.51 | perplexity: 33.57 | lr: 1.30e-04 | norm: 0.2631 | dt: 20570.3578ms | tok/sec: 19.1157\n",
      "step 2204 | train loss: 3.48 | val loss: 3.51 | perplexity: 33.55 | lr: 1.30e-04 | norm: 0.2970 | dt: 20695.4460ms | tok/sec: 19.0001\n",
      "step 2205 | train loss: 3.69 | val loss: 3.51 | perplexity: 33.53 | lr: 1.29e-04 | norm: 0.2860 | dt: 20614.9769ms | tok/sec: 19.0743\n",
      "step 2206 | train loss: 3.62 | val loss: 3.51 | perplexity: 33.52 | lr: 1.29e-04 | norm: 0.2652 | dt: 20552.4769ms | tok/sec: 19.1323\n",
      "step 2207 | train loss: 3.64 | val loss: 3.51 | perplexity: 33.52 | lr: 1.29e-04 | norm: 0.2501 | dt: 20673.2593ms | tok/sec: 19.0205\n",
      "step 2208 | train loss: 4.03 | val loss: 3.51 | perplexity: 33.51 | lr: 1.29e-04 | norm: 0.3694 | dt: 20613.9979ms | tok/sec: 19.0752\n",
      "step 2209 | train loss: 3.37 | val loss: 3.51 | perplexity: 33.51 | lr: 1.28e-04 | norm: 0.3224 | dt: 20707.1109ms | tok/sec: 18.9894\n",
      "step 2210 | train loss: 3.54 | val loss: 3.51 | perplexity: 33.53 | lr: 1.28e-04 | norm: 0.2903 | dt: 20748.6689ms | tok/sec: 18.9514\n",
      "step 2211 | train loss: 3.53 | val loss: 3.51 | perplexity: 33.50 | lr: 1.28e-04 | norm: 0.2904 | dt: 20751.4751ms | tok/sec: 18.9488\n",
      "step 2212 | train loss: 3.63 | val loss: 3.51 | perplexity: 33.46 | lr: 1.27e-04 | norm: 0.2571 | dt: 20590.9021ms | tok/sec: 19.0966\n",
      "step 2213 | train loss: 3.60 | val loss: 3.51 | perplexity: 33.41 | lr: 1.27e-04 | norm: 0.2725 | dt: 20762.7594ms | tok/sec: 18.9385\n",
      "step 2214 | train loss: 3.54 | val loss: 3.51 | perplexity: 33.41 | lr: 1.27e-04 | norm: 0.3303 | dt: 20619.0209ms | tok/sec: 19.0705\n",
      "step 2215 | train loss: 3.51 | val loss: 3.51 | perplexity: 33.42 | lr: 1.27e-04 | norm: 0.2732 | dt: 20695.8230ms | tok/sec: 18.9998\n",
      "step 2216 | train loss: 3.69 | val loss: 3.51 | perplexity: 33.41 | lr: 1.26e-04 | norm: 0.2773 | dt: 20706.7406ms | tok/sec: 18.9898\n",
      "step 2217 | train loss: 3.42 | val loss: 3.51 | perplexity: 33.39 | lr: 1.26e-04 | norm: 0.2761 | dt: 20745.8844ms | tok/sec: 18.9539\n",
      "step 2218 | train loss: 3.54 | val loss: 3.51 | perplexity: 33.40 | lr: 1.26e-04 | norm: 0.2758 | dt: 20667.1257ms | tok/sec: 19.0262\n",
      "step 2219 | train loss: 3.66 | val loss: 3.51 | perplexity: 33.40 | lr: 1.25e-04 | norm: 0.2668 | dt: 20617.6810ms | tok/sec: 19.0718\n",
      "step 2220 | train loss: 3.74 | val loss: 3.51 | perplexity: 33.39 | lr: 1.25e-04 | norm: 0.3054 | dt: 20660.4784ms | tok/sec: 19.0323\n",
      "step 2221 | train loss: 3.45 | val loss: 3.51 | perplexity: 33.36 | lr: 1.25e-04 | norm: 0.2756 | dt: 20742.6207ms | tok/sec: 18.9569\n",
      "step 2222 | train loss: 3.53 | val loss: 3.51 | perplexity: 33.35 | lr: 1.25e-04 | norm: 0.2665 | dt: 20573.9570ms | tok/sec: 19.1123\n",
      "step 2223 | train loss: 3.55 | val loss: 3.51 | perplexity: 33.37 | lr: 1.24e-04 | norm: 0.3685 | dt: 20605.4349ms | tok/sec: 19.0831\n",
      "step 2224 | train loss: 3.63 | val loss: 3.51 | perplexity: 33.40 | lr: 1.24e-04 | norm: 0.2589 | dt: 20496.9511ms | tok/sec: 19.1841\n",
      "step 2225 | train loss: 3.56 | val loss: 3.51 | perplexity: 33.41 | lr: 1.24e-04 | norm: 0.2728 | dt: 20646.2190ms | tok/sec: 19.0454\n",
      "step 2226 | train loss: 3.88 | val loss: 3.51 | perplexity: 33.41 | lr: 1.23e-04 | norm: 0.2892 | dt: 21126.1692ms | tok/sec: 18.6127\n",
      "step 2227 | train loss: 3.38 | val loss: 3.51 | perplexity: 33.39 | lr: 1.23e-04 | norm: 0.2432 | dt: 20647.5964ms | tok/sec: 19.0442\n",
      "step 2228 | train loss: 3.44 | val loss: 3.51 | perplexity: 33.36 | lr: 1.23e-04 | norm: 0.3071 | dt: 20732.5218ms | tok/sec: 18.9661\n",
      "step 2229 | train loss: 3.87 | val loss: 3.51 | perplexity: 33.34 | lr: 1.23e-04 | norm: 0.3016 | dt: 20721.7612ms | tok/sec: 18.9760\n",
      "step 2230 | train loss: 3.88 | val loss: 3.51 | perplexity: 33.32 | lr: 1.22e-04 | norm: 0.3410 | dt: 20718.3063ms | tok/sec: 18.9792\n",
      "step 2231 | train loss: 3.59 | val loss: 3.51 | perplexity: 33.32 | lr: 1.22e-04 | norm: 0.2771 | dt: 20488.9202ms | tok/sec: 19.1916\n",
      "step 2232 | train loss: 3.64 | val loss: 3.51 | perplexity: 33.36 | lr: 1.22e-04 | norm: 0.2545 | dt: 20615.8867ms | tok/sec: 19.0734\n",
      "step 2233 | train loss: 3.60 | val loss: 3.51 | perplexity: 33.40 | lr: 1.22e-04 | norm: 0.2822 | dt: 20671.7296ms | tok/sec: 19.0219\n",
      "step 2234 | train loss: 3.55 | val loss: 3.51 | perplexity: 33.42 | lr: 1.21e-04 | norm: 0.3076 | dt: 20592.1202ms | tok/sec: 19.0955\n",
      "step 2235 | train loss: 3.81 | val loss: 3.51 | perplexity: 33.40 | lr: 1.21e-04 | norm: 0.3077 | dt: 20628.7920ms | tok/sec: 19.0615\n",
      "step 2236 | train loss: 3.50 | val loss: 3.51 | perplexity: 33.40 | lr: 1.21e-04 | norm: 0.2710 | dt: 20728.0092ms | tok/sec: 18.9703\n",
      "step 2237 | train loss: 3.70 | val loss: 3.51 | perplexity: 33.40 | lr: 1.21e-04 | norm: 0.3385 | dt: 20658.3722ms | tok/sec: 19.0342\n",
      "step 2238 | train loss: 3.75 | val loss: 3.51 | perplexity: 33.46 | lr: 1.20e-04 | norm: 0.3838 | dt: 20706.3992ms | tok/sec: 18.9901\n",
      "step 2239 | train loss: 3.66 | val loss: 3.51 | perplexity: 33.49 | lr: 1.20e-04 | norm: 0.2930 | dt: 20770.4546ms | tok/sec: 18.9315\n",
      "step 2240 | train loss: 3.77 | val loss: 3.51 | perplexity: 33.46 | lr: 1.20e-04 | norm: 0.2827 | dt: 20592.6623ms | tok/sec: 19.0950\n",
      "step 2241 | train loss: 3.82 | val loss: 3.51 | perplexity: 33.43 | lr: 1.20e-04 | norm: 0.3556 | dt: 20591.7060ms | tok/sec: 19.0958\n",
      "step 2242 | train loss: 3.55 | val loss: 3.51 | perplexity: 33.43 | lr: 1.19e-04 | norm: 0.2680 | dt: 20763.8032ms | tok/sec: 18.9376\n",
      "step 2243 | train loss: 3.66 | val loss: 3.51 | perplexity: 33.44 | lr: 1.19e-04 | norm: 0.2875 | dt: 20900.8234ms | tok/sec: 18.8134\n",
      "step 2244 | train loss: 3.81 | val loss: 3.51 | perplexity: 33.43 | lr: 1.19e-04 | norm: 0.2671 | dt: 20666.2571ms | tok/sec: 19.0270\n",
      "step 2245 | train loss: 3.71 | val loss: 3.51 | perplexity: 33.44 | lr: 1.19e-04 | norm: 0.2754 | dt: 20661.1385ms | tok/sec: 19.0317\n",
      "step 2246 | train loss: 3.61 | val loss: 3.51 | perplexity: 33.44 | lr: 1.18e-04 | norm: 0.2497 | dt: 20651.9649ms | tok/sec: 19.0401\n",
      "step 2247 | train loss: 3.83 | val loss: 3.51 | perplexity: 33.40 | lr: 1.18e-04 | norm: 0.3260 | dt: 20909.7815ms | tok/sec: 18.8054\n",
      "step 2248 | train loss: 4.22 | val loss: 3.51 | perplexity: 33.39 | lr: 1.18e-04 | norm: 0.7338 | dt: 20648.3881ms | tok/sec: 19.0434\n",
      "step 2249 | train loss: 3.89 | val loss: 3.51 | perplexity: 33.38 | lr: 1.18e-04 | norm: 0.2997 | dt: 20758.4078ms | tok/sec: 18.9425\n",
      "step 2250 | train loss: 3.69 | val loss: 3.51 | perplexity: 33.36 | lr: 1.17e-04 | norm: 0.2803 | dt: 20637.5422ms | tok/sec: 19.0534\n",
      "step 2251 | train loss: 3.62 | val loss: 3.51 | perplexity: 33.36 | lr: 1.17e-04 | norm: 0.2835 | dt: 20842.9995ms | tok/sec: 18.8656\n",
      "step 2252 | train loss: 3.66 | val loss: 3.51 | perplexity: 33.37 | lr: 1.17e-04 | norm: 0.2815 | dt: 20750.9897ms | tok/sec: 18.9493\n",
      "step 2253 | train loss: 3.40 | val loss: 3.51 | perplexity: 33.37 | lr: 1.17e-04 | norm: 0.3448 | dt: 20619.7255ms | tok/sec: 19.0699\n",
      "step 2254 | train loss: 3.52 | val loss: 3.51 | perplexity: 33.36 | lr: 1.17e-04 | norm: 0.2796 | dt: 20642.5455ms | tok/sec: 19.0488\n",
      "step 2255 | train loss: 3.49 | val loss: 3.51 | perplexity: 33.33 | lr: 1.16e-04 | norm: 0.2893 | dt: 20586.9403ms | tok/sec: 19.1003\n",
      "step 2256 | train loss: 3.21 | val loss: 3.51 | perplexity: 33.29 | lr: 1.16e-04 | norm: 0.2902 | dt: 20696.2273ms | tok/sec: 18.9994\n",
      "step 2257 | train loss: 3.88 | val loss: 3.50 | perplexity: 33.25 | lr: 1.16e-04 | norm: 0.3755 | dt: 20688.7701ms | tok/sec: 19.0063\n",
      "step 2258 | train loss: 3.27 | val loss: 3.50 | perplexity: 33.23 | lr: 1.16e-04 | norm: 0.2861 | dt: 20637.1841ms | tok/sec: 19.0538\n",
      "step 2259 | train loss: 3.39 | val loss: 3.50 | perplexity: 33.23 | lr: 1.15e-04 | norm: 0.2775 | dt: 20694.1037ms | tok/sec: 19.0014\n",
      "step 2260 | train loss: 3.52 | val loss: 3.50 | perplexity: 33.26 | lr: 1.15e-04 | norm: 0.3614 | dt: 20697.2299ms | tok/sec: 18.9985\n",
      "step 2261 | train loss: 4.22 | val loss: 3.51 | perplexity: 33.37 | lr: 1.15e-04 | norm: 0.5817 | dt: 20608.8655ms | tok/sec: 19.0799\n",
      "step 2262 | train loss: 3.31 | val loss: 3.51 | perplexity: 33.48 | lr: 1.15e-04 | norm: 0.3204 | dt: 20738.6479ms | tok/sec: 18.9605\n",
      "step 2263 | train loss: 3.57 | val loss: 3.51 | perplexity: 33.51 | lr: 1.15e-04 | norm: 0.4520 | dt: 20588.4774ms | tok/sec: 19.0988\n",
      "step 2264 | train loss: 3.54 | val loss: 3.51 | perplexity: 33.47 | lr: 1.14e-04 | norm: 0.4098 | dt: 20671.1128ms | tok/sec: 19.0225\n",
      "step 2265 | train loss: 3.55 | val loss: 3.51 | perplexity: 33.41 | lr: 1.14e-04 | norm: 0.3537 | dt: 20602.3266ms | tok/sec: 19.0860\n",
      "step 2266 | train loss: 3.70 | val loss: 3.51 | perplexity: 33.38 | lr: 1.14e-04 | norm: 0.3515 | dt: 20698.3812ms | tok/sec: 18.9974\n",
      "step 2267 | train loss: 3.75 | val loss: 3.51 | perplexity: 33.32 | lr: 1.14e-04 | norm: 0.3731 | dt: 20771.5571ms | tok/sec: 18.9305\n",
      "step 2268 | train loss: 3.56 | val loss: 3.51 | perplexity: 33.30 | lr: 1.14e-04 | norm: 0.2929 | dt: 20530.3965ms | tok/sec: 19.1529\n",
      "step 2269 | train loss: 3.88 | val loss: 3.51 | perplexity: 33.31 | lr: 1.13e-04 | norm: 0.3359 | dt: 20731.6699ms | tok/sec: 18.9669\n",
      "step 2270 | train loss: 3.79 | val loss: 3.51 | perplexity: 33.33 | lr: 1.13e-04 | norm: 0.3440 | dt: 20736.9735ms | tok/sec: 18.9621\n",
      "step 2271 | train loss: 3.93 | val loss: 3.51 | perplexity: 33.35 | lr: 1.13e-04 | norm: 0.4075 | dt: 20629.8451ms | tok/sec: 19.0605\n",
      "step 2272 | train loss: 3.54 | val loss: 3.51 | perplexity: 33.35 | lr: 1.13e-04 | norm: 0.4213 | dt: 20662.9834ms | tok/sec: 19.0300\n",
      "step 2273 | train loss: 3.58 | val loss: 3.51 | perplexity: 33.33 | lr: 1.13e-04 | norm: 0.3240 | dt: 20777.7376ms | tok/sec: 18.9249\n",
      "step 2274 | train loss: 3.40 | val loss: 3.51 | perplexity: 33.33 | lr: 1.12e-04 | norm: 0.3550 | dt: 20658.9303ms | tok/sec: 19.0337\n",
      "step 2275 | train loss: 3.39 | val loss: 3.51 | perplexity: 33.35 | lr: 1.12e-04 | norm: 0.2717 | dt: 20520.6590ms | tok/sec: 19.1620\n",
      "step 2276 | train loss: 3.87 | val loss: 3.51 | perplexity: 33.37 | lr: 1.12e-04 | norm: 0.3522 | dt: 20635.9129ms | tok/sec: 19.0549\n",
      "step 2277 | train loss: 3.55 | val loss: 3.51 | perplexity: 33.37 | lr: 1.12e-04 | norm: 0.2934 | dt: 20787.4455ms | tok/sec: 18.9160\n",
      "step 2278 | train loss: 3.45 | val loss: 3.51 | perplexity: 33.36 | lr: 1.12e-04 | norm: 0.2925 | dt: 19203.6507ms | tok/sec: 20.4761\n",
      "step 2279 | train loss: 3.64 | val loss: 3.51 | perplexity: 33.32 | lr: 1.11e-04 | norm: 0.2863 | dt: 18057.2295ms | tok/sec: 21.7761\n",
      "step 2280 | train loss: 3.65 | val loss: 3.50 | perplexity: 33.27 | lr: 1.11e-04 | norm: 0.2912 | dt: 17916.3964ms | tok/sec: 21.9473\n",
      "step 2281 | train loss: 3.55 | val loss: 3.50 | perplexity: 33.24 | lr: 1.11e-04 | norm: 0.2837 | dt: 17972.5428ms | tok/sec: 21.8787\n",
      "step 2282 | train loss: 3.72 | val loss: 3.50 | perplexity: 33.22 | lr: 1.11e-04 | norm: 0.2911 | dt: 17906.2667ms | tok/sec: 21.9597\n",
      "step 2283 | train loss: 3.67 | val loss: 3.50 | perplexity: 33.22 | lr: 1.11e-04 | norm: 0.2749 | dt: 17885.1039ms | tok/sec: 21.9857\n",
      "step 2284 | train loss: 3.79 | val loss: 3.50 | perplexity: 33.21 | lr: 1.10e-04 | norm: 0.2673 | dt: 18103.5895ms | tok/sec: 21.7203\n",
      "step 2285 | train loss: 3.45 | val loss: 3.50 | perplexity: 33.20 | lr: 1.10e-04 | norm: 0.2738 | dt: 18093.0870ms | tok/sec: 21.7329\n",
      "step 2286 | train loss: 3.90 | val loss: 3.50 | perplexity: 33.21 | lr: 1.10e-04 | norm: 0.3642 | dt: 17877.5144ms | tok/sec: 21.9950\n",
      "step 2287 | train loss: 3.61 | val loss: 3.50 | perplexity: 33.23 | lr: 1.10e-04 | norm: 0.2792 | dt: 18029.5675ms | tok/sec: 21.8095\n",
      "step 2288 | train loss: 3.80 | val loss: 3.50 | perplexity: 33.24 | lr: 1.10e-04 | norm: 0.3006 | dt: 17917.0446ms | tok/sec: 21.9465\n",
      "step 2289 | train loss: 3.57 | val loss: 3.50 | perplexity: 33.24 | lr: 1.10e-04 | norm: 0.2884 | dt: 17946.1062ms | tok/sec: 21.9109\n",
      "step 2290 | train loss: 3.78 | val loss: 3.50 | perplexity: 33.25 | lr: 1.09e-04 | norm: 0.3027 | dt: 17948.0729ms | tok/sec: 21.9085\n",
      "step 2291 | train loss: 3.41 | val loss: 3.50 | perplexity: 33.27 | lr: 1.09e-04 | norm: 0.2521 | dt: 17929.6014ms | tok/sec: 21.9311\n",
      "step 2292 | train loss: 3.70 | val loss: 3.51 | perplexity: 33.30 | lr: 1.09e-04 | norm: 0.3208 | dt: 17901.2544ms | tok/sec: 21.9658\n",
      "step 2293 | train loss: 3.52 | val loss: 3.51 | perplexity: 33.28 | lr: 1.09e-04 | norm: 0.2969 | dt: 17867.4586ms | tok/sec: 22.0074\n",
      "step 2294 | train loss: 3.59 | val loss: 3.50 | perplexity: 33.23 | lr: 1.09e-04 | norm: 0.3180 | dt: 17901.7317ms | tok/sec: 21.9652\n",
      "step 2295 | train loss: 3.82 | val loss: 3.50 | perplexity: 33.19 | lr: 1.09e-04 | norm: 0.2659 | dt: 17955.2155ms | tok/sec: 21.8998\n",
      "step 2296 | train loss: 3.72 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.4214 | dt: 17938.2019ms | tok/sec: 21.9206\n",
      "step 2297 | train loss: 3.60 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.2854 | dt: 17895.5786ms | tok/sec: 21.9728\n",
      "step 2298 | train loss: 3.52 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.2803 | dt: 17913.3883ms | tok/sec: 21.9510\n",
      "step 2299 | train loss: 3.94 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.3078 | dt: 17912.7038ms | tok/sec: 21.9518\n",
      "step 2300 | train loss: 3.58 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.2815 | dt: 17934.5713ms | tok/sec: 21.9250\n",
      "step 2301 | train loss: 3.33 | val loss: 3.50 | perplexity: 33.20 | lr: 1.08e-04 | norm: 0.3170 | dt: 17890.7816ms | tok/sec: 21.9787\n",
      "step 2302 | train loss: 4.31 | val loss: 3.50 | perplexity: 33.26 | lr: 1.07e-04 | norm: 1.0717 | dt: 17961.1087ms | tok/sec: 21.8926\n",
      "step 2303 | train loss: 3.84 | val loss: 3.51 | perplexity: 33.31 | lr: 1.07e-04 | norm: 0.3432 | dt: 17911.4666ms | tok/sec: 21.9533\n",
      "step 2304 | train loss: 3.58 | val loss: 3.51 | perplexity: 33.31 | lr: 1.07e-04 | norm: 0.2753 | dt: 17955.7834ms | tok/sec: 21.8991\n",
      "step 2305 | train loss: 3.61 | val loss: 3.50 | perplexity: 33.28 | lr: 1.07e-04 | norm: 0.2931 | dt: 17920.3882ms | tok/sec: 21.9424\n",
      "step 2306 | train loss: 3.49 | val loss: 3.50 | perplexity: 33.25 | lr: 1.07e-04 | norm: 0.3269 | dt: 17920.1796ms | tok/sec: 21.9426\n",
      "step 2307 | train loss: 3.56 | val loss: 3.50 | perplexity: 33.23 | lr: 1.07e-04 | norm: 0.3069 | dt: 17989.3067ms | tok/sec: 21.8583\n",
      "step 2308 | train loss: 3.74 | val loss: 3.50 | perplexity: 33.23 | lr: 1.07e-04 | norm: 0.2967 | dt: 17909.8859ms | tok/sec: 21.9552\n",
      "step 2309 | train loss: 3.76 | val loss: 3.50 | perplexity: 33.21 | lr: 1.06e-04 | norm: 0.2872 | dt: 18048.7287ms | tok/sec: 21.7864\n",
      "step 2310 | train loss: 3.75 | val loss: 3.50 | perplexity: 33.20 | lr: 1.06e-04 | norm: 0.4061 | dt: 17940.4087ms | tok/sec: 21.9179\n",
      "step 2311 | train loss: 3.51 | val loss: 3.50 | perplexity: 33.19 | lr: 1.06e-04 | norm: 0.3554 | dt: 17930.5642ms | tok/sec: 21.9299\n",
      "step 2312 | train loss: 3.82 | val loss: 3.50 | perplexity: 33.18 | lr: 1.06e-04 | norm: 0.3881 | dt: 17918.9930ms | tok/sec: 21.9441\n",
      "step 2313 | train loss: 3.36 | val loss: 3.50 | perplexity: 33.16 | lr: 1.06e-04 | norm: 0.3206 | dt: 17954.7462ms | tok/sec: 21.9004\n",
      "step 2314 | train loss: 3.55 | val loss: 3.50 | perplexity: 33.16 | lr: 1.06e-04 | norm: 0.2839 | dt: 17913.0638ms | tok/sec: 21.9514\n",
      "step 2315 | train loss: 3.69 | val loss: 3.50 | perplexity: 33.15 | lr: 1.06e-04 | norm: 0.3069 | dt: 17934.3171ms | tok/sec: 21.9253\n",
      "step 2316 | train loss: 3.85 | val loss: 3.50 | perplexity: 33.15 | lr: 1.06e-04 | norm: 0.3660 | dt: 18058.0485ms | tok/sec: 21.7751\n",
      "step 2317 | train loss: 3.73 | val loss: 3.50 | perplexity: 33.18 | lr: 1.05e-04 | norm: 0.3389 | dt: 17943.5036ms | tok/sec: 21.9141\n",
      "step 2318 | train loss: 3.99 | val loss: 3.50 | perplexity: 33.21 | lr: 1.05e-04 | norm: 0.3588 | dt: 17929.3864ms | tok/sec: 21.9314\n",
      "step 2319 | train loss: 3.60 | val loss: 3.50 | perplexity: 33.23 | lr: 1.05e-04 | norm: 0.3627 | dt: 17939.1959ms | tok/sec: 21.9194\n",
      "step 2320 | train loss: 3.36 | val loss: 3.50 | perplexity: 33.22 | lr: 1.05e-04 | norm: 0.2711 | dt: 17906.8916ms | tok/sec: 21.9589\n",
      "step 2321 | train loss: 3.60 | val loss: 3.50 | perplexity: 33.19 | lr: 1.05e-04 | norm: 0.2944 | dt: 17952.0922ms | tok/sec: 21.9036\n",
      "step 2322 | train loss: 4.04 | val loss: 3.50 | perplexity: 33.15 | lr: 1.05e-04 | norm: 0.3753 | dt: 17962.2777ms | tok/sec: 21.8912\n",
      "step 2323 | train loss: 3.74 | val loss: 3.50 | perplexity: 33.13 | lr: 1.05e-04 | norm: 0.2793 | dt: 17996.1586ms | tok/sec: 21.8500\n",
      "step 2324 | train loss: 3.76 | val loss: 3.50 | perplexity: 33.13 | lr: 1.05e-04 | norm: 0.2733 | dt: 17935.0815ms | tok/sec: 21.9244\n",
      "step 2325 | train loss: 3.53 | val loss: 3.50 | perplexity: 33.14 | lr: 1.04e-04 | norm: 0.2894 | dt: 17896.9500ms | tok/sec: 21.9711\n",
      "step 2326 | train loss: 3.54 | val loss: 3.50 | perplexity: 33.12 | lr: 1.04e-04 | norm: 0.3252 | dt: 18088.3787ms | tok/sec: 21.7386\n",
      "step 2327 | train loss: 3.83 | val loss: 3.50 | perplexity: 33.10 | lr: 1.04e-04 | norm: 0.3235 | dt: 17920.0156ms | tok/sec: 21.9428\n",
      "step 2328 | train loss: 3.79 | val loss: 3.50 | perplexity: 33.08 | lr: 1.04e-04 | norm: 0.2790 | dt: 17937.4092ms | tok/sec: 21.9216\n",
      "step 2329 | train loss: 3.45 | val loss: 3.50 | perplexity: 33.07 | lr: 1.04e-04 | norm: 0.2618 | dt: 17974.3261ms | tok/sec: 21.8765\n",
      "step 2330 | train loss: 3.59 | val loss: 3.50 | perplexity: 33.07 | lr: 1.04e-04 | norm: 0.2710 | dt: 18123.3342ms | tok/sec: 21.6967\n",
      "step 2331 | train loss: 3.41 | val loss: 3.50 | perplexity: 33.06 | lr: 1.04e-04 | norm: 0.2907 | dt: 17963.8920ms | tok/sec: 21.8892\n",
      "step 2332 | train loss: 3.62 | val loss: 3.50 | perplexity: 33.03 | lr: 1.04e-04 | norm: 0.3238 | dt: 17915.4651ms | tok/sec: 21.9484\n",
      "step 2333 | train loss: 3.57 | val loss: 3.50 | perplexity: 33.01 | lr: 1.04e-04 | norm: 0.2575 | dt: 17939.1184ms | tok/sec: 21.9195\n",
      "step 2334 | train loss: 3.28 | val loss: 3.50 | perplexity: 33.01 | lr: 1.03e-04 | norm: 0.2979 | dt: 17966.8570ms | tok/sec: 21.8856\n",
      "step 2335 | train loss: 3.74 | val loss: 3.50 | perplexity: 33.04 | lr: 1.03e-04 | norm: 0.3024 | dt: 18165.8890ms | tok/sec: 21.6458\n",
      "step 2336 | train loss: 3.57 | val loss: 3.50 | perplexity: 33.05 | lr: 1.03e-04 | norm: 0.2940 | dt: 17909.5538ms | tok/sec: 21.9557\n",
      "step 2337 | train loss: 3.39 | val loss: 3.50 | perplexity: 33.04 | lr: 1.03e-04 | norm: 0.2767 | dt: 17927.1326ms | tok/sec: 21.9341\n",
      "step 2338 | train loss: 3.72 | val loss: 3.50 | perplexity: 33.03 | lr: 1.03e-04 | norm: 0.3164 | dt: 17933.0466ms | tok/sec: 21.9269\n",
      "step 2339 | train loss: 3.31 | val loss: 3.50 | perplexity: 33.03 | lr: 1.03e-04 | norm: 0.3159 | dt: 17890.0595ms | tok/sec: 21.9796\n",
      "step 2340 | train loss: 3.43 | val loss: 3.50 | perplexity: 33.03 | lr: 1.03e-04 | norm: 0.3006 | dt: 17885.3061ms | tok/sec: 21.9854\n",
      "step 2341 | train loss: 3.54 | val loss: 3.50 | perplexity: 33.04 | lr: 1.03e-04 | norm: 0.3039 | dt: 17968.5967ms | tok/sec: 21.8835\n",
      "step 2342 | train loss: 3.47 | val loss: 3.50 | perplexity: 33.03 | lr: 1.03e-04 | norm: 0.2665 | dt: 17990.5925ms | tok/sec: 21.8568\n",
      "step 2343 | train loss: 3.47 | val loss: 3.50 | perplexity: 33.00 | lr: 1.03e-04 | norm: 0.3525 | dt: 18226.4628ms | tok/sec: 21.5739\n",
      "step 2344 | train loss: 3.69 | val loss: 3.50 | perplexity: 32.98 | lr: 1.02e-04 | norm: 0.2692 | dt: 17912.8489ms | tok/sec: 21.9516\n",
      "step 2345 | train loss: 3.44 | val loss: 3.50 | perplexity: 33.00 | lr: 1.02e-04 | norm: 0.2933 | dt: 18027.2138ms | tok/sec: 21.8124\n",
      "step 2346 | train loss: 3.51 | val loss: 3.50 | perplexity: 33.06 | lr: 1.02e-04 | norm: 0.2711 | dt: 17954.9496ms | tok/sec: 21.9001\n",
      "step 2347 | train loss: 3.63 | val loss: 3.50 | perplexity: 33.11 | lr: 1.02e-04 | norm: 0.3221 | dt: 17944.5696ms | tok/sec: 21.9128\n",
      "step 2348 | train loss: 3.37 | val loss: 3.50 | perplexity: 33.12 | lr: 1.02e-04 | norm: 0.2521 | dt: 17913.3422ms | tok/sec: 21.9510\n",
      "step 2349 | train loss: 3.54 | val loss: 3.50 | perplexity: 33.11 | lr: 1.02e-04 | norm: 0.2630 | dt: 17925.7584ms | tok/sec: 21.9358\n",
      "step 2350 | train loss: 3.56 | val loss: 3.50 | perplexity: 33.06 | lr: 1.02e-04 | norm: 0.2749 | dt: 17953.4671ms | tok/sec: 21.9020\n",
      "step 2351 | train loss: 3.41 | val loss: 3.50 | perplexity: 33.03 | lr: 1.02e-04 | norm: 0.2587 | dt: 17988.3108ms | tok/sec: 21.8595\n",
      "step 2352 | train loss: 3.29 | val loss: 3.50 | perplexity: 33.02 | lr: 1.02e-04 | norm: 0.3078 | dt: 17955.6353ms | tok/sec: 21.8993\n",
      "step 2353 | train loss: 3.59 | val loss: 3.50 | perplexity: 32.99 | lr: 1.02e-04 | norm: 0.2677 | dt: 17922.3342ms | tok/sec: 21.9400\n",
      "step 2354 | train loss: 3.66 | val loss: 3.50 | perplexity: 32.96 | lr: 1.02e-04 | norm: 0.2929 | dt: 17970.8645ms | tok/sec: 21.8808\n",
      "step 2355 | train loss: 3.30 | val loss: 3.49 | perplexity: 32.95 | lr: 1.02e-04 | norm: 0.2736 | dt: 17962.7697ms | tok/sec: 21.8906\n",
      "step 2356 | train loss: 3.56 | val loss: 3.49 | perplexity: 32.94 | lr: 1.02e-04 | norm: 0.2759 | dt: 17932.9200ms | tok/sec: 21.9270\n",
      "step 2357 | train loss: 3.54 | val loss: 3.49 | perplexity: 32.91 | lr: 1.01e-04 | norm: 0.2835 | dt: 17899.6210ms | tok/sec: 21.9678\n",
      "step 2358 | train loss: 3.41 | val loss: 3.49 | perplexity: 32.89 | lr: 1.01e-04 | norm: 0.2558 | dt: 17916.4500ms | tok/sec: 21.9472\n",
      "step 2359 | train loss: 3.48 | val loss: 3.49 | perplexity: 32.87 | lr: 1.01e-04 | norm: 0.2581 | dt: 17942.0874ms | tok/sec: 21.9158\n",
      "step 2360 | train loss: 3.69 | val loss: 3.49 | perplexity: 32.86 | lr: 1.01e-04 | norm: 0.3105 | dt: 17929.5876ms | tok/sec: 21.9311\n",
      "step 2361 | train loss: 3.74 | val loss: 3.49 | perplexity: 32.86 | lr: 1.01e-04 | norm: 0.3617 | dt: 17911.1192ms | tok/sec: 21.9537\n",
      "step 2362 | train loss: 3.89 | val loss: 3.49 | perplexity: 32.91 | lr: 1.01e-04 | norm: 0.3186 | dt: 17934.5374ms | tok/sec: 21.9251\n",
      "step 2363 | train loss: 3.56 | val loss: 3.49 | perplexity: 32.94 | lr: 1.01e-04 | norm: 0.3059 | dt: 17930.6457ms | tok/sec: 21.9298\n",
      "step 2364 | train loss: 3.39 | val loss: 3.50 | perplexity: 32.96 | lr: 1.01e-04 | norm: 0.2948 | dt: 17882.1614ms | tok/sec: 21.9893\n",
      "step 2365 | train loss: 3.48 | val loss: 3.49 | perplexity: 32.94 | lr: 1.01e-04 | norm: 0.3048 | dt: 17947.0406ms | tok/sec: 21.9098\n",
      "step 2366 | train loss: 3.42 | val loss: 3.49 | perplexity: 32.94 | lr: 1.01e-04 | norm: 0.3200 | dt: 17919.8546ms | tok/sec: 21.9430\n",
      "step 2367 | train loss: 3.22 | val loss: 3.50 | perplexity: 32.95 | lr: 1.01e-04 | norm: 0.2533 | dt: 17864.0838ms | tok/sec: 22.0115\n",
      "step 2368 | train loss: 3.62 | val loss: 3.49 | perplexity: 32.93 | lr: 1.01e-04 | norm: 0.3048 | dt: 18437.8791ms | tok/sec: 21.3265\n",
      "step 2369 | train loss: 3.91 | val loss: 3.49 | perplexity: 32.91 | lr: 1.01e-04 | norm: 0.3632 | dt: 17949.5132ms | tok/sec: 21.9068\n",
      "step 2370 | train loss: 3.68 | val loss: 3.49 | perplexity: 32.91 | lr: 1.01e-04 | norm: 0.2986 | dt: 17952.4441ms | tok/sec: 21.9032\n",
      "step 2371 | train loss: 3.66 | val loss: 3.49 | perplexity: 32.92 | lr: 1.01e-04 | norm: 0.3375 | dt: 17962.0774ms | tok/sec: 21.8915\n",
      "step 2372 | train loss: 3.56 | val loss: 3.49 | perplexity: 32.94 | lr: 1.01e-04 | norm: 0.2603 | dt: 17947.7470ms | tok/sec: 21.9089\n",
      "step 2373 | train loss: 3.61 | val loss: 3.49 | perplexity: 32.92 | lr: 1.01e-04 | norm: 0.2768 | dt: 18083.9179ms | tok/sec: 21.7440\n",
      "step 2374 | train loss: 3.58 | val loss: 3.49 | perplexity: 32.90 | lr: 1.01e-04 | norm: 0.6043 | dt: 17943.4888ms | tok/sec: 21.9141\n",
      "step 2375 | train loss: 3.76 | val loss: 3.49 | perplexity: 32.88 | lr: 1.00e-04 | norm: 0.2805 | dt: 17932.3275ms | tok/sec: 21.9278\n",
      "step 2376 | train loss: 3.66 | val loss: 3.49 | perplexity: 32.89 | lr: 1.00e-04 | norm: 0.2940 | dt: 17961.5216ms | tok/sec: 21.8921\n",
      "step 2377 | train loss: 3.80 | val loss: 3.49 | perplexity: 32.94 | lr: 1.00e-04 | norm: 0.2942 | dt: 18094.0831ms | tok/sec: 21.7317\n",
      "step 2378 | train loss: 3.64 | val loss: 3.50 | perplexity: 32.97 | lr: 1.00e-04 | norm: 0.3925 | dt: 17966.5349ms | tok/sec: 21.8860\n",
      "step 2379 | train loss: 3.37 | val loss: 3.50 | perplexity: 32.99 | lr: 1.00e-04 | norm: 0.2721 | dt: 18141.0060ms | tok/sec: 21.6755\n",
      "step 2380 | train loss: 3.80 | val loss: 3.50 | perplexity: 32.99 | lr: 1.00e-04 | norm: 0.3035 | dt: 18023.2604ms | tok/sec: 21.8171\n",
      "step 2381 | train loss: 3.57 | val loss: 3.50 | perplexity: 32.97 | lr: 1.00e-04 | norm: 0.2538 | dt: 17894.4194ms | tok/sec: 21.9742\n",
      "step 2382 | train loss: 3.34 | val loss: 3.49 | perplexity: 32.92 | lr: 1.00e-04 | norm: 0.2999 | dt: 17987.6878ms | tok/sec: 21.8603\n",
      "step 2383 | train loss: 3.46 | val loss: 3.49 | perplexity: 32.88 | lr: 1.00e-04 | norm: 0.2533 | dt: 17938.5464ms | tok/sec: 21.9202\n",
      "step 2384 | train loss: 3.38 | val loss: 3.49 | perplexity: 32.84 | lr: 1.00e-04 | norm: 0.2824 | dt: 17933.4643ms | tok/sec: 21.9264\n",
      "step 2385 | train loss: 3.62 | val loss: 3.49 | perplexity: 32.81 | lr: 1.00e-04 | norm: 0.2979 | dt: 17918.5219ms | tok/sec: 21.9447\n",
      "step 2386 | train loss: 3.52 | val loss: 3.49 | perplexity: 32.79 | lr: 1.00e-04 | norm: 0.2503 | dt: 17937.8335ms | tok/sec: 21.9210\n",
      "step 2387 | train loss: 3.30 | val loss: 3.49 | perplexity: 32.81 | lr: 1.00e-04 | norm: 0.2502 | dt: 17981.1769ms | tok/sec: 21.8682\n",
      "step 2388 | train loss: 3.47 | val loss: 3.49 | perplexity: 32.81 | lr: 1.00e-04 | norm: 0.3039 | dt: 17944.1357ms | tok/sec: 21.9133\n",
      "step 2389 | train loss: 3.49 | val loss: 3.49 | perplexity: 32.82 | lr: 1.00e-04 | norm: 0.2890 | dt: 17886.8711ms | tok/sec: 21.9835\n",
      "step 2390 | train loss: 3.36 | val loss: 3.49 | perplexity: 32.84 | lr: 1.00e-04 | norm: 0.2514 | dt: 17870.2121ms | tok/sec: 22.0040\n",
      "step 2391 | train loss: 3.60 | val loss: 3.49 | perplexity: 32.87 | lr: 1.00e-04 | norm: 0.3198 | dt: 18187.5160ms | tok/sec: 21.6201\n",
      "step 2392 | train loss: 3.40 | val loss: 3.49 | perplexity: 32.89 | lr: 1.00e-04 | norm: 0.2549 | dt: 17960.8896ms | tok/sec: 21.8929\n",
      "step 2393 | train loss: 3.82 | val loss: 3.49 | perplexity: 32.89 | lr: 1.00e-04 | norm: 0.3015 | dt: 17894.4101ms | tok/sec: 21.9742\n",
      "step 2394 | train loss: 3.65 | val loss: 3.49 | perplexity: 32.86 | lr: 1.00e-04 | norm: 0.3377 | dt: 18096.0913ms | tok/sec: 21.7293\n",
      "step 2395 | train loss: 3.55 | val loss: 3.49 | perplexity: 32.83 | lr: 1.00e-04 | norm: 0.2590 | dt: 17893.0395ms | tok/sec: 21.9759\n",
      "step 2396 | train loss: 3.71 | val loss: 3.49 | perplexity: 32.83 | lr: 1.00e-04 | norm: 0.2924 | dt: 17952.0118ms | tok/sec: 21.9037\n",
      "step 2397 | train loss: 3.63 | val loss: 3.49 | perplexity: 32.85 | lr: 1.00e-04 | norm: 0.3189 | dt: 17922.1346ms | tok/sec: 21.9402\n",
      "step 2398 | train loss: 3.52 | val loss: 3.49 | perplexity: 32.85 | lr: 1.00e-04 | norm: 0.3038 | dt: 17888.9859ms | tok/sec: 21.9809\n",
      "step 2399 | train loss: 3.33 | val loss: 3.49 | perplexity: 32.84 | lr: 1.00e-04 | norm: 0.2790 | dt: 17945.5698ms | tok/sec: 21.9116\n",
      "[Text Eval] samples=60 BLEU=0.01 ROUGE-L=0.0902 SELF-BLEU=0.82 REP=0.8719 D1=0.0846 D2=0.1147\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, token_encoder, EVAL_FREQ, grad_accum_steps, device,master_process, logpath)\n",
    "history,evaluation = trainer.train(MAX_STEPS, WARMUP_STEPS, MAX_LR, MIN_LR)\n",
    "dt = (time.time() - start_time) / (60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b4f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 13.7442hr\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training time: {dt:.4f}hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b89f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4965cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(evaluation, f, indent=4)\n",
    "\n",
    "with open('training_history.json','w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
