{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3058e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "from src.model import GPT,Config\n",
    "from src.trainer import Trainer\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b993b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = './log'\n",
    "DATASET_PATH = './data/gutenberg'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes, split='train'):\n",
    "        super().__init__()\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "        \n",
    "        # get the shard filenames\n",
    "        data_root = DATASET_PATH\n",
    "        shard_filenames = os.listdir(data_root)\n",
    "        shard_filenames = sorted([filename for filename in shard_filenames if split in filename])\n",
    "        self.shard_filepaths = [os.path.join(data_root, filename) for filename in shard_filenames]\n",
    "        assert len(self.shard_filepaths) > 0, f'no shards found for split {split}'\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f'found {len(self.shard_filepaths)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def load_tokens(self, filepath):\n",
    "        tokens = torch.tensor(np.load(filepath).astype(np.int32), dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard 0\n",
    "        self.curr_shard = 0\n",
    "        self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "        self.curr_pos = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        batch = self.tokens[self.curr_pos : self.curr_pos + B*T + 1]\n",
    "        x_batch = batch[:-1].view(B, T)\n",
    "        y_batch = batch[1:].view(B, T)\n",
    "        self.curr_pos += B * T * self.num_processes\n",
    "        if self.curr_pos + (B * T + 1) > len(self.tokens):\n",
    "            self.curr_shard = (self.curr_shard + 1) % len(self.shard_filepaths)\n",
    "            self.tokens = self.load_tokens(self.shard_filepaths[self.curr_shard])\n",
    "            self.curr_pos = self.B * self.T * self.process_rank\n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a831a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add76cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 6\n",
    "CTX_LENGTH = 2048\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 10\n",
    "EMBED_DIM = 768\n",
    "WEIGHT_DECAY =0.1\n",
    "MAX_LR = 1e-3\n",
    "MIN_LR = 1e-4\n",
    "EVAL_FREQ = 1\n",
    "MAX_STEPS = 1000\n",
    "WARMUP_STEPS = 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbf5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf24d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='train')\n",
    "val_loader = DataLoaderLite(B=MINI_BATCH_SIZE, T=CTX_LENGTH, process_rank=0, num_processes=1, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7461c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 111,086,592\n",
      "num decay parameter tensors: 42 with 110,985,216 parameters\n",
      "num nodecay parameter tensors: 82 with 101,376 parameters\n",
      "using fused AdamW optimizer: True\n"
     ]
    }
   ],
   "source": [
    "gpt_config = Config(vocab_size=50304,  # number of tokens: 50000 BPE merges + 256 bytes tokens + 1 <endoftext> token = 50257, \n",
    "                    # 50304 (nice number, lots of power of 2s) used instead of 50257 (bad, odd number)\n",
    "                           context_length=CTX_LENGTH, \n",
    "                           num_layers=NUM_LAYERS, \n",
    "                           num_heads=NUM_HEADS, \n",
    "                           embedding_dim=EMBED_DIM\n",
    "                           )\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of trainable parameters: {total_params:,}')\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "optimizer = model.configure_optimizer(weight_decay=WEIGHT_DECAY,lr=MAX_LR,device_type=device_type,master_process=master_process)\n",
    "token_encoder = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f51484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | train loss: 10.98 | val loss: 10.91 | perplexity: 54657.88 | lr: 1.40e-06 | norm: 16.4291 | dt: 11500.3209ms | tok/sec: 34.1917\n",
      "step    1 | train loss: 10.91 | val loss: 10.74 | perplexity: 46384.05 | lr: 2.80e-06 | norm: 16.9148 | dt: 11643.7054ms | tok/sec: 33.7707\n",
      "step    2 | train loss: 10.76 | val loss: 10.53 | perplexity: 37585.60 | lr: 4.20e-06 | norm: 14.8329 | dt: 11632.7412ms | tok/sec: 33.8025\n",
      "step    3 | train loss: 10.55 | val loss: 10.32 | perplexity: 30361.78 | lr: 5.59e-06 | norm: 13.3457 | dt: 11680.6197ms | tok/sec: 33.6640\n",
      "step    4 | train loss: 10.34 | val loss: 10.13 | perplexity: 25108.92 | lr: 6.99e-06 | norm: 10.5061 | dt: 11663.3134ms | tok/sec: 33.7139\n",
      "step    5 | train loss: 10.18 | val loss: 9.97 | perplexity: 21358.39 | lr: 8.39e-06 | norm: 8.1916 | dt: 11644.9687ms | tok/sec: 33.7670\n",
      "step    6 | train loss: 10.00 | val loss: 9.83 | perplexity: 18592.84 | lr: 9.79e-06 | norm: 7.2506 | dt: 11694.1407ms | tok/sec: 33.6250\n",
      "step    7 | train loss: 9.93 | val loss: 9.72 | perplexity: 16723.96 | lr: 1.12e-05 | norm: 6.4305 | dt: 11656.6298ms | tok/sec: 33.7332\n",
      "step    8 | train loss: 9.75 | val loss: 9.64 | perplexity: 15405.63 | lr: 1.26e-05 | norm: 5.5585 | dt: 11698.1606ms | tok/sec: 33.6135\n",
      "step    9 | train loss: 9.66 | val loss: 9.58 | perplexity: 14464.13 | lr: 1.40e-05 | norm: 4.9994 | dt: 11641.6759ms | tok/sec: 33.7766\n",
      "step   10 | train loss: 9.53 | val loss: 9.52 | perplexity: 13590.77 | lr: 1.54e-05 | norm: 4.2220 | dt: 11697.3016ms | tok/sec: 33.6160\n",
      "step   11 | train loss: 9.54 | val loss: 9.48 | perplexity: 13087.34 | lr: 1.68e-05 | norm: 4.3084 | dt: 11648.3312ms | tok/sec: 33.7573\n",
      "step   12 | train loss: 9.72 | val loss: 9.44 | perplexity: 12547.91 | lr: 1.82e-05 | norm: 3.4555 | dt: 11630.7473ms | tok/sec: 33.8083\n",
      "step   13 | train loss: 9.28 | val loss: 9.39 | perplexity: 11954.23 | lr: 1.96e-05 | norm: 3.8560 | dt: 11644.0151ms | tok/sec: 33.7698\n",
      "step   14 | train loss: 9.39 | val loss: 9.34 | perplexity: 11336.75 | lr: 2.10e-05 | norm: 3.3596 | dt: 11666.1043ms | tok/sec: 33.7059\n",
      "step   15 | train loss: 9.35 | val loss: 9.29 | perplexity: 10814.43 | lr: 2.24e-05 | norm: 2.9290 | dt: 11656.8835ms | tok/sec: 33.7325\n",
      "step   16 | train loss: 9.27 | val loss: 9.25 | perplexity: 10454.98 | lr: 2.38e-05 | norm: 3.0834 | dt: 11656.1170ms | tok/sec: 33.7347\n",
      "step   17 | train loss: 9.27 | val loss: 9.23 | perplexity: 10150.71 | lr: 2.52e-05 | norm: 2.7511 | dt: 11683.2383ms | tok/sec: 33.6564\n",
      "step   18 | train loss: 9.12 | val loss: 9.20 | perplexity: 9861.72 | lr: 2.66e-05 | norm: 3.7137 | dt: 11647.6934ms | tok/sec: 33.7591\n",
      "step   19 | train loss: 9.22 | val loss: 9.15 | perplexity: 9431.96 | lr: 2.80e-05 | norm: 3.5005 | dt: 11644.5987ms | tok/sec: 33.7681\n",
      "step   20 | train loss: 9.15 | val loss: 9.09 | perplexity: 8829.65 | lr: 2.94e-05 | norm: 3.6030 | dt: 11639.4989ms | tok/sec: 33.7829\n",
      "step   21 | train loss: 9.02 | val loss: 9.03 | perplexity: 8338.37 | lr: 3.08e-05 | norm: 3.6425 | dt: 11696.7964ms | tok/sec: 33.6174\n",
      "step   22 | train loss: 8.92 | val loss: 8.98 | perplexity: 7963.79 | lr: 3.22e-05 | norm: 3.0243 | dt: 11680.4276ms | tok/sec: 33.6645\n",
      "step   23 | train loss: 9.02 | val loss: 8.94 | perplexity: 7605.82 | lr: 3.36e-05 | norm: 3.0120 | dt: 11618.1645ms | tok/sec: 33.8449\n",
      "step   24 | train loss: 8.99 | val loss: 8.88 | perplexity: 7199.39 | lr: 3.50e-05 | norm: 3.6216 | dt: 11667.0804ms | tok/sec: 33.7030\n",
      "step   25 | train loss: 8.92 | val loss: 8.83 | perplexity: 6868.63 | lr: 3.64e-05 | norm: 2.9658 | dt: 11693.4183ms | tok/sec: 33.6271\n",
      "step   26 | train loss: 8.89 | val loss: 8.78 | perplexity: 6531.69 | lr: 3.78e-05 | norm: 2.6252 | dt: 11653.2116ms | tok/sec: 33.7431\n",
      "step   27 | train loss: 8.72 | val loss: 8.73 | perplexity: 6208.79 | lr: 3.92e-05 | norm: 2.6148 | dt: 11692.0395ms | tok/sec: 33.6311\n",
      "step   28 | train loss: 8.65 | val loss: 8.68 | perplexity: 5870.00 | lr: 4.06e-05 | norm: 2.5836 | dt: 11664.1328ms | tok/sec: 33.7116\n",
      "step   29 | train loss: 8.62 | val loss: 8.62 | perplexity: 5556.32 | lr: 4.20e-05 | norm: 2.4636 | dt: 11688.8509ms | tok/sec: 33.6403\n",
      "step   30 | train loss: 8.15 | val loss: 8.57 | perplexity: 5286.36 | lr: 4.34e-05 | norm: 2.9890 | dt: 11695.2643ms | tok/sec: 33.6218\n",
      "step   31 | train loss: 8.25 | val loss: 8.54 | perplexity: 5094.95 | lr: 4.48e-05 | norm: 3.1951 | dt: 11678.4167ms | tok/sec: 33.6703\n",
      "step   32 | train loss: 8.49 | val loss: 8.50 | perplexity: 4906.84 | lr: 4.62e-05 | norm: 2.3844 | dt: 11682.4992ms | tok/sec: 33.6586\n",
      "step   33 | train loss: 8.67 | val loss: 8.45 | perplexity: 4691.47 | lr: 4.76e-05 | norm: 2.4822 | dt: 11675.1163ms | tok/sec: 33.6798\n",
      "step   34 | train loss: 8.58 | val loss: 8.40 | perplexity: 4453.96 | lr: 4.90e-05 | norm: 2.2966 | dt: 11667.8162ms | tok/sec: 33.7009\n",
      "step   35 | train loss: 8.13 | val loss: 8.34 | perplexity: 4206.83 | lr: 5.03e-05 | norm: 2.7148 | dt: 11703.4576ms | tok/sec: 33.5983\n",
      "step   36 | train loss: 8.36 | val loss: 8.29 | perplexity: 3971.26 | lr: 5.17e-05 | norm: 2.3137 | dt: 11668.4349ms | tok/sec: 33.6991\n",
      "step   37 | train loss: 8.03 | val loss: 8.23 | perplexity: 3764.37 | lr: 5.31e-05 | norm: 2.8508 | dt: 11655.2610ms | tok/sec: 33.7372\n",
      "step   38 | train loss: 8.25 | val loss: 8.18 | perplexity: 3580.85 | lr: 5.45e-05 | norm: 2.6211 | dt: 11695.0555ms | tok/sec: 33.6224\n",
      "step   39 | train loss: 8.04 | val loss: 8.13 | perplexity: 3409.97 | lr: 5.59e-05 | norm: 2.3817 | dt: 11653.9593ms | tok/sec: 33.7410\n",
      "step   40 | train loss: 7.81 | val loss: 8.08 | perplexity: 3222.85 | lr: 5.73e-05 | norm: 2.7341 | dt: 11700.8128ms | tok/sec: 33.6059\n",
      "step   41 | train loss: 7.73 | val loss: 8.04 | perplexity: 3105.91 | lr: 5.87e-05 | norm: 2.4028 | dt: 11657.7432ms | tok/sec: 33.7300\n",
      "step   42 | train loss: 7.58 | val loss: 8.01 | perplexity: 3019.53 | lr: 6.01e-05 | norm: 2.3060 | dt: 11696.0421ms | tok/sec: 33.6196\n",
      "step   43 | train loss: 7.89 | val loss: 7.94 | perplexity: 2819.98 | lr: 6.15e-05 | norm: 3.2615 | dt: 11679.4298ms | tok/sec: 33.6674\n",
      "step   44 | train loss: 7.64 | val loss: 7.88 | perplexity: 2652.46 | lr: 6.29e-05 | norm: 2.0626 | dt: 11656.5206ms | tok/sec: 33.7336\n",
      "step   45 | train loss: 7.69 | val loss: 7.83 | perplexity: 2521.21 | lr: 6.43e-05 | norm: 1.8754 | dt: 11693.4071ms | tok/sec: 33.6272\n",
      "step   46 | train loss: 7.44 | val loss: 7.78 | perplexity: 2392.28 | lr: 6.57e-05 | norm: 1.9673 | dt: 11676.5149ms | tok/sec: 33.6758\n",
      "step   47 | train loss: 7.49 | val loss: 7.72 | perplexity: 2254.12 | lr: 6.71e-05 | norm: 2.2338 | dt: 11674.9866ms | tok/sec: 33.6802\n",
      "step   48 | train loss: 7.80 | val loss: 7.66 | perplexity: 2111.88 | lr: 6.85e-05 | norm: 1.8649 | dt: 11692.8031ms | tok/sec: 33.6289\n",
      "step   49 | train loss: 7.66 | val loss: 7.59 | perplexity: 1976.02 | lr: 6.99e-05 | norm: 1.9855 | dt: 11681.6161ms | tok/sec: 33.6611\n",
      "step   50 | train loss: 7.46 | val loss: 7.53 | perplexity: 1867.68 | lr: 7.13e-05 | norm: 1.7217 | dt: 11671.9425ms | tok/sec: 33.6890\n",
      "step   51 | train loss: 7.73 | val loss: 7.48 | perplexity: 1766.06 | lr: 7.27e-05 | norm: 2.0608 | dt: 11668.1025ms | tok/sec: 33.7001\n",
      "step   52 | train loss: 7.39 | val loss: 7.41 | perplexity: 1656.76 | lr: 7.41e-05 | norm: 2.1921 | dt: 11701.7384ms | tok/sec: 33.6032\n",
      "step   53 | train loss: 7.51 | val loss: 7.37 | perplexity: 1580.02 | lr: 7.55e-05 | norm: 1.4672 | dt: 11705.9414ms | tok/sec: 33.5911\n",
      "step   54 | train loss: 7.59 | val loss: 7.33 | perplexity: 1517.80 | lr: 7.69e-05 | norm: 2.0533 | dt: 11702.6298ms | tok/sec: 33.6007\n",
      "step   55 | train loss: 7.16 | val loss: 7.27 | perplexity: 1436.49 | lr: 7.83e-05 | norm: 3.1302 | dt: 11678.6737ms | tok/sec: 33.6696\n",
      "step   56 | train loss: 7.30 | val loss: 7.23 | perplexity: 1382.47 | lr: 7.97e-05 | norm: 1.7229 | dt: 11712.2118ms | tok/sec: 33.5732\n",
      "step   57 | train loss: 7.29 | val loss: 7.19 | perplexity: 1324.06 | lr: 8.11e-05 | norm: 1.5956 | dt: 11685.9493ms | tok/sec: 33.6486\n",
      "step   58 | train loss: 7.05 | val loss: 7.14 | perplexity: 1256.08 | lr: 8.25e-05 | norm: 1.5887 | dt: 11676.9736ms | tok/sec: 33.6745\n",
      "step   59 | train loss: 6.95 | val loss: 7.09 | perplexity: 1198.79 | lr: 8.39e-05 | norm: 1.3976 | dt: 11691.1173ms | tok/sec: 33.6337\n",
      "step   60 | train loss: 7.06 | val loss: 7.03 | perplexity: 1133.63 | lr: 8.53e-05 | norm: 1.6493 | dt: 11660.8160ms | tok/sec: 33.7211\n",
      "step   61 | train loss: 7.25 | val loss: 6.98 | perplexity: 1074.58 | lr: 8.67e-05 | norm: 1.3919 | dt: 11719.9001ms | tok/sec: 33.5511\n",
      "step   62 | train loss: 7.24 | val loss: 6.94 | perplexity: 1031.51 | lr: 8.81e-05 | norm: 1.3730 | dt: 11661.2680ms | tok/sec: 33.7198\n",
      "step   63 | train loss: 7.41 | val loss: 6.93 | perplexity: 1027.46 | lr: 8.95e-05 | norm: 1.8170 | dt: 11704.0641ms | tok/sec: 33.5965\n",
      "step   64 | train loss: 7.19 | val loss: 6.88 | perplexity: 968.02 | lr: 9.09e-05 | norm: 2.1218 | dt: 11690.1987ms | tok/sec: 33.6364\n",
      "step   65 | train loss: 7.25 | val loss: 6.82 | perplexity: 919.24 | lr: 9.23e-05 | norm: 1.5692 | dt: 11669.7125ms | tok/sec: 33.6954\n",
      "step   66 | train loss: 6.90 | val loss: 6.78 | perplexity: 876.71 | lr: 9.37e-05 | norm: 1.4336 | dt: 11686.1489ms | tok/sec: 33.6480\n",
      "step   67 | train loss: 6.70 | val loss: 6.73 | perplexity: 835.03 | lr: 9.51e-05 | norm: 1.5909 | dt: 11708.5953ms | tok/sec: 33.5835\n",
      "step   68 | train loss: 6.73 | val loss: 6.69 | perplexity: 800.78 | lr: 9.65e-05 | norm: 1.3243 | dt: 11700.2723ms | tok/sec: 33.6074\n",
      "step   69 | train loss: 6.44 | val loss: 6.65 | perplexity: 770.68 | lr: 9.79e-05 | norm: 1.3089 | dt: 11670.4366ms | tok/sec: 33.6933\n",
      "step   70 | train loss: 6.39 | val loss: 6.61 | perplexity: 739.57 | lr: 9.93e-05 | norm: 1.0222 | dt: 11713.6106ms | tok/sec: 33.5692\n",
      "step   71 | train loss: 6.76 | val loss: 6.57 | perplexity: 713.90 | lr: 1.01e-04 | norm: 1.1048 | dt: 11663.4369ms | tok/sec: 33.7136\n",
      "step   72 | train loss: 6.67 | val loss: 6.55 | perplexity: 699.18 | lr: 1.02e-04 | norm: 1.1682 | dt: 11684.3076ms | tok/sec: 33.6533\n",
      "step   73 | train loss: 6.34 | val loss: 6.53 | perplexity: 686.91 | lr: 1.03e-04 | norm: 1.2485 | dt: 11643.2464ms | tok/sec: 33.7720\n",
      "step   74 | train loss: 6.49 | val loss: 6.50 | perplexity: 665.02 | lr: 1.05e-04 | norm: 1.4539 | dt: 11669.6424ms | tok/sec: 33.6956\n",
      "step   75 | train loss: 6.53 | val loss: 6.46 | perplexity: 637.27 | lr: 1.06e-04 | norm: 0.9808 | dt: 11700.3167ms | tok/sec: 33.6073\n",
      "step   76 | train loss: 7.13 | val loss: 6.44 | perplexity: 627.33 | lr: 1.08e-04 | norm: 1.0508 | dt: 11649.6561ms | tok/sec: 33.7534\n",
      "step   77 | train loss: 6.47 | val loss: 6.41 | perplexity: 610.79 | lr: 1.09e-04 | norm: 1.2031 | dt: 11698.1232ms | tok/sec: 33.6136\n",
      "step   78 | train loss: 6.19 | val loss: 6.38 | perplexity: 591.08 | lr: 1.10e-04 | norm: 1.1357 | dt: 11687.7930ms | tok/sec: 33.6433\n",
      "step   79 | train loss: 6.58 | val loss: 6.36 | perplexity: 579.98 | lr: 1.12e-04 | norm: 1.3714 | dt: 11682.2033ms | tok/sec: 33.6594\n",
      "step   80 | train loss: 6.39 | val loss: 6.35 | perplexity: 574.21 | lr: 1.13e-04 | norm: 1.1945 | dt: 11690.7237ms | tok/sec: 33.6349\n",
      "step   81 | train loss: 6.44 | val loss: 6.34 | perplexity: 564.09 | lr: 1.15e-04 | norm: 1.2621 | dt: 11678.2179ms | tok/sec: 33.6709\n",
      "step   82 | train loss: 6.10 | val loss: 6.30 | perplexity: 543.28 | lr: 1.16e-04 | norm: 1.2028 | dt: 11695.7145ms | tok/sec: 33.6205\n",
      "step   83 | train loss: 6.42 | val loss: 6.26 | perplexity: 523.17 | lr: 1.17e-04 | norm: 1.3436 | dt: 11706.4645ms | tok/sec: 33.5896\n",
      "step   84 | train loss: 6.26 | val loss: 6.24 | perplexity: 513.38 | lr: 1.19e-04 | norm: 0.7861 | dt: 11663.4500ms | tok/sec: 33.7135\n",
      "step   85 | train loss: 6.29 | val loss: 6.23 | perplexity: 507.03 | lr: 1.20e-04 | norm: 0.9745 | dt: 11703.6011ms | tok/sec: 33.5979\n",
      "step   86 | train loss: 6.41 | val loss: 6.22 | perplexity: 504.94 | lr: 1.22e-04 | norm: 1.1371 | dt: 11700.8994ms | tok/sec: 33.6056\n",
      "step   87 | train loss: 6.18 | val loss: 6.22 | perplexity: 500.38 | lr: 1.23e-04 | norm: 1.4041 | dt: 11697.8130ms | tok/sec: 33.6145\n",
      "step   88 | train loss: 5.84 | val loss: 6.19 | perplexity: 488.42 | lr: 1.24e-04 | norm: 1.6476 | dt: 11686.5587ms | tok/sec: 33.6469\n",
      "step   89 | train loss: 6.10 | val loss: 6.15 | perplexity: 470.86 | lr: 1.26e-04 | norm: 1.1828 | dt: 11667.2242ms | tok/sec: 33.7026\n",
      "step   90 | train loss: 5.96 | val loss: 6.14 | perplexity: 465.58 | lr: 1.27e-04 | norm: 1.1198 | dt: 11678.1495ms | tok/sec: 33.6711\n",
      "step   91 | train loss: 6.14 | val loss: 6.13 | perplexity: 460.32 | lr: 1.29e-04 | norm: 0.9200 | dt: 11693.6255ms | tok/sec: 33.6265\n",
      "step   92 | train loss: 6.57 | val loss: 6.13 | perplexity: 457.44 | lr: 1.30e-04 | norm: 0.6791 | dt: 11682.6503ms | tok/sec: 33.6581\n",
      "step   93 | train loss: 6.42 | val loss: 6.10 | perplexity: 447.15 | lr: 1.31e-04 | norm: 1.0637 | dt: 11678.9250ms | tok/sec: 33.6689\n",
      "step   94 | train loss: 6.34 | val loss: 6.10 | perplexity: 445.49 | lr: 1.33e-04 | norm: 0.5203 | dt: 11697.7108ms | tok/sec: 33.6148\n",
      "step   95 | train loss: 6.09 | val loss: 6.08 | perplexity: 439.04 | lr: 1.34e-04 | norm: 1.2083 | dt: 11673.1265ms | tok/sec: 33.6856\n",
      "step   96 | train loss: 5.86 | val loss: 6.08 | perplexity: 436.13 | lr: 1.36e-04 | norm: 0.9205 | dt: 11682.0889ms | tok/sec: 33.6597\n",
      "step   97 | train loss: 6.25 | val loss: 6.06 | perplexity: 427.35 | lr: 1.37e-04 | norm: 1.1906 | dt: 11693.5635ms | tok/sec: 33.6267\n",
      "step   98 | train loss: 6.39 | val loss: 6.05 | perplexity: 424.14 | lr: 1.38e-04 | norm: 0.8723 | dt: 11664.8755ms | tok/sec: 33.7094\n",
      "step   99 | train loss: 5.92 | val loss: 6.05 | perplexity: 425.33 | lr: 1.40e-04 | norm: 1.0513 | dt: 11660.3496ms | tok/sec: 33.7225\n",
      "step  100 | train loss: 6.14 | val loss: 6.03 | perplexity: 414.57 | lr: 1.41e-04 | norm: 1.1979 | dt: 11682.4865ms | tok/sec: 33.6586\n",
      "step  101 | train loss: 6.17 | val loss: 6.01 | perplexity: 407.87 | lr: 1.43e-04 | norm: 0.8562 | dt: 11659.7049ms | tok/sec: 33.7244\n",
      "step  102 | train loss: 6.01 | val loss: 6.00 | perplexity: 401.89 | lr: 1.44e-04 | norm: 0.9204 | dt: 11678.6518ms | tok/sec: 33.6696\n",
      "step  103 | train loss: 5.58 | val loss: 6.00 | perplexity: 404.33 | lr: 1.45e-04 | norm: 0.9014 | dt: 11678.8604ms | tok/sec: 33.6690\n",
      "step  104 | train loss: 6.02 | val loss: 5.99 | perplexity: 397.97 | lr: 1.47e-04 | norm: 1.3713 | dt: 11698.8707ms | tok/sec: 33.6114\n",
      "step  105 | train loss: 5.98 | val loss: 5.98 | perplexity: 394.89 | lr: 1.48e-04 | norm: 0.6998 | dt: 11657.5565ms | tok/sec: 33.7306\n",
      "step  106 | train loss: 5.89 | val loss: 5.96 | perplexity: 385.89 | lr: 1.50e-04 | norm: 1.0264 | dt: 11694.9575ms | tok/sec: 33.6227\n",
      "step  107 | train loss: 6.28 | val loss: 5.95 | perplexity: 384.17 | lr: 1.51e-04 | norm: 0.8069 | dt: 11654.0711ms | tok/sec: 33.7407\n",
      "step  108 | train loss: 5.90 | val loss: 5.93 | perplexity: 375.53 | lr: 1.52e-04 | norm: 1.4819 | dt: 11708.0274ms | tok/sec: 33.5852\n",
      "step  109 | train loss: 5.88 | val loss: 5.94 | perplexity: 378.42 | lr: 1.54e-04 | norm: 1.1301 | dt: 11699.5201ms | tok/sec: 33.6096\n",
      "step  110 | train loss: 6.19 | val loss: 5.93 | perplexity: 377.37 | lr: 1.55e-04 | norm: 1.1889 | dt: 11693.4698ms | tok/sec: 33.6270\n",
      "step  111 | train loss: 5.98 | val loss: 5.93 | perplexity: 375.10 | lr: 1.57e-04 | norm: 1.0209 | dt: 11694.9365ms | tok/sec: 33.6228\n",
      "step  112 | train loss: 5.89 | val loss: 5.91 | perplexity: 367.34 | lr: 1.58e-04 | norm: 1.2526 | dt: 11687.4788ms | tok/sec: 33.6442\n",
      "step  113 | train loss: 6.04 | val loss: 5.89 | perplexity: 359.93 | lr: 1.59e-04 | norm: 0.9769 | dt: 11691.0748ms | tok/sec: 33.6339\n",
      "step  114 | train loss: 5.72 | val loss: 5.88 | perplexity: 356.08 | lr: 1.61e-04 | norm: 1.3336 | dt: 11677.0604ms | tok/sec: 33.6742\n",
      "step  115 | train loss: 5.98 | val loss: 5.86 | perplexity: 352.41 | lr: 1.62e-04 | norm: 1.0134 | dt: 11675.4539ms | tok/sec: 33.6789\n",
      "step  116 | train loss: 5.88 | val loss: 5.85 | perplexity: 348.74 | lr: 1.64e-04 | norm: 1.0200 | dt: 11700.1994ms | tok/sec: 33.6076\n",
      "step  117 | train loss: 5.97 | val loss: 5.84 | perplexity: 343.50 | lr: 1.65e-04 | norm: 0.9460 | dt: 11705.9352ms | tok/sec: 33.5912\n",
      "step  118 | train loss: 5.80 | val loss: 5.82 | perplexity: 338.18 | lr: 1.66e-04 | norm: 0.7856 | dt: 11649.5228ms | tok/sec: 33.7538\n",
      "step  119 | train loss: 5.85 | val loss: 5.82 | perplexity: 335.31 | lr: 1.68e-04 | norm: 0.7295 | dt: 11698.6077ms | tok/sec: 33.6122\n",
      "step  120 | train loss: 5.52 | val loss: 5.80 | perplexity: 331.93 | lr: 1.69e-04 | norm: 0.8932 | dt: 11692.4305ms | tok/sec: 33.6300\n",
      "step  121 | train loss: 5.92 | val loss: 5.79 | perplexity: 326.74 | lr: 1.71e-04 | norm: 0.6410 | dt: 11652.7805ms | tok/sec: 33.7444\n",
      "step  122 | train loss: 5.78 | val loss: 5.77 | perplexity: 320.45 | lr: 1.72e-04 | norm: 0.6982 | dt: 11700.5184ms | tok/sec: 33.6067\n",
      "step  123 | train loss: 5.79 | val loss: 5.77 | perplexity: 319.34 | lr: 1.73e-04 | norm: 0.6930 | dt: 11672.3964ms | tok/sec: 33.6877\n",
      "step  124 | train loss: 5.90 | val loss: 5.77 | perplexity: 321.95 | lr: 1.75e-04 | norm: 0.8508 | dt: 11697.8796ms | tok/sec: 33.6143\n",
      "step  125 | train loss: 5.71 | val loss: 5.76 | perplexity: 317.92 | lr: 1.76e-04 | norm: 1.0030 | dt: 11706.0046ms | tok/sec: 33.5910\n",
      "step  126 | train loss: 5.83 | val loss: 5.76 | perplexity: 318.09 | lr: 1.78e-04 | norm: 0.6135 | dt: 11718.3747ms | tok/sec: 33.5555\n",
      "step  127 | train loss: 5.57 | val loss: 5.75 | perplexity: 312.81 | lr: 1.79e-04 | norm: 0.8026 | dt: 11704.0257ms | tok/sec: 33.5966\n",
      "step  128 | train loss: 5.33 | val loss: 5.74 | perplexity: 310.98 | lr: 1.80e-04 | norm: 1.0301 | dt: 11686.7714ms | tok/sec: 33.6462\n",
      "step  129 | train loss: 5.92 | val loss: 5.75 | perplexity: 312.91 | lr: 1.82e-04 | norm: 0.8898 | dt: 11699.3012ms | tok/sec: 33.6102\n",
      "step  130 | train loss: 5.56 | val loss: 5.74 | perplexity: 310.84 | lr: 1.83e-04 | norm: 1.2027 | dt: 11659.9817ms | tok/sec: 33.7236\n",
      "step  131 | train loss: 5.58 | val loss: 5.74 | perplexity: 309.72 | lr: 1.85e-04 | norm: 1.1888 | dt: 11688.4975ms | tok/sec: 33.6413\n",
      "step  132 | train loss: 5.98 | val loss: 5.74 | perplexity: 311.23 | lr: 1.86e-04 | norm: 1.0069 | dt: 11707.0410ms | tok/sec: 33.5880\n",
      "step  133 | train loss: 5.89 | val loss: 5.73 | perplexity: 307.01 | lr: 1.87e-04 | norm: 1.2137 | dt: 11672.2620ms | tok/sec: 33.6881\n",
      "step  134 | train loss: 6.21 | val loss: 5.72 | perplexity: 306.01 | lr: 1.89e-04 | norm: 0.6626 | dt: 11698.9830ms | tok/sec: 33.6111\n",
      "step  135 | train loss: 5.99 | val loss: 5.72 | perplexity: 303.50 | lr: 1.90e-04 | norm: 0.9744 | dt: 11709.2535ms | tok/sec: 33.5816\n",
      "step  136 | train loss: 5.60 | val loss: 5.71 | perplexity: 300.96 | lr: 1.92e-04 | norm: 1.1604 | dt: 11707.4637ms | tok/sec: 33.5868\n",
      "step  137 | train loss: 5.76 | val loss: 5.71 | perplexity: 303.15 | lr: 1.93e-04 | norm: 0.8393 | dt: 11723.7282ms | tok/sec: 33.5402\n",
      "step  138 | train loss: 5.78 | val loss: 5.71 | perplexity: 302.65 | lr: 1.94e-04 | norm: 1.1271 | dt: 11694.0749ms | tok/sec: 33.6252\n",
      "step  139 | train loss: 5.67 | val loss: 5.69 | perplexity: 296.01 | lr: 1.96e-04 | norm: 0.8085 | dt: 11686.7614ms | tok/sec: 33.6463\n",
      "step  140 | train loss: 5.95 | val loss: 5.68 | perplexity: 294.39 | lr: 1.97e-04 | norm: 0.7807 | dt: 11719.5425ms | tok/sec: 33.5522\n",
      "step  141 | train loss: 5.98 | val loss: 5.68 | perplexity: 293.65 | lr: 1.99e-04 | norm: 0.8340 | dt: 11715.3664ms | tok/sec: 33.5641\n",
      "step  142 | train loss: 5.65 | val loss: 5.66 | perplexity: 287.87 | lr: 2.00e-04 | norm: 0.9971 | dt: 11714.2384ms | tok/sec: 33.5674\n",
      "step  143 | train loss: 5.65 | val loss: 5.65 | perplexity: 284.81 | lr: 2.01e-04 | norm: 0.8857 | dt: 11684.7546ms | tok/sec: 33.6521\n",
      "step  144 | train loss: 5.71 | val loss: 5.64 | perplexity: 281.22 | lr: 2.03e-04 | norm: 1.1664 | dt: 11688.5612ms | tok/sec: 33.6411\n",
      "step  145 | train loss: 5.78 | val loss: 5.64 | perplexity: 282.60 | lr: 2.04e-04 | norm: 0.9016 | dt: 11722.2509ms | tok/sec: 33.5444\n",
      "step  146 | train loss: 5.83 | val loss: 5.64 | perplexity: 281.99 | lr: 2.06e-04 | norm: 0.8559 | dt: 11713.2299ms | tok/sec: 33.5702\n",
      "step  147 | train loss: 6.04 | val loss: 5.63 | perplexity: 278.08 | lr: 2.07e-04 | norm: 1.0954 | dt: 11710.0208ms | tok/sec: 33.5794\n",
      "step  148 | train loss: 5.86 | val loss: 5.61 | perplexity: 273.44 | lr: 2.08e-04 | norm: 0.8042 | dt: 11688.8814ms | tok/sec: 33.6402\n",
      "step  149 | train loss: 5.79 | val loss: 5.61 | perplexity: 272.02 | lr: 2.10e-04 | norm: 1.0442 | dt: 11710.4726ms | tok/sec: 33.5781\n",
      "step  150 | train loss: 5.53 | val loss: 5.62 | perplexity: 276.41 | lr: 2.11e-04 | norm: 0.8375 | dt: 11723.7432ms | tok/sec: 33.5401\n",
      "step  151 | train loss: 5.62 | val loss: 5.59 | perplexity: 267.62 | lr: 2.13e-04 | norm: 1.1249 | dt: 11712.5757ms | tok/sec: 33.5721\n",
      "step  152 | train loss: 5.91 | val loss: 5.57 | perplexity: 262.75 | lr: 2.14e-04 | norm: 0.7309 | dt: 11719.5437ms | tok/sec: 33.5522\n",
      "step  153 | train loss: 5.79 | val loss: 5.57 | perplexity: 262.73 | lr: 2.15e-04 | norm: 1.0286 | dt: 11717.4058ms | tok/sec: 33.5583\n",
      "step  154 | train loss: 5.58 | val loss: 5.56 | perplexity: 259.79 | lr: 2.17e-04 | norm: 0.9638 | dt: 11714.3142ms | tok/sec: 33.5671\n",
      "step  155 | train loss: 5.62 | val loss: 5.56 | perplexity: 261.08 | lr: 2.18e-04 | norm: 0.9247 | dt: 11716.5725ms | tok/sec: 33.5607\n",
      "step  156 | train loss: 6.06 | val loss: 5.57 | perplexity: 263.65 | lr: 2.20e-04 | norm: 1.0074 | dt: 11716.5506ms | tok/sec: 33.5607\n",
      "step  157 | train loss: 6.02 | val loss: 5.58 | perplexity: 264.41 | lr: 2.21e-04 | norm: 1.2087 | dt: 11692.8704ms | tok/sec: 33.6287\n",
      "step  158 | train loss: 5.67 | val loss: 5.57 | perplexity: 261.21 | lr: 2.22e-04 | norm: 0.7640 | dt: 11682.6746ms | tok/sec: 33.6580\n",
      "step  159 | train loss: 6.05 | val loss: 5.56 | perplexity: 259.54 | lr: 2.24e-04 | norm: 1.1907 | dt: 11718.1709ms | tok/sec: 33.5561\n",
      "step  160 | train loss: 5.24 | val loss: 5.55 | perplexity: 257.73 | lr: 2.25e-04 | norm: 0.9803 | dt: 11668.6919ms | tok/sec: 33.6984\n",
      "step  161 | train loss: 5.76 | val loss: 5.53 | perplexity: 253.01 | lr: 2.27e-04 | norm: 0.7770 | dt: 11703.1517ms | tok/sec: 33.5992\n",
      "step  162 | train loss: 5.82 | val loss: 5.52 | perplexity: 250.03 | lr: 2.28e-04 | norm: 0.7792 | dt: 11716.3882ms | tok/sec: 33.5612\n",
      "step  163 | train loss: 5.88 | val loss: 5.52 | perplexity: 250.04 | lr: 2.29e-04 | norm: 1.0756 | dt: 11710.8355ms | tok/sec: 33.5771\n",
      "step  164 | train loss: 5.89 | val loss: 5.53 | perplexity: 252.96 | lr: 2.31e-04 | norm: 0.7248 | dt: 11708.3199ms | tok/sec: 33.5843\n",
      "step  165 | train loss: 5.62 | val loss: 5.53 | perplexity: 251.45 | lr: 2.32e-04 | norm: 0.8329 | dt: 11717.9530ms | tok/sec: 33.5567\n",
      "step  166 | train loss: 5.88 | val loss: 5.54 | perplexity: 254.60 | lr: 2.34e-04 | norm: 0.7569 | dt: 11725.1012ms | tok/sec: 33.5363\n",
      "step  167 | train loss: 6.09 | val loss: 5.53 | perplexity: 252.68 | lr: 2.35e-04 | norm: 0.8976 | dt: 11723.4001ms | tok/sec: 33.5411\n",
      "step  168 | train loss: 6.26 | val loss: 5.52 | perplexity: 250.66 | lr: 2.36e-04 | norm: 0.7413 | dt: 11714.8535ms | tok/sec: 33.5656\n",
      "step  169 | train loss: 5.53 | val loss: 5.52 | perplexity: 248.72 | lr: 2.38e-04 | norm: 1.0309 | dt: 11718.6534ms | tok/sec: 33.5547\n",
      "step  170 | train loss: 5.70 | val loss: 5.52 | perplexity: 250.58 | lr: 2.39e-04 | norm: 0.9195 | dt: 11711.3936ms | tok/sec: 33.5755\n",
      "step  171 | train loss: 5.77 | val loss: 5.51 | perplexity: 247.99 | lr: 2.41e-04 | norm: 0.9859 | dt: 11688.7429ms | tok/sec: 33.6406\n",
      "step  172 | train loss: 5.75 | val loss: 5.52 | perplexity: 249.51 | lr: 2.42e-04 | norm: 0.7129 | dt: 11667.9709ms | tok/sec: 33.7005\n",
      "step  173 | train loss: 5.57 | val loss: 5.50 | perplexity: 245.06 | lr: 2.43e-04 | norm: 0.8710 | dt: 11707.8743ms | tok/sec: 33.5856\n",
      "step  174 | train loss: 5.70 | val loss: 5.49 | perplexity: 242.41 | lr: 2.45e-04 | norm: 0.6093 | dt: 11722.2753ms | tok/sec: 33.5443\n",
      "step  175 | train loss: 6.03 | val loss: 5.50 | perplexity: 243.81 | lr: 2.46e-04 | norm: 0.9741 | dt: 11670.9940ms | tok/sec: 33.6917\n",
      "step  176 | train loss: 6.27 | val loss: 5.51 | perplexity: 248.02 | lr: 2.48e-04 | norm: 1.0749 | dt: 11708.1804ms | tok/sec: 33.5847\n",
      "step  177 | train loss: 5.48 | val loss: 5.52 | perplexity: 250.68 | lr: 2.49e-04 | norm: 1.4433 | dt: 11707.6881ms | tok/sec: 33.5861\n",
      "step  178 | train loss: 6.07 | val loss: 5.50 | perplexity: 245.68 | lr: 2.50e-04 | norm: 1.4278 | dt: 11716.4061ms | tok/sec: 33.5611\n",
      "step  179 | train loss: 5.59 | val loss: 5.53 | perplexity: 251.26 | lr: 2.52e-04 | norm: 1.1604 | dt: 11712.5819ms | tok/sec: 33.5721\n",
      "step  180 | train loss: 5.85 | val loss: 5.51 | perplexity: 246.99 | lr: 2.53e-04 | norm: 2.8873 | dt: 11709.8227ms | tok/sec: 33.5800\n",
      "step  181 | train loss: 5.58 | val loss: 5.51 | perplexity: 246.24 | lr: 2.55e-04 | norm: 1.1338 | dt: 11689.2920ms | tok/sec: 33.6390\n",
      "step  182 | train loss: 5.76 | val loss: 5.51 | perplexity: 245.94 | lr: 2.56e-04 | norm: 1.0709 | dt: 11717.4420ms | tok/sec: 33.5582\n",
      "step  183 | train loss: 5.50 | val loss: 5.51 | perplexity: 246.08 | lr: 2.57e-04 | norm: 0.8361 | dt: 11725.6639ms | tok/sec: 33.5346\n",
      "step  184 | train loss: 5.55 | val loss: 5.49 | perplexity: 242.11 | lr: 2.59e-04 | norm: 0.8506 | dt: 11720.4041ms | tok/sec: 33.5497\n",
      "step  185 | train loss: 5.23 | val loss: 5.48 | perplexity: 240.36 | lr: 2.60e-04 | norm: 0.7333 | dt: 11718.6308ms | tok/sec: 33.5548\n",
      "step  186 | train loss: 5.49 | val loss: 5.48 | perplexity: 238.93 | lr: 2.62e-04 | norm: 0.9118 | dt: 11727.8323ms | tok/sec: 33.5284\n",
      "step  187 | train loss: 5.63 | val loss: 5.46 | perplexity: 235.37 | lr: 2.63e-04 | norm: 0.7149 | dt: 11722.2881ms | tok/sec: 33.5443\n",
      "step  188 | train loss: 5.75 | val loss: 5.45 | perplexity: 233.10 | lr: 2.64e-04 | norm: 0.6799 | dt: 11720.9256ms | tok/sec: 33.5482\n",
      "step  189 | train loss: 5.62 | val loss: 5.44 | perplexity: 231.53 | lr: 2.66e-04 | norm: 0.7935 | dt: 11711.3874ms | tok/sec: 33.5755\n",
      "step  190 | train loss: 5.56 | val loss: 5.46 | perplexity: 234.84 | lr: 2.67e-04 | norm: 0.5486 | dt: 11739.8980ms | tok/sec: 33.4940\n",
      "step  191 | train loss: 5.44 | val loss: 5.47 | perplexity: 238.18 | lr: 2.69e-04 | norm: 0.7443 | dt: 11727.9332ms | tok/sec: 33.5282\n",
      "step  192 | train loss: 5.42 | val loss: 5.47 | perplexity: 236.52 | lr: 2.70e-04 | norm: 0.8616 | dt: 11708.1068ms | tok/sec: 33.5849\n",
      "step  193 | train loss: 5.77 | val loss: 5.47 | perplexity: 236.52 | lr: 2.71e-04 | norm: 0.7064 | dt: 11722.7902ms | tok/sec: 33.5429\n",
      "step  194 | train loss: 5.62 | val loss: 5.49 | perplexity: 243.34 | lr: 2.73e-04 | norm: 0.9037 | dt: 11711.9341ms | tok/sec: 33.5740\n",
      "step  195 | train loss: 5.69 | val loss: 5.49 | perplexity: 242.93 | lr: 2.74e-04 | norm: 1.1259 | dt: 11712.4152ms | tok/sec: 33.5726\n",
      "step  196 | train loss: 5.62 | val loss: 5.47 | perplexity: 237.88 | lr: 2.76e-04 | norm: 0.9261 | dt: 11729.3065ms | tok/sec: 33.5242\n",
      "step  197 | train loss: 5.68 | val loss: 5.45 | perplexity: 232.86 | lr: 2.77e-04 | norm: 0.7579 | dt: 11709.0352ms | tok/sec: 33.5823\n",
      "step  198 | train loss: 5.40 | val loss: 5.45 | perplexity: 232.15 | lr: 2.78e-04 | norm: 0.7378 | dt: 11717.5503ms | tok/sec: 33.5579\n",
      "step  199 | train loss: 5.61 | val loss: 5.44 | perplexity: 230.87 | lr: 2.80e-04 | norm: 0.8144 | dt: 11728.4002ms | tok/sec: 33.5268\n",
      "step  200 | train loss: 5.59 | val loss: 5.43 | perplexity: 227.32 | lr: 2.81e-04 | norm: 0.9093 | dt: 11719.3391ms | tok/sec: 33.5527\n",
      "step  201 | train loss: 5.73 | val loss: 5.42 | perplexity: 226.46 | lr: 2.83e-04 | norm: 0.6653 | dt: 11708.1437ms | tok/sec: 33.5848\n",
      "step  202 | train loss: 5.55 | val loss: 5.44 | perplexity: 229.60 | lr: 2.84e-04 | norm: 0.8611 | dt: 11705.7064ms | tok/sec: 33.5918\n",
      "step  203 | train loss: 5.77 | val loss: 5.43 | perplexity: 228.93 | lr: 2.85e-04 | norm: 0.8295 | dt: 11723.8884ms | tok/sec: 33.5397\n",
      "step  204 | train loss: 5.71 | val loss: 5.42 | perplexity: 225.10 | lr: 2.87e-04 | norm: 0.8929 | dt: 11728.8170ms | tok/sec: 33.5256\n",
      "step  205 | train loss: 5.63 | val loss: 5.40 | perplexity: 221.33 | lr: 2.88e-04 | norm: 0.8718 | dt: 11742.1341ms | tok/sec: 33.4876\n",
      "step  206 | train loss: 5.71 | val loss: 5.37 | perplexity: 214.51 | lr: 2.90e-04 | norm: 1.0532 | dt: 11706.8563ms | tok/sec: 33.5885\n",
      "step  207 | train loss: 5.51 | val loss: 5.34 | perplexity: 209.04 | lr: 2.91e-04 | norm: 0.7084 | dt: 11710.5629ms | tok/sec: 33.5779\n",
      "step  208 | train loss: 5.60 | val loss: 5.35 | perplexity: 209.95 | lr: 2.92e-04 | norm: 0.7416 | dt: 11705.2915ms | tok/sec: 33.5930\n",
      "step  209 | train loss: 5.46 | val loss: 5.36 | perplexity: 213.12 | lr: 2.94e-04 | norm: 0.8319 | dt: 11693.0585ms | tok/sec: 33.6282\n",
      "step  210 | train loss: 5.71 | val loss: 5.37 | perplexity: 214.02 | lr: 2.95e-04 | norm: 0.6552 | dt: 11695.6611ms | tok/sec: 33.6207\n",
      "step  211 | train loss: 5.44 | val loss: 5.35 | perplexity: 209.62 | lr: 2.97e-04 | norm: 0.7519 | dt: 11707.2952ms | tok/sec: 33.5873\n",
      "step  212 | train loss: 5.93 | val loss: 5.36 | perplexity: 212.59 | lr: 2.98e-04 | norm: 0.5929 | dt: 11720.4680ms | tok/sec: 33.5495\n",
      "step  213 | train loss: 5.81 | val loss: 5.36 | perplexity: 212.66 | lr: 2.99e-04 | norm: 0.9486 | dt: 11707.9585ms | tok/sec: 33.5854\n",
      "step  214 | train loss: 5.48 | val loss: 5.38 | perplexity: 216.06 | lr: 3.01e-04 | norm: 0.7533 | dt: 11719.9078ms | tok/sec: 33.5511\n",
      "step  215 | train loss: 5.30 | val loss: 5.36 | perplexity: 212.67 | lr: 3.02e-04 | norm: 1.1172 | dt: 11715.9660ms | tok/sec: 33.5624\n",
      "step  216 | train loss: 5.61 | val loss: 5.35 | perplexity: 210.85 | lr: 3.03e-04 | norm: 0.8798 | dt: 11716.2583ms | tok/sec: 33.5616\n",
      "step  217 | train loss: 5.46 | val loss: 5.35 | perplexity: 210.50 | lr: 3.05e-04 | norm: 0.7728 | dt: 11745.8279ms | tok/sec: 33.4771\n",
      "step  218 | train loss: 5.49 | val loss: 5.35 | perplexity: 209.72 | lr: 3.06e-04 | norm: 0.7289 | dt: 11688.5760ms | tok/sec: 33.6411\n",
      "step  219 | train loss: 5.51 | val loss: 5.35 | perplexity: 210.40 | lr: 3.08e-04 | norm: 0.5608 | dt: 11718.1423ms | tok/sec: 33.5562\n",
      "step  220 | train loss: 5.61 | val loss: 5.34 | perplexity: 209.04 | lr: 3.09e-04 | norm: 0.5767 | dt: 11717.9646ms | tok/sec: 33.5567\n",
      "step  221 | train loss: 5.60 | val loss: 5.33 | perplexity: 205.84 | lr: 3.10e-04 | norm: 1.0366 | dt: 11718.6580ms | tok/sec: 33.5547\n",
      "step  222 | train loss: 5.53 | val loss: 5.32 | perplexity: 205.08 | lr: 3.12e-04 | norm: 0.8859 | dt: 11733.8026ms | tok/sec: 33.5114\n",
      "step  223 | train loss: 5.49 | val loss: 5.31 | perplexity: 202.62 | lr: 3.13e-04 | norm: 0.6493 | dt: 11717.3951ms | tok/sec: 33.5583\n",
      "step  224 | train loss: 5.54 | val loss: 5.31 | perplexity: 202.88 | lr: 3.15e-04 | norm: 0.7488 | dt: 11744.9465ms | tok/sec: 33.4796\n",
      "step  225 | train loss: 5.61 | val loss: 5.31 | perplexity: 202.64 | lr: 3.16e-04 | norm: 0.6547 | dt: 11677.2699ms | tok/sec: 33.6736\n",
      "step  226 | train loss: 5.37 | val loss: 5.31 | perplexity: 203.10 | lr: 3.17e-04 | norm: 0.7301 | dt: 11713.7439ms | tok/sec: 33.5688\n",
      "step  227 | train loss: 6.70 | val loss: 5.37 | perplexity: 214.23 | lr: 3.19e-04 | norm: 3.1010 | dt: 11716.1787ms | tok/sec: 33.5618\n",
      "step  228 | train loss: 5.62 | val loss: 5.35 | perplexity: 211.60 | lr: 3.20e-04 | norm: 1.3596 | dt: 11714.8416ms | tok/sec: 33.5656\n",
      "step  229 | train loss: 5.56 | val loss: 5.42 | perplexity: 225.06 | lr: 3.22e-04 | norm: 1.0534 | dt: 11712.9397ms | tok/sec: 33.5711\n",
      "step  230 | train loss: 5.70 | val loss: 5.34 | perplexity: 209.12 | lr: 3.23e-04 | norm: 2.3556 | dt: 11695.2207ms | tok/sec: 33.6219\n",
      "step  231 | train loss: 5.39 | val loss: 5.37 | perplexity: 214.22 | lr: 3.24e-04 | norm: 1.1028 | dt: 11670.2299ms | tok/sec: 33.6939\n",
      "step  232 | train loss: 5.63 | val loss: 5.36 | perplexity: 212.74 | lr: 3.26e-04 | norm: 1.4938 | dt: 11698.6310ms | tok/sec: 33.6121\n",
      "step  233 | train loss: 5.78 | val loss: 5.36 | perplexity: 213.42 | lr: 3.27e-04 | norm: 0.6840 | dt: 11672.7586ms | tok/sec: 33.6866\n",
      "step  234 | train loss: 5.49 | val loss: 5.36 | perplexity: 213.45 | lr: 3.29e-04 | norm: 0.8892 | dt: 11699.8227ms | tok/sec: 33.6087\n",
      "step  235 | train loss: 5.69 | val loss: 5.34 | perplexity: 207.76 | lr: 3.30e-04 | norm: 0.9944 | dt: 11692.6644ms | tok/sec: 33.6293\n",
      "step  236 | train loss: 5.39 | val loss: 5.32 | perplexity: 205.22 | lr: 3.31e-04 | norm: 0.5999 | dt: 11706.7332ms | tok/sec: 33.5889\n",
      "step  237 | train loss: 5.49 | val loss: 5.32 | perplexity: 205.32 | lr: 3.33e-04 | norm: 1.0703 | dt: 11711.7665ms | tok/sec: 33.5744\n",
      "step  238 | train loss: 5.68 | val loss: 5.34 | perplexity: 208.27 | lr: 3.34e-04 | norm: 0.8696 | dt: 11681.6144ms | tok/sec: 33.6611\n",
      "step  239 | train loss: 5.67 | val loss: 5.34 | perplexity: 209.05 | lr: 3.36e-04 | norm: 0.7596 | dt: 11661.3579ms | tok/sec: 33.7196\n",
      "step  240 | train loss: 5.75 | val loss: 5.35 | perplexity: 209.89 | lr: 3.37e-04 | norm: 0.8418 | dt: 11697.3534ms | tok/sec: 33.6158\n",
      "step  241 | train loss: 5.73 | val loss: 5.35 | perplexity: 210.38 | lr: 3.38e-04 | norm: 0.7834 | dt: 11695.5681ms | tok/sec: 33.6209\n",
      "step  242 | train loss: 5.53 | val loss: 5.35 | perplexity: 209.81 | lr: 3.40e-04 | norm: 0.8818 | dt: 11685.1659ms | tok/sec: 33.6509\n",
      "step  243 | train loss: 5.62 | val loss: 5.35 | perplexity: 211.40 | lr: 3.41e-04 | norm: 0.7630 | dt: 11683.0528ms | tok/sec: 33.6570\n",
      "step  244 | train loss: 5.42 | val loss: 5.33 | perplexity: 206.44 | lr: 3.43e-04 | norm: 0.8206 | dt: 11715.2874ms | tok/sec: 33.5643\n",
      "step  245 | train loss: 5.44 | val loss: 5.32 | perplexity: 204.84 | lr: 3.44e-04 | norm: 0.6507 | dt: 11700.7682ms | tok/sec: 33.6060\n",
      "step  246 | train loss: 5.91 | val loss: 5.35 | perplexity: 209.92 | lr: 3.45e-04 | norm: 0.7959 | dt: 11694.0372ms | tok/sec: 33.6253\n",
      "step  247 | train loss: 5.75 | val loss: 5.33 | perplexity: 207.46 | lr: 3.47e-04 | norm: 0.8978 | dt: 11703.0773ms | tok/sec: 33.5994\n",
      "step  248 | train loss: 5.44 | val loss: 5.31 | perplexity: 201.93 | lr: 3.48e-04 | norm: 0.6714 | dt: 11712.3120ms | tok/sec: 33.5729\n",
      "step  249 | train loss: 5.22 | val loss: 5.30 | perplexity: 200.00 | lr: 3.50e-04 | norm: 0.7751 | dt: 11639.3821ms | tok/sec: 33.7832\n",
      "step  250 | train loss: 5.60 | val loss: 5.29 | perplexity: 197.98 | lr: 3.51e-04 | norm: 0.6767 | dt: 11722.6813ms | tok/sec: 33.5432\n",
      "step  251 | train loss: 5.65 | val loss: 5.29 | perplexity: 199.05 | lr: 3.52e-04 | norm: 0.8434 | dt: 11715.6861ms | tok/sec: 33.5632\n",
      "step  252 | train loss: 5.59 | val loss: 5.30 | perplexity: 200.63 | lr: 3.54e-04 | norm: 0.5049 | dt: 11706.7881ms | tok/sec: 33.5887\n",
      "step  253 | train loss: 5.52 | val loss: 5.29 | perplexity: 199.11 | lr: 3.55e-04 | norm: 0.5431 | dt: 11733.8977ms | tok/sec: 33.5111\n",
      "step  254 | train loss: 5.68 | val loss: 5.28 | perplexity: 196.06 | lr: 3.57e-04 | norm: 0.6477 | dt: 11730.7050ms | tok/sec: 33.5202\n",
      "step  255 | train loss: 5.56 | val loss: 5.27 | perplexity: 194.10 | lr: 3.58e-04 | norm: 0.6710 | dt: 11707.7529ms | tok/sec: 33.5859\n",
      "step  256 | train loss: 5.44 | val loss: 5.26 | perplexity: 193.37 | lr: 3.59e-04 | norm: 0.6397 | dt: 11703.1691ms | tok/sec: 33.5991\n",
      "step  257 | train loss: 5.60 | val loss: 5.25 | perplexity: 191.05 | lr: 3.61e-04 | norm: 0.6979 | dt: 11699.3670ms | tok/sec: 33.6100\n",
      "step  258 | train loss: 5.74 | val loss: 5.26 | perplexity: 191.89 | lr: 3.62e-04 | norm: 0.5885 | dt: 11713.0082ms | tok/sec: 33.5709\n",
      "step  259 | train loss: 5.22 | val loss: 5.28 | perplexity: 196.08 | lr: 3.64e-04 | norm: 0.7826 | dt: 11688.8809ms | tok/sec: 33.6402\n",
      "step  260 | train loss: 5.21 | val loss: 5.28 | perplexity: 196.20 | lr: 3.65e-04 | norm: 0.7461 | dt: 11683.1326ms | tok/sec: 33.6567\n",
      "step  261 | train loss: 5.56 | val loss: 5.26 | perplexity: 193.37 | lr: 3.66e-04 | norm: 0.7603 | dt: 11724.9880ms | tok/sec: 33.5366\n",
      "step  262 | train loss: 5.42 | val loss: 5.27 | perplexity: 195.03 | lr: 3.68e-04 | norm: 0.5926 | dt: 11707.3879ms | tok/sec: 33.5870\n",
      "step  263 | train loss: 5.56 | val loss: 5.26 | perplexity: 192.82 | lr: 3.69e-04 | norm: 0.6316 | dt: 11705.9870ms | tok/sec: 33.5910\n",
      "step  264 | train loss: 5.70 | val loss: 5.26 | perplexity: 191.55 | lr: 3.71e-04 | norm: 0.5716 | dt: 11712.3358ms | tok/sec: 33.5728\n",
      "step  265 | train loss: 5.52 | val loss: 5.26 | perplexity: 192.12 | lr: 3.72e-04 | norm: 0.7693 | dt: 11708.7789ms | tok/sec: 33.5830\n",
      "step  266 | train loss: 5.36 | val loss: 5.27 | perplexity: 194.83 | lr: 3.73e-04 | norm: 0.8083 | dt: 11684.2170ms | tok/sec: 33.6536\n",
      "step  267 | train loss: 5.28 | val loss: 5.28 | perplexity: 196.26 | lr: 3.75e-04 | norm: 0.6973 | dt: 11685.6608ms | tok/sec: 33.6494\n",
      "step  268 | train loss: 5.29 | val loss: 5.26 | perplexity: 193.09 | lr: 3.76e-04 | norm: 0.8069 | dt: 11717.2656ms | tok/sec: 33.5587\n",
      "step  269 | train loss: 5.25 | val loss: 5.28 | perplexity: 196.13 | lr: 3.78e-04 | norm: 0.8024 | dt: 11718.2887ms | tok/sec: 33.5558\n",
      "step  270 | train loss: 5.57 | val loss: 5.26 | perplexity: 193.10 | lr: 3.79e-04 | norm: 0.9004 | dt: 11705.6332ms | tok/sec: 33.5920\n",
      "step  271 | train loss: 5.53 | val loss: 5.26 | perplexity: 193.21 | lr: 3.80e-04 | norm: 0.7000 | dt: 11713.3331ms | tok/sec: 33.5699\n",
      "step  272 | train loss: 5.30 | val loss: 5.27 | perplexity: 193.80 | lr: 3.82e-04 | norm: 0.7195 | dt: 11725.6861ms | tok/sec: 33.5346\n",
      "step  273 | train loss: 5.68 | val loss: 5.26 | perplexity: 193.29 | lr: 3.83e-04 | norm: 0.7396 | dt: 11704.4101ms | tok/sec: 33.5955\n",
      "step  274 | train loss: 5.78 | val loss: 5.27 | perplexity: 194.39 | lr: 3.85e-04 | norm: 0.7157 | dt: 11692.3857ms | tok/sec: 33.6301\n",
      "step  275 | train loss: 5.41 | val loss: 5.27 | perplexity: 194.65 | lr: 3.86e-04 | norm: 0.6369 | dt: 11700.1791ms | tok/sec: 33.6077\n",
      "step  276 | train loss: 5.74 | val loss: 5.25 | perplexity: 190.90 | lr: 3.87e-04 | norm: 0.8122 | dt: 11713.1977ms | tok/sec: 33.5703\n",
      "step  277 | train loss: 5.25 | val loss: 5.25 | perplexity: 191.39 | lr: 3.89e-04 | norm: 0.5575 | dt: 11700.3710ms | tok/sec: 33.6071\n",
      "step  278 | train loss: 5.51 | val loss: 5.25 | perplexity: 190.06 | lr: 3.90e-04 | norm: 0.8376 | dt: 11685.2160ms | tok/sec: 33.6507\n",
      "step  279 | train loss: 5.66 | val loss: 5.27 | perplexity: 194.76 | lr: 3.92e-04 | norm: 0.6670 | dt: 11683.4788ms | tok/sec: 33.6557\n",
      "step  280 | train loss: 5.36 | val loss: 5.27 | perplexity: 193.85 | lr: 3.93e-04 | norm: 0.7842 | dt: 11720.7384ms | tok/sec: 33.5487\n",
      "step  281 | train loss: 5.28 | val loss: 5.26 | perplexity: 191.69 | lr: 3.94e-04 | norm: 0.6165 | dt: 11716.2387ms | tok/sec: 33.5616\n",
      "step  282 | train loss: 5.23 | val loss: 5.25 | perplexity: 190.91 | lr: 3.96e-04 | norm: 0.7139 | dt: 11708.6430ms | tok/sec: 33.5834\n",
      "step  283 | train loss: 5.26 | val loss: 5.27 | perplexity: 195.09 | lr: 3.97e-04 | norm: 0.5724 | dt: 11727.0293ms | tok/sec: 33.5307\n",
      "step  284 | train loss: 5.48 | val loss: 5.30 | perplexity: 199.46 | lr: 3.99e-04 | norm: 0.5746 | dt: 11693.5441ms | tok/sec: 33.6268\n",
      "step  285 | train loss: 5.55 | val loss: 5.31 | perplexity: 202.21 | lr: 4.00e-04 | norm: 0.6226 | dt: 11674.9766ms | tok/sec: 33.6802\n",
      "step  286 | train loss: 5.48 | val loss: 5.30 | perplexity: 200.02 | lr: 4.01e-04 | norm: 0.7290 | dt: 11726.4557ms | tok/sec: 33.5324\n",
      "step  287 | train loss: 5.39 | val loss: 5.30 | perplexity: 200.24 | lr: 4.03e-04 | norm: 0.6185 | dt: 11711.0677ms | tok/sec: 33.5764\n",
      "step  288 | train loss: 5.75 | val loss: 5.30 | perplexity: 200.51 | lr: 4.04e-04 | norm: 1.0143 | dt: 11710.4475ms | tok/sec: 33.5782\n",
      "step  289 | train loss: 5.58 | val loss: 5.28 | perplexity: 196.14 | lr: 4.06e-04 | norm: 1.1245 | dt: 11726.0978ms | tok/sec: 33.5334\n",
      "step  290 | train loss: 5.06 | val loss: 5.27 | perplexity: 194.79 | lr: 4.07e-04 | norm: 0.7294 | dt: 11715.1515ms | tok/sec: 33.5647\n",
      "step  291 | train loss: 5.41 | val loss: 5.26 | perplexity: 193.12 | lr: 4.08e-04 | norm: 0.7562 | dt: 11689.2161ms | tok/sec: 33.6392\n",
      "step  292 | train loss: 6.14 | val loss: 5.28 | perplexity: 195.42 | lr: 4.10e-04 | norm: 1.5091 | dt: 11702.2302ms | tok/sec: 33.6018\n",
      "step  293 | train loss: 5.34 | val loss: 5.25 | perplexity: 191.06 | lr: 4.11e-04 | norm: 0.9573 | dt: 11718.1995ms | tok/sec: 33.5560\n",
      "step  294 | train loss: 5.51 | val loss: 5.29 | perplexity: 199.27 | lr: 4.13e-04 | norm: 0.9225 | dt: 11725.6069ms | tok/sec: 33.5348\n",
      "step  295 | train loss: 5.17 | val loss: 5.27 | perplexity: 195.03 | lr: 4.14e-04 | norm: 1.4505 | dt: 11703.9824ms | tok/sec: 33.5968\n",
      "step  296 | train loss: 5.34 | val loss: 5.27 | perplexity: 194.74 | lr: 4.15e-04 | norm: 0.6747 | dt: 11713.3920ms | tok/sec: 33.5698\n",
      "step  297 | train loss: 5.17 | val loss: 5.26 | perplexity: 193.36 | lr: 4.17e-04 | norm: 0.9795 | dt: 11718.7572ms | tok/sec: 33.5544\n",
      "step  298 | train loss: 5.34 | val loss: 5.27 | perplexity: 194.75 | lr: 4.18e-04 | norm: 0.5843 | dt: 11689.8317ms | tok/sec: 33.6374\n",
      "step  299 | train loss: 5.50 | val loss: 5.29 | perplexity: 197.45 | lr: 4.20e-04 | norm: 0.6064 | dt: 11697.5567ms | tok/sec: 33.6152\n",
      "step  300 | train loss: 5.96 | val loss: 5.29 | perplexity: 198.12 | lr: 4.21e-04 | norm: 0.6622 | dt: 11699.6667ms | tok/sec: 33.6092\n",
      "step  301 | train loss: 5.20 | val loss: 5.27 | perplexity: 195.35 | lr: 4.22e-04 | norm: 0.6318 | dt: 11728.8878ms | tok/sec: 33.5254\n",
      "step  302 | train loss: 5.32 | val loss: 5.27 | perplexity: 194.02 | lr: 4.24e-04 | norm: 0.6585 | dt: 11710.2458ms | tok/sec: 33.5788\n",
      "step  303 | train loss: 5.42 | val loss: 5.23 | perplexity: 185.98 | lr: 4.25e-04 | norm: 0.7376 | dt: 11690.2063ms | tok/sec: 33.6364\n",
      "step  304 | train loss: 5.57 | val loss: 5.20 | perplexity: 181.96 | lr: 4.27e-04 | norm: 0.6967 | dt: 11703.2101ms | tok/sec: 33.5990\n",
      "step  305 | train loss: 5.59 | val loss: 5.21 | perplexity: 182.93 | lr: 4.28e-04 | norm: 0.5980 | dt: 11713.4364ms | tok/sec: 33.5697\n",
      "step  306 | train loss: 5.40 | val loss: 5.20 | perplexity: 180.45 | lr: 4.29e-04 | norm: 0.7490 | dt: 11708.2641ms | tok/sec: 33.5845\n",
      "step  307 | train loss: 5.51 | val loss: 5.19 | perplexity: 180.14 | lr: 4.31e-04 | norm: 0.7139 | dt: 11681.5674ms | tok/sec: 33.6612\n",
      "step  308 | train loss: 5.36 | val loss: 5.20 | perplexity: 181.68 | lr: 4.32e-04 | norm: 0.4996 | dt: 11702.1070ms | tok/sec: 33.6022\n",
      "step  309 | train loss: 5.39 | val loss: 5.18 | perplexity: 178.09 | lr: 4.34e-04 | norm: 0.7478 | dt: 11729.3456ms | tok/sec: 33.5241\n",
      "step  310 | train loss: 5.63 | val loss: 5.19 | perplexity: 179.51 | lr: 4.35e-04 | norm: 0.6010 | dt: 11701.3700ms | tok/sec: 33.6043\n",
      "step  311 | train loss: 5.27 | val loss: 5.20 | perplexity: 180.67 | lr: 4.36e-04 | norm: 0.6883 | dt: 11722.3394ms | tok/sec: 33.5442\n",
      "step  312 | train loss: 5.23 | val loss: 5.20 | perplexity: 181.54 | lr: 4.38e-04 | norm: 0.6451 | dt: 11723.6245ms | tok/sec: 33.5405\n",
      "step  313 | train loss: 5.39 | val loss: 5.19 | perplexity: 178.87 | lr: 4.39e-04 | norm: 0.5877 | dt: 11721.0674ms | tok/sec: 33.5478\n",
      "step  314 | train loss: 5.27 | val loss: 5.18 | perplexity: 177.97 | lr: 4.41e-04 | norm: 0.5204 | dt: 11725.9188ms | tok/sec: 33.5339\n",
      "step  315 | train loss: 5.07 | val loss: 5.18 | perplexity: 177.81 | lr: 4.42e-04 | norm: 0.7805 | dt: 11722.3220ms | tok/sec: 33.5442\n",
      "step  316 | train loss: 5.51 | val loss: 5.18 | perplexity: 178.38 | lr: 4.43e-04 | norm: 0.5890 | dt: 11738.8833ms | tok/sec: 33.4969\n",
      "step  317 | train loss: 5.27 | val loss: 5.18 | perplexity: 178.56 | lr: 4.45e-04 | norm: 0.5441 | dt: 11722.7485ms | tok/sec: 33.5430\n",
      "step  318 | train loss: 5.42 | val loss: 5.19 | perplexity: 178.62 | lr: 4.46e-04 | norm: 0.6243 | dt: 11719.8465ms | tok/sec: 33.5513\n",
      "step  319 | train loss: 5.19 | val loss: 5.18 | perplexity: 178.28 | lr: 4.48e-04 | norm: 0.6140 | dt: 11716.4416ms | tok/sec: 33.5610\n",
      "step  320 | train loss: 5.25 | val loss: 5.19 | perplexity: 178.97 | lr: 4.49e-04 | norm: 0.6064 | dt: 11726.4662ms | tok/sec: 33.5324\n",
      "step  321 | train loss: 5.53 | val loss: 5.18 | perplexity: 177.95 | lr: 4.50e-04 | norm: 0.5227 | dt: 11714.5295ms | tok/sec: 33.5665\n",
      "step  322 | train loss: 5.33 | val loss: 5.18 | perplexity: 177.76 | lr: 4.52e-04 | norm: 0.4685 | dt: 11726.4626ms | tok/sec: 33.5324\n",
      "step  323 | train loss: 5.34 | val loss: 5.18 | perplexity: 177.66 | lr: 4.53e-04 | norm: 0.5637 | dt: 11707.0265ms | tok/sec: 33.5880\n",
      "step  324 | train loss: 5.17 | val loss: 5.17 | perplexity: 176.62 | lr: 4.55e-04 | norm: 0.6323 | dt: 11712.9903ms | tok/sec: 33.5709\n",
      "step  325 | train loss: 5.60 | val loss: 5.19 | perplexity: 178.80 | lr: 4.56e-04 | norm: 0.5552 | dt: 11710.6144ms | tok/sec: 33.5777\n",
      "step  326 | train loss: 5.71 | val loss: 5.19 | perplexity: 178.58 | lr: 4.57e-04 | norm: 0.7539 | dt: 11739.5301ms | tok/sec: 33.4950\n",
      "step  327 | train loss: 5.23 | val loss: 5.19 | perplexity: 178.78 | lr: 4.59e-04 | norm: 0.6806 | dt: 11765.3511ms | tok/sec: 33.4215\n",
      "step  328 | train loss: 5.04 | val loss: 5.19 | perplexity: 180.20 | lr: 4.60e-04 | norm: 0.7938 | dt: 11728.2467ms | tok/sec: 33.5273\n",
      "step  329 | train loss: 5.08 | val loss: 5.18 | perplexity: 176.88 | lr: 4.62e-04 | norm: 0.6496 | dt: 11726.7196ms | tok/sec: 33.5316\n",
      "step  330 | train loss: 5.24 | val loss: 5.17 | perplexity: 176.32 | lr: 4.63e-04 | norm: 0.5512 | dt: 11696.7916ms | tok/sec: 33.6174\n",
      "step  331 | train loss: 5.49 | val loss: 5.16 | perplexity: 174.32 | lr: 4.64e-04 | norm: 0.5605 | dt: 11732.9323ms | tok/sec: 33.5139\n",
      "step  332 | train loss: 5.46 | val loss: 5.15 | perplexity: 172.70 | lr: 4.66e-04 | norm: 0.7457 | dt: 11733.8300ms | tok/sec: 33.5113\n",
      "step  333 | train loss: 5.37 | val loss: 5.17 | perplexity: 175.24 | lr: 4.67e-04 | norm: 0.5787 | dt: 11702.0943ms | tok/sec: 33.6022\n",
      "step  334 | train loss: 5.59 | val loss: 5.15 | perplexity: 172.88 | lr: 4.69e-04 | norm: 0.8232 | dt: 11724.3860ms | tok/sec: 33.5383\n",
      "step  335 | train loss: 5.68 | val loss: 5.15 | perplexity: 172.93 | lr: 4.70e-04 | norm: 0.6859 | dt: 11721.9245ms | tok/sec: 33.5453\n",
      "step  336 | train loss: 5.19 | val loss: 5.16 | perplexity: 174.10 | lr: 4.71e-04 | norm: 0.6392 | dt: 11723.6564ms | tok/sec: 33.5404\n",
      "step  337 | train loss: 5.02 | val loss: 5.17 | perplexity: 176.66 | lr: 4.73e-04 | norm: 0.8910 | dt: 11721.1878ms | tok/sec: 33.5475\n",
      "step  338 | train loss: 5.01 | val loss: 5.15 | perplexity: 172.15 | lr: 4.74e-04 | norm: 0.8654 | dt: 11723.0196ms | tok/sec: 33.5422\n",
      "step  339 | train loss: 5.49 | val loss: 5.16 | perplexity: 174.64 | lr: 4.76e-04 | norm: 0.9640 | dt: 11738.2951ms | tok/sec: 33.4986\n",
      "step  340 | train loss: 5.48 | val loss: 5.18 | perplexity: 178.23 | lr: 4.77e-04 | norm: 0.6260 | dt: 11724.0493ms | tok/sec: 33.5393\n",
      "step  341 | train loss: 5.08 | val loss: 5.17 | perplexity: 176.05 | lr: 4.78e-04 | norm: 0.8165 | dt: 11696.4900ms | tok/sec: 33.6183\n",
      "step  342 | train loss: 5.54 | val loss: 5.17 | perplexity: 175.27 | lr: 4.80e-04 | norm: 0.7659 | dt: 11702.6012ms | tok/sec: 33.6007\n",
      "step  343 | train loss: 5.55 | val loss: 5.16 | perplexity: 174.43 | lr: 4.81e-04 | norm: 0.4821 | dt: 11706.2695ms | tok/sec: 33.5902\n",
      "step  344 | train loss: 5.16 | val loss: 5.15 | perplexity: 172.20 | lr: 4.83e-04 | norm: 0.6804 | dt: 11688.8549ms | tok/sec: 33.6402\n",
      "step  345 | train loss: 5.43 | val loss: 5.14 | perplexity: 171.52 | lr: 4.84e-04 | norm: 0.5404 | dt: 11718.7703ms | tok/sec: 33.5544\n",
      "step  346 | train loss: 5.38 | val loss: 5.13 | perplexity: 169.71 | lr: 4.85e-04 | norm: 0.5493 | dt: 11740.3178ms | tok/sec: 33.4928\n",
      "step  347 | train loss: 5.30 | val loss: 5.14 | perplexity: 170.73 | lr: 4.87e-04 | norm: 0.4732 | dt: 11710.8612ms | tok/sec: 33.5770\n",
      "step  348 | train loss: 5.27 | val loss: 5.14 | perplexity: 171.52 | lr: 4.88e-04 | norm: 0.4981 | dt: 11716.8643ms | tok/sec: 33.5598\n",
      "step  349 | train loss: 5.16 | val loss: 5.17 | perplexity: 175.86 | lr: 4.90e-04 | norm: 0.4674 | dt: 11719.6510ms | tok/sec: 33.5519\n",
      "step  350 | train loss: 5.46 | val loss: 5.16 | perplexity: 173.65 | lr: 4.91e-04 | norm: 0.6184 | dt: 11723.8481ms | tok/sec: 33.5398\n",
      "step  351 | train loss: 5.34 | val loss: 5.15 | perplexity: 172.02 | lr: 4.92e-04 | norm: 0.7101 | dt: 11720.8245ms | tok/sec: 33.5485\n",
      "step  352 | train loss: 5.42 | val loss: 5.14 | perplexity: 171.06 | lr: 4.94e-04 | norm: 0.5586 | dt: 11732.7750ms | tok/sec: 33.5143\n",
      "step  353 | train loss: 5.42 | val loss: 5.13 | perplexity: 169.62 | lr: 4.95e-04 | norm: 0.5389 | dt: 11704.2770ms | tok/sec: 33.5959\n",
      "step  354 | train loss: 5.17 | val loss: 5.13 | perplexity: 168.64 | lr: 4.97e-04 | norm: 0.6142 | dt: 11713.1417ms | tok/sec: 33.5705\n",
      "step  355 | train loss: 5.27 | val loss: 5.15 | perplexity: 171.96 | lr: 4.98e-04 | norm: 0.4491 | dt: 11727.7532ms | tok/sec: 33.5287\n",
      "step  356 | train loss: 5.14 | val loss: 5.15 | perplexity: 171.64 | lr: 4.99e-04 | norm: 0.5543 | dt: 11701.6175ms | tok/sec: 33.6036\n",
      "step  357 | train loss: 5.44 | val loss: 5.15 | perplexity: 172.16 | lr: 5.01e-04 | norm: 0.4789 | dt: 11712.9755ms | tok/sec: 33.5710\n",
      "step  358 | train loss: 5.41 | val loss: 5.14 | perplexity: 171.12 | lr: 5.02e-04 | norm: 0.6157 | dt: 11682.2131ms | tok/sec: 33.6594\n",
      "step  359 | train loss: 5.57 | val loss: 5.14 | perplexity: 171.49 | lr: 5.03e-04 | norm: 0.5830 | dt: 11720.4757ms | tok/sec: 33.5495\n",
      "step  360 | train loss: 5.43 | val loss: 5.16 | perplexity: 174.60 | lr: 5.05e-04 | norm: 0.5830 | dt: 11705.4379ms | tok/sec: 33.5926\n",
      "step  361 | train loss: 5.49 | val loss: 5.16 | perplexity: 173.92 | lr: 5.06e-04 | norm: 0.5327 | dt: 11688.5602ms | tok/sec: 33.6411\n",
      "step  362 | train loss: 5.13 | val loss: 5.15 | perplexity: 172.55 | lr: 5.08e-04 | norm: 0.6459 | dt: 11703.8383ms | tok/sec: 33.5972\n",
      "step  363 | train loss: 5.16 | val loss: 5.13 | perplexity: 168.98 | lr: 5.09e-04 | norm: 0.6732 | dt: 11695.2448ms | tok/sec: 33.6219\n",
      "step  364 | train loss: 5.58 | val loss: 5.14 | perplexity: 170.99 | lr: 5.10e-04 | norm: 0.6020 | dt: 11686.6665ms | tok/sec: 33.6465\n",
      "step  365 | train loss: 5.51 | val loss: 5.12 | perplexity: 166.97 | lr: 5.12e-04 | norm: 0.6602 | dt: 11687.2876ms | tok/sec: 33.6448\n",
      "step  366 | train loss: 5.78 | val loss: 5.13 | perplexity: 168.40 | lr: 5.13e-04 | norm: 0.5510 | dt: 11700.7914ms | tok/sec: 33.6059\n",
      "step  367 | train loss: 5.65 | val loss: 5.14 | perplexity: 170.25 | lr: 5.15e-04 | norm: 0.5587 | dt: 11705.4255ms | tok/sec: 33.5926\n",
      "step  368 | train loss: 5.62 | val loss: 5.15 | perplexity: 172.05 | lr: 5.16e-04 | norm: 0.5311 | dt: 11729.4536ms | tok/sec: 33.5238\n",
      "step  369 | train loss: 5.47 | val loss: 5.15 | perplexity: 171.94 | lr: 5.17e-04 | norm: 0.5281 | dt: 11667.1004ms | tok/sec: 33.7030\n",
      "step  370 | train loss: 5.39 | val loss: 5.15 | perplexity: 171.83 | lr: 5.19e-04 | norm: 0.5626 | dt: 11711.1905ms | tok/sec: 33.5761\n",
      "step  371 | train loss: 5.53 | val loss: 5.16 | perplexity: 174.13 | lr: 5.20e-04 | norm: 0.5099 | dt: 11690.4137ms | tok/sec: 33.6358\n",
      "step  372 | train loss: 5.35 | val loss: 5.14 | perplexity: 171.48 | lr: 5.22e-04 | norm: 0.6207 | dt: 11695.2717ms | tok/sec: 33.6218\n",
      "step  373 | train loss: 5.25 | val loss: 5.14 | perplexity: 170.04 | lr: 5.23e-04 | norm: 0.4642 | dt: 11716.8112ms | tok/sec: 33.5600\n",
      "step  374 | train loss: 5.66 | val loss: 5.16 | perplexity: 173.62 | lr: 5.24e-04 | norm: 0.6162 | dt: 11711.9572ms | tok/sec: 33.5739\n",
      "step  375 | train loss: 5.25 | val loss: 5.16 | perplexity: 173.76 | lr: 5.26e-04 | norm: 0.6947 | dt: 11714.5870ms | tok/sec: 33.5664\n",
      "step  376 | train loss: 5.51 | val loss: 5.14 | perplexity: 170.24 | lr: 5.27e-04 | norm: 0.8019 | dt: 11694.8218ms | tok/sec: 33.6231\n",
      "step  377 | train loss: 5.42 | val loss: 5.14 | perplexity: 171.08 | lr: 5.29e-04 | norm: 0.5392 | dt: 11725.0919ms | tok/sec: 33.5363\n",
      "step  378 | train loss: 5.20 | val loss: 5.14 | perplexity: 170.45 | lr: 5.30e-04 | norm: 0.6218 | dt: 11704.7832ms | tok/sec: 33.5945\n",
      "step  379 | train loss: 5.23 | val loss: 5.14 | perplexity: 170.61 | lr: 5.31e-04 | norm: 0.5889 | dt: 11716.5387ms | tok/sec: 33.5608\n",
      "step  380 | train loss: 5.39 | val loss: 5.13 | perplexity: 169.70 | lr: 5.33e-04 | norm: 0.6473 | dt: 11697.6690ms | tok/sec: 33.6149\n",
      "step  381 | train loss: 5.02 | val loss: 5.13 | perplexity: 168.69 | lr: 5.34e-04 | norm: 0.6367 | dt: 11700.6686ms | tok/sec: 33.6063\n",
      "step  382 | train loss: 5.37 | val loss: 5.14 | perplexity: 171.13 | lr: 5.36e-04 | norm: 0.6501 | dt: 11714.3323ms | tok/sec: 33.5671\n",
      "step  383 | train loss: 5.54 | val loss: 5.15 | perplexity: 172.39 | lr: 5.37e-04 | norm: 0.7644 | dt: 11703.4554ms | tok/sec: 33.5983\n",
      "step  384 | train loss: 5.35 | val loss: 5.15 | perplexity: 172.38 | lr: 5.38e-04 | norm: 0.5887 | dt: 11722.2881ms | tok/sec: 33.5443\n",
      "step  385 | train loss: 5.20 | val loss: 5.14 | perplexity: 170.30 | lr: 5.40e-04 | norm: 0.5618 | dt: 11716.7447ms | tok/sec: 33.5602\n",
      "step  386 | train loss: 5.20 | val loss: 5.13 | perplexity: 169.33 | lr: 5.41e-04 | norm: 0.5535 | dt: 11710.1977ms | tok/sec: 33.5789\n",
      "step  387 | train loss: 5.27 | val loss: 5.12 | perplexity: 167.77 | lr: 5.43e-04 | norm: 0.5212 | dt: 11737.0312ms | tok/sec: 33.5022\n",
      "step  388 | train loss: 5.21 | val loss: 5.12 | perplexity: 167.45 | lr: 5.44e-04 | norm: 0.5485 | dt: 11711.4391ms | tok/sec: 33.5754\n",
      "step  389 | train loss: 5.14 | val loss: 5.12 | perplexity: 167.60 | lr: 5.45e-04 | norm: 0.6241 | dt: 11712.2962ms | tok/sec: 33.5729\n",
      "step  390 | train loss: 5.35 | val loss: 5.11 | perplexity: 166.35 | lr: 5.47e-04 | norm: 0.6074 | dt: 11696.7037ms | tok/sec: 33.6177\n",
      "step  391 | train loss: 5.30 | val loss: 5.12 | perplexity: 168.03 | lr: 5.48e-04 | norm: 0.4528 | dt: 11690.5367ms | tok/sec: 33.6354\n",
      "step  392 | train loss: 5.02 | val loss: 5.09 | perplexity: 162.48 | lr: 5.50e-04 | norm: 0.7450 | dt: 11687.9785ms | tok/sec: 33.6428\n",
      "step  393 | train loss: 5.29 | val loss: 5.10 | perplexity: 163.26 | lr: 5.51e-04 | norm: 0.5310 | dt: 11672.4548ms | tok/sec: 33.6875\n",
      "step  394 | train loss: 5.16 | val loss: 5.12 | perplexity: 167.95 | lr: 5.52e-04 | norm: 0.6841 | dt: 11687.7730ms | tok/sec: 33.6434\n",
      "step  395 | train loss: 5.45 | val loss: 5.13 | perplexity: 168.78 | lr: 5.54e-04 | norm: 0.4910 | dt: 11723.7236ms | tok/sec: 33.5402\n",
      "step  396 | train loss: 5.82 | val loss: 5.13 | perplexity: 169.14 | lr: 5.55e-04 | norm: 1.2091 | dt: 11659.2925ms | tok/sec: 33.7255\n",
      "step  397 | train loss: 5.10 | val loss: 5.09 | perplexity: 163.18 | lr: 5.57e-04 | norm: 0.7838 | dt: 11699.3530ms | tok/sec: 33.6101\n",
      "step  398 | train loss: 5.45 | val loss: 5.11 | perplexity: 165.21 | lr: 5.58e-04 | norm: 0.6993 | dt: 11704.5250ms | tok/sec: 33.5952\n",
      "step  399 | train loss: 5.74 | val loss: 5.11 | perplexity: 166.41 | lr: 5.59e-04 | norm: 1.0160 | dt: 11716.5847ms | tok/sec: 33.5606\n",
      "step  400 | train loss: 5.48 | val loss: 5.10 | perplexity: 164.53 | lr: 5.61e-04 | norm: 0.6911 | dt: 11684.9995ms | tok/sec: 33.6513\n",
      "step  401 | train loss: 5.23 | val loss: 5.10 | perplexity: 163.57 | lr: 5.62e-04 | norm: 0.5756 | dt: 11702.5049ms | tok/sec: 33.6010\n",
      "step  402 | train loss: 5.02 | val loss: 5.10 | perplexity: 163.76 | lr: 5.64e-04 | norm: 0.5335 | dt: 11714.6533ms | tok/sec: 33.5662\n",
      "step  403 | train loss: 5.20 | val loss: 5.09 | perplexity: 162.58 | lr: 5.65e-04 | norm: 0.6160 | dt: 11711.1468ms | tok/sec: 33.5762\n",
      "step  404 | train loss: 5.34 | val loss: 5.10 | perplexity: 163.32 | lr: 5.66e-04 | norm: 0.5639 | dt: 11674.1946ms | tok/sec: 33.6825\n",
      "step  405 | train loss: 5.37 | val loss: 5.10 | perplexity: 164.40 | lr: 5.68e-04 | norm: 0.6564 | dt: 11687.0756ms | tok/sec: 33.6454\n",
      "step  406 | train loss: 5.08 | val loss: 5.09 | perplexity: 163.04 | lr: 5.69e-04 | norm: 0.5838 | dt: 11683.6596ms | tok/sec: 33.6552\n",
      "step  407 | train loss: 5.17 | val loss: 5.11 | perplexity: 165.48 | lr: 5.71e-04 | norm: 0.5721 | dt: 11709.9679ms | tok/sec: 33.5796\n",
      "step  408 | train loss: 5.21 | val loss: 5.09 | perplexity: 162.22 | lr: 5.72e-04 | norm: 0.6213 | dt: 11715.8885ms | tok/sec: 33.5626\n",
      "step  409 | train loss: 5.33 | val loss: 5.07 | perplexity: 159.08 | lr: 5.73e-04 | norm: 0.5802 | dt: 11712.3041ms | tok/sec: 33.5729\n",
      "step  410 | train loss: 5.23 | val loss: 5.08 | perplexity: 160.89 | lr: 5.75e-04 | norm: 0.5766 | dt: 11733.0112ms | tok/sec: 33.5136\n",
      "step  411 | train loss: 5.33 | val loss: 5.09 | perplexity: 162.07 | lr: 5.76e-04 | norm: 0.6334 | dt: 11704.5634ms | tok/sec: 33.5951\n",
      "step  412 | train loss: 5.47 | val loss: 5.10 | perplexity: 163.85 | lr: 5.78e-04 | norm: 0.6734 | dt: 11730.8161ms | tok/sec: 33.5199\n",
      "step  413 | train loss: 5.09 | val loss: 5.08 | perplexity: 160.46 | lr: 5.79e-04 | norm: 0.8358 | dt: 11687.8481ms | tok/sec: 33.6431\n",
      "step  414 | train loss: 5.30 | val loss: 5.08 | perplexity: 160.37 | lr: 5.80e-04 | norm: 0.4402 | dt: 11686.5938ms | tok/sec: 33.6468\n",
      "step  415 | train loss: 5.01 | val loss: 5.09 | perplexity: 161.70 | lr: 5.82e-04 | norm: 0.4882 | dt: 11707.0773ms | tok/sec: 33.5879\n",
      "step  416 | train loss: 5.21 | val loss: 5.07 | perplexity: 159.50 | lr: 5.83e-04 | norm: 0.5912 | dt: 11705.7829ms | tok/sec: 33.5916\n",
      "step  417 | train loss: 5.51 | val loss: 5.07 | perplexity: 159.53 | lr: 5.85e-04 | norm: 0.5609 | dt: 11665.5514ms | tok/sec: 33.7075\n",
      "step  418 | train loss: 5.48 | val loss: 5.08 | perplexity: 160.86 | lr: 5.86e-04 | norm: 0.7200 | dt: 11745.2395ms | tok/sec: 33.4788\n",
      "step  419 | train loss: 5.11 | val loss: 5.08 | perplexity: 160.83 | lr: 5.87e-04 | norm: 0.7344 | dt: 11729.5909ms | tok/sec: 33.5234\n",
      "step  420 | train loss: 5.14 | val loss: 5.07 | perplexity: 158.98 | lr: 5.89e-04 | norm: 0.6552 | dt: 11696.9640ms | tok/sec: 33.6169\n",
      "step  421 | train loss: 5.40 | val loss: 5.08 | perplexity: 161.31 | lr: 5.90e-04 | norm: 0.4958 | dt: 11718.3638ms | tok/sec: 33.5555\n",
      "step  422 | train loss: 5.34 | val loss: 5.06 | perplexity: 156.85 | lr: 5.92e-04 | norm: 0.6837 | dt: 11716.3370ms | tok/sec: 33.5613\n",
      "step  423 | train loss: 5.26 | val loss: 5.06 | perplexity: 158.30 | lr: 5.93e-04 | norm: 0.5336 | dt: 11704.4375ms | tok/sec: 33.5955\n",
      "step  424 | train loss: 5.19 | val loss: 5.08 | perplexity: 160.53 | lr: 5.94e-04 | norm: 0.5394 | dt: 11688.0865ms | tok/sec: 33.6425\n",
      "step  425 | train loss: 5.31 | val loss: 5.08 | perplexity: 160.23 | lr: 5.96e-04 | norm: 0.5952 | dt: 11706.9433ms | tok/sec: 33.5883\n",
      "step  426 | train loss: 5.34 | val loss: 5.07 | perplexity: 158.52 | lr: 5.97e-04 | norm: 0.6033 | dt: 11667.9153ms | tok/sec: 33.7006\n",
      "step  427 | train loss: 5.35 | val loss: 5.07 | perplexity: 158.72 | lr: 5.99e-04 | norm: 0.5483 | dt: 11727.0012ms | tok/sec: 33.5308\n",
      "step  428 | train loss: 5.47 | val loss: 5.09 | perplexity: 162.62 | lr: 6.00e-04 | norm: 0.4467 | dt: 11701.3912ms | tok/sec: 33.6042\n",
      "step  429 | train loss: 5.21 | val loss: 5.10 | perplexity: 163.85 | lr: 6.01e-04 | norm: 0.5087 | dt: 11709.7893ms | tok/sec: 33.5801\n",
      "step  430 | train loss: 5.49 | val loss: 5.09 | perplexity: 162.38 | lr: 6.03e-04 | norm: 0.5175 | dt: 11694.6337ms | tok/sec: 33.6236\n",
      "step  431 | train loss: 5.20 | val loss: 5.07 | perplexity: 159.52 | lr: 6.04e-04 | norm: 0.4475 | dt: 11705.6592ms | tok/sec: 33.5920\n",
      "step  432 | train loss: 5.13 | val loss: 5.06 | perplexity: 157.28 | lr: 6.06e-04 | norm: 0.4595 | dt: 11727.3328ms | tok/sec: 33.5299\n",
      "step  433 | train loss: 5.34 | val loss: 5.06 | perplexity: 157.57 | lr: 6.07e-04 | norm: 0.3709 | dt: 11735.1296ms | tok/sec: 33.5076\n",
      "step  434 | train loss: 5.23 | val loss: 5.06 | perplexity: 158.14 | lr: 6.08e-04 | norm: 0.4640 | dt: 11733.2385ms | tok/sec: 33.5130\n",
      "step  435 | train loss: 5.36 | val loss: 5.07 | perplexity: 159.79 | lr: 6.10e-04 | norm: 0.3936 | dt: 11701.3853ms | tok/sec: 33.6042\n",
      "step  436 | train loss: 5.45 | val loss: 5.09 | perplexity: 163.04 | lr: 6.11e-04 | norm: 0.4706 | dt: 11680.7597ms | tok/sec: 33.6636\n",
      "step  437 | train loss: 5.20 | val loss: 5.10 | perplexity: 163.57 | lr: 6.13e-04 | norm: 0.5234 | dt: 11721.2913ms | tok/sec: 33.5472\n",
      "step  438 | train loss: 5.21 | val loss: 5.10 | perplexity: 163.64 | lr: 6.14e-04 | norm: 0.5699 | dt: 11713.8228ms | tok/sec: 33.5685\n",
      "step  439 | train loss: 5.10 | val loss: 5.10 | perplexity: 163.40 | lr: 6.15e-04 | norm: 0.4766 | dt: 11710.3894ms | tok/sec: 33.5784\n",
      "step  440 | train loss: 5.24 | val loss: 5.09 | perplexity: 162.24 | lr: 6.17e-04 | norm: 0.5527 | dt: 11714.7837ms | tok/sec: 33.5658\n",
      "step  441 | train loss: 5.00 | val loss: 5.08 | perplexity: 160.59 | lr: 6.18e-04 | norm: 0.5723 | dt: 11754.6217ms | tok/sec: 33.4520\n",
      "step  442 | train loss: 4.98 | val loss: 5.06 | perplexity: 157.21 | lr: 6.20e-04 | norm: 0.4516 | dt: 11731.2953ms | tok/sec: 33.5185\n",
      "step  443 | train loss: 5.29 | val loss: 5.06 | perplexity: 156.94 | lr: 6.21e-04 | norm: 0.5619 | dt: 11709.5604ms | tok/sec: 33.5808\n",
      "step  444 | train loss: 5.13 | val loss: 5.05 | perplexity: 156.09 | lr: 6.22e-04 | norm: 0.6359 | dt: 11719.7309ms | tok/sec: 33.5516\n",
      "step  445 | train loss: 5.24 | val loss: 5.03 | perplexity: 153.51 | lr: 6.24e-04 | norm: 0.4823 | dt: 11728.6634ms | tok/sec: 33.5261\n",
      "step  446 | train loss: 5.44 | val loss: 5.04 | perplexity: 155.19 | lr: 6.25e-04 | norm: 0.5583 | dt: 11734.1785ms | tok/sec: 33.5103\n",
      "step  447 | train loss: 5.54 | val loss: 5.05 | perplexity: 155.72 | lr: 6.27e-04 | norm: 0.5407 | dt: 11711.9696ms | tok/sec: 33.5739\n",
      "step  448 | train loss: 5.40 | val loss: 5.07 | perplexity: 158.49 | lr: 6.28e-04 | norm: 0.4919 | dt: 11721.8776ms | tok/sec: 33.5455\n",
      "step  449 | train loss: 5.42 | val loss: 5.06 | perplexity: 158.26 | lr: 6.29e-04 | norm: 0.7624 | dt: 11727.9787ms | tok/sec: 33.5280\n",
      "step  450 | train loss: 5.19 | val loss: 5.05 | perplexity: 156.14 | lr: 6.31e-04 | norm: 0.5833 | dt: 11716.0640ms | tok/sec: 33.5621\n",
      "step  451 | train loss: 5.09 | val loss: 5.06 | perplexity: 157.65 | lr: 6.32e-04 | norm: 0.5970 | dt: 11731.5080ms | tok/sec: 33.5179\n",
      "step  452 | train loss: 5.22 | val loss: 5.05 | perplexity: 155.62 | lr: 6.34e-04 | norm: 0.6344 | dt: 11714.1089ms | tok/sec: 33.5677\n",
      "step  453 | train loss: 5.40 | val loss: 5.04 | perplexity: 154.58 | lr: 6.35e-04 | norm: 0.5870 | dt: 11693.5363ms | tok/sec: 33.6268\n",
      "step  454 | train loss: 5.11 | val loss: 5.03 | perplexity: 152.34 | lr: 6.36e-04 | norm: 0.6330 | dt: 11710.3112ms | tok/sec: 33.5786\n",
      "step  455 | train loss: 5.51 | val loss: 5.04 | perplexity: 154.90 | lr: 6.38e-04 | norm: 0.7141 | dt: 11736.3248ms | tok/sec: 33.5042\n",
      "step  456 | train loss: 5.14 | val loss: 5.05 | perplexity: 155.52 | lr: 6.39e-04 | norm: 1.5982 | dt: 11714.1876ms | tok/sec: 33.5675\n",
      "step  457 | train loss: 5.47 | val loss: 5.06 | perplexity: 156.83 | lr: 6.41e-04 | norm: 0.6161 | dt: 11734.6148ms | tok/sec: 33.5091\n",
      "step  458 | train loss: 5.30 | val loss: 5.05 | perplexity: 156.43 | lr: 6.42e-04 | norm: 0.6404 | dt: 11737.7033ms | tok/sec: 33.5003\n",
      "step  459 | train loss: 5.03 | val loss: 5.08 | perplexity: 160.36 | lr: 6.43e-04 | norm: 0.7721 | dt: 11721.3013ms | tok/sec: 33.5471\n",
      "step  460 | train loss: 5.32 | val loss: 5.07 | perplexity: 159.69 | lr: 6.45e-04 | norm: 0.4824 | dt: 11707.5114ms | tok/sec: 33.5866\n",
      "step  461 | train loss: 5.31 | val loss: 5.07 | perplexity: 159.15 | lr: 6.46e-04 | norm: 0.5286 | dt: 11726.2368ms | tok/sec: 33.5330\n",
      "step  462 | train loss: 5.30 | val loss: 5.06 | perplexity: 158.31 | lr: 6.48e-04 | norm: 0.5936 | dt: 11721.1013ms | tok/sec: 33.5477\n",
      "step  463 | train loss: 5.30 | val loss: 5.07 | perplexity: 159.20 | lr: 6.49e-04 | norm: 0.5073 | dt: 11736.0303ms | tok/sec: 33.5050\n",
      "step  464 | train loss: 5.35 | val loss: 5.06 | perplexity: 158.09 | lr: 6.50e-04 | norm: 0.4934 | dt: 11726.5933ms | tok/sec: 33.5320\n",
      "step  465 | train loss: 5.13 | val loss: 5.06 | perplexity: 156.89 | lr: 6.52e-04 | norm: 0.6504 | dt: 11731.4947ms | tok/sec: 33.5180\n",
      "step  466 | train loss: 5.30 | val loss: 5.05 | perplexity: 156.22 | lr: 6.53e-04 | norm: 0.7639 | dt: 11671.7076ms | tok/sec: 33.6897\n",
      "step  467 | train loss: 5.30 | val loss: 5.04 | perplexity: 154.67 | lr: 6.55e-04 | norm: 0.5035 | dt: 11723.8913ms | tok/sec: 33.5397\n",
      "step  468 | train loss: 5.28 | val loss: 5.03 | perplexity: 153.37 | lr: 6.56e-04 | norm: 0.4760 | dt: 11701.9169ms | tok/sec: 33.6027\n",
      "step  469 | train loss: 5.54 | val loss: 5.04 | perplexity: 154.88 | lr: 6.57e-04 | norm: 0.5621 | dt: 11720.0885ms | tok/sec: 33.5506\n",
      "step  470 | train loss: 5.28 | val loss: 5.07 | perplexity: 159.20 | lr: 6.59e-04 | norm: 0.5785 | dt: 11702.4393ms | tok/sec: 33.6012\n",
      "step  471 | train loss: 4.92 | val loss: 5.06 | perplexity: 157.13 | lr: 6.60e-04 | norm: 0.7763 | dt: 11709.0957ms | tok/sec: 33.5821\n",
      "step  472 | train loss: 5.18 | val loss: 5.04 | perplexity: 154.26 | lr: 6.62e-04 | norm: 0.6561 | dt: 11715.9109ms | tok/sec: 33.5626\n",
      "step  473 | train loss: 5.19 | val loss: 5.02 | perplexity: 151.81 | lr: 6.63e-04 | norm: 0.7240 | dt: 11758.4293ms | tok/sec: 33.4412\n",
      "step  474 | train loss: 5.21 | val loss: 5.02 | perplexity: 151.08 | lr: 6.64e-04 | norm: 0.5267 | dt: 11694.5612ms | tok/sec: 33.6238\n",
      "step  475 | train loss: 5.34 | val loss: 5.02 | perplexity: 151.07 | lr: 6.66e-04 | norm: 0.5439 | dt: 11721.3392ms | tok/sec: 33.5470\n",
      "step  476 | train loss: 5.15 | val loss: 5.02 | perplexity: 151.80 | lr: 6.67e-04 | norm: 0.5231 | dt: 11763.3288ms | tok/sec: 33.4273\n",
      "step  477 | train loss: 5.44 | val loss: 5.03 | perplexity: 153.24 | lr: 6.69e-04 | norm: 0.5250 | dt: 11684.5453ms | tok/sec: 33.6527\n",
      "step  478 | train loss: 5.34 | val loss: 5.03 | perplexity: 153.13 | lr: 6.70e-04 | norm: 0.5125 | dt: 11707.7184ms | tok/sec: 33.5860\n",
      "step  479 | train loss: 6.06 | val loss: 5.14 | perplexity: 170.48 | lr: 6.71e-04 | norm: 1.0012 | dt: 11712.5793ms | tok/sec: 33.5721\n",
      "step  480 | train loss: 5.43 | val loss: 5.06 | perplexity: 157.97 | lr: 6.73e-04 | norm: 1.4863 | dt: 11723.0539ms | tok/sec: 33.5421\n",
      "step  481 | train loss: 5.09 | val loss: 5.06 | perplexity: 157.42 | lr: 6.74e-04 | norm: 0.6924 | dt: 11724.3638ms | tok/sec: 33.5384\n",
      "step  482 | train loss: 5.04 | val loss: 5.05 | perplexity: 156.08 | lr: 6.76e-04 | norm: 0.5982 | dt: 11727.7453ms | tok/sec: 33.5287\n",
      "step  483 | train loss: 5.23 | val loss: 5.05 | perplexity: 155.65 | lr: 6.77e-04 | norm: 0.5521 | dt: 11676.2447ms | tok/sec: 33.6766\n",
      "step  484 | train loss: 5.45 | val loss: 5.05 | perplexity: 156.57 | lr: 6.78e-04 | norm: 0.5310 | dt: 11708.3247ms | tok/sec: 33.5843\n",
      "step  485 | train loss: 5.37 | val loss: 5.06 | perplexity: 156.92 | lr: 6.80e-04 | norm: 0.4695 | dt: 11721.6415ms | tok/sec: 33.5462\n",
      "step  486 | train loss: 5.24 | val loss: 5.04 | perplexity: 155.09 | lr: 6.81e-04 | norm: 0.5450 | dt: 11697.4411ms | tok/sec: 33.6156\n",
      "step  487 | train loss: 5.20 | val loss: 5.05 | perplexity: 156.20 | lr: 6.83e-04 | norm: 0.5511 | dt: 11693.8252ms | tok/sec: 33.6260\n",
      "step  488 | train loss: 5.19 | val loss: 5.06 | perplexity: 156.88 | lr: 6.84e-04 | norm: 0.5125 | dt: 11740.4783ms | tok/sec: 33.4923\n",
      "step  489 | train loss: 5.34 | val loss: 5.05 | perplexity: 156.52 | lr: 6.85e-04 | norm: 0.8781 | dt: 11701.0286ms | tok/sec: 33.6053\n",
      "step  490 | train loss: 5.19 | val loss: 5.05 | perplexity: 156.48 | lr: 6.87e-04 | norm: 0.6383 | dt: 11716.2063ms | tok/sec: 33.5617\n",
      "step  491 | train loss: 4.95 | val loss: 5.04 | perplexity: 154.66 | lr: 6.88e-04 | norm: 0.5075 | dt: 11716.6927ms | tok/sec: 33.5603\n",
      "step  492 | train loss: 5.35 | val loss: 5.02 | perplexity: 151.45 | lr: 6.90e-04 | norm: 0.5521 | dt: 11698.6120ms | tok/sec: 33.6122\n",
      "step  493 | train loss: 5.16 | val loss: 5.03 | perplexity: 152.39 | lr: 6.91e-04 | norm: 0.6687 | dt: 11728.6596ms | tok/sec: 33.5261\n",
      "step  494 | train loss: 5.12 | val loss: 5.03 | perplexity: 153.30 | lr: 6.92e-04 | norm: 0.7413 | dt: 11694.5722ms | tok/sec: 33.6238\n",
      "step  495 | train loss: 5.13 | val loss: 5.04 | perplexity: 154.93 | lr: 6.94e-04 | norm: 0.5720 | dt: 11722.1107ms | tok/sec: 33.5448\n",
      "step  496 | train loss: 4.86 | val loss: 5.03 | perplexity: 153.53 | lr: 6.95e-04 | norm: 0.7215 | dt: 11695.9898ms | tok/sec: 33.6197\n",
      "step  497 | train loss: 5.24 | val loss: 5.03 | perplexity: 152.53 | lr: 6.97e-04 | norm: 0.5612 | dt: 11717.0587ms | tok/sec: 33.5593\n",
      "step  498 | train loss: 5.15 | val loss: 5.03 | perplexity: 152.98 | lr: 6.98e-04 | norm: 0.5964 | dt: 11722.3697ms | tok/sec: 33.5441\n",
      "step  499 | train loss: 5.44 | val loss: 5.04 | perplexity: 154.41 | lr: 6.99e-04 | norm: 0.5311 | dt: 11689.1849ms | tok/sec: 33.6393\n",
      "step  500 | train loss: 5.41 | val loss: 5.06 | perplexity: 157.55 | lr: 7.01e-04 | norm: 0.4721 | dt: 11702.8987ms | tok/sec: 33.5999\n",
      "step  501 | train loss: 5.34 | val loss: 5.05 | perplexity: 156.50 | lr: 7.02e-04 | norm: 0.7624 | dt: 11698.4608ms | tok/sec: 33.6126\n",
      "step  502 | train loss: 5.03 | val loss: 5.01 | perplexity: 150.63 | lr: 7.03e-04 | norm: 0.6578 | dt: 11704.1376ms | tok/sec: 33.5963\n",
      "step  503 | train loss: 5.57 | val loss: 5.03 | perplexity: 152.55 | lr: 7.05e-04 | norm: 0.7550 | dt: 11715.4167ms | tok/sec: 33.5640\n",
      "step  504 | train loss: 5.26 | val loss: 5.02 | perplexity: 151.35 | lr: 7.06e-04 | norm: 0.5780 | dt: 11713.9781ms | tok/sec: 33.5681\n",
      "step  505 | train loss: 5.34 | val loss: 5.01 | perplexity: 150.26 | lr: 7.08e-04 | norm: 0.5543 | dt: 11720.9933ms | tok/sec: 33.5480\n",
      "step  506 | train loss: 5.47 | val loss: 5.00 | perplexity: 149.02 | lr: 7.09e-04 | norm: 0.5317 | dt: 11705.7798ms | tok/sec: 33.5916\n",
      "step  507 | train loss: 5.30 | val loss: 5.01 | perplexity: 150.20 | lr: 7.10e-04 | norm: 0.7881 | dt: 11680.8045ms | tok/sec: 33.6634\n",
      "step  508 | train loss: 5.43 | val loss: 5.02 | perplexity: 151.00 | lr: 7.12e-04 | norm: 0.5624 | dt: 11716.5697ms | tok/sec: 33.5607\n",
      "step  509 | train loss: 5.36 | val loss: 5.03 | perplexity: 152.36 | lr: 7.13e-04 | norm: 0.5908 | dt: 11691.2329ms | tok/sec: 33.6334\n",
      "step  510 | train loss: 5.12 | val loss: 5.01 | perplexity: 150.27 | lr: 7.15e-04 | norm: 0.6643 | dt: 11688.3416ms | tok/sec: 33.6417\n",
      "step  511 | train loss: 5.29 | val loss: 5.01 | perplexity: 149.56 | lr: 7.16e-04 | norm: 0.6379 | dt: 11710.6352ms | tok/sec: 33.5777\n",
      "step  512 | train loss: 5.34 | val loss: 5.01 | perplexity: 150.57 | lr: 7.17e-04 | norm: 0.6044 | dt: 11712.5447ms | tok/sec: 33.5722\n",
      "step  513 | train loss: 5.48 | val loss: 5.02 | perplexity: 151.73 | lr: 7.19e-04 | norm: 0.5053 | dt: 11727.3116ms | tok/sec: 33.5299\n",
      "step  514 | train loss: 5.39 | val loss: 5.01 | perplexity: 149.92 | lr: 7.20e-04 | norm: 0.6119 | dt: 11732.4905ms | tok/sec: 33.5151\n",
      "step  515 | train loss: 5.41 | val loss: 5.01 | perplexity: 149.72 | lr: 7.22e-04 | norm: 0.6391 | dt: 11699.5139ms | tok/sec: 33.6096\n",
      "step  516 | train loss: 5.29 | val loss: 5.07 | perplexity: 158.47 | lr: 7.23e-04 | norm: 0.6015 | dt: 11697.3560ms | tok/sec: 33.6158\n",
      "step  517 | train loss: 5.31 | val loss: 5.02 | perplexity: 151.96 | lr: 7.24e-04 | norm: 0.7863 | dt: 11700.8898ms | tok/sec: 33.6056\n",
      "step  518 | train loss: 5.34 | val loss: 5.01 | perplexity: 150.33 | lr: 7.26e-04 | norm: 0.5480 | dt: 11685.0886ms | tok/sec: 33.6511\n",
      "step  519 | train loss: 5.26 | val loss: 5.01 | perplexity: 149.89 | lr: 7.27e-04 | norm: 0.4071 | dt: 11692.4729ms | tok/sec: 33.6298\n",
      "step  520 | train loss: 5.14 | val loss: 5.00 | perplexity: 148.37 | lr: 7.29e-04 | norm: 0.5387 | dt: 11720.7885ms | tok/sec: 33.5486\n",
      "step  521 | train loss: 5.37 | val loss: 5.00 | perplexity: 148.89 | lr: 7.30e-04 | norm: 0.6509 | dt: 11716.3389ms | tok/sec: 33.5613\n",
      "step  522 | train loss: 4.91 | val loss: 5.01 | perplexity: 150.31 | lr: 7.31e-04 | norm: 0.5142 | dt: 11671.4232ms | tok/sec: 33.6905\n",
      "step  523 | train loss: 5.08 | val loss: 5.01 | perplexity: 149.53 | lr: 7.33e-04 | norm: 0.6365 | dt: 11707.1705ms | tok/sec: 33.5876\n",
      "step  524 | train loss: 5.14 | val loss: 5.01 | perplexity: 149.21 | lr: 7.34e-04 | norm: 0.4563 | dt: 11703.6371ms | tok/sec: 33.5978\n",
      "step  525 | train loss: 5.28 | val loss: 5.00 | perplexity: 148.17 | lr: 7.36e-04 | norm: 0.4297 | dt: 11720.5627ms | tok/sec: 33.5492\n",
      "step  526 | train loss: 5.35 | val loss: 5.00 | perplexity: 148.79 | lr: 7.37e-04 | norm: 1.2594 | dt: 11732.7471ms | tok/sec: 33.5144\n",
      "step  527 | train loss: 5.06 | val loss: 5.01 | perplexity: 150.24 | lr: 7.38e-04 | norm: 0.6480 | dt: 11720.7174ms | tok/sec: 33.5488\n",
      "step  528 | train loss: 5.01 | val loss: 5.02 | perplexity: 151.35 | lr: 7.40e-04 | norm: 0.5689 | dt: 11695.9357ms | tok/sec: 33.6199\n",
      "step  529 | train loss: 5.36 | val loss: 5.03 | perplexity: 152.80 | lr: 7.41e-04 | norm: 0.5041 | dt: 11678.3836ms | tok/sec: 33.6704\n",
      "step  530 | train loss: 5.17 | val loss: 5.01 | perplexity: 149.94 | lr: 7.43e-04 | norm: 0.5425 | dt: 11713.2587ms | tok/sec: 33.5702\n",
      "step  531 | train loss: 5.21 | val loss: 5.01 | perplexity: 149.93 | lr: 7.44e-04 | norm: 0.4865 | dt: 11710.0792ms | tok/sec: 33.5793\n",
      "step  532 | train loss: 5.25 | val loss: 5.01 | perplexity: 149.86 | lr: 7.45e-04 | norm: 0.5060 | dt: 11682.7872ms | tok/sec: 33.6577\n",
      "step  533 | train loss: 5.04 | val loss: 5.01 | perplexity: 149.63 | lr: 7.47e-04 | norm: 0.5200 | dt: 11705.4636ms | tok/sec: 33.5925\n",
      "step  534 | train loss: 5.25 | val loss: 5.02 | perplexity: 151.01 | lr: 7.48e-04 | norm: 0.5153 | dt: 11694.6104ms | tok/sec: 33.6237\n",
      "step  535 | train loss: 5.31 | val loss: 5.03 | perplexity: 152.70 | lr: 7.50e-04 | norm: 0.4208 | dt: 11699.0175ms | tok/sec: 33.6110\n",
      "step  536 | train loss: 5.21 | val loss: 5.02 | perplexity: 152.05 | lr: 7.51e-04 | norm: 0.4625 | dt: 11725.4317ms | tok/sec: 33.5353\n",
      "step  537 | train loss: 5.16 | val loss: 5.02 | perplexity: 151.14 | lr: 7.52e-04 | norm: 0.5611 | dt: 11713.5656ms | tok/sec: 33.5693\n",
      "step  538 | train loss: 5.36 | val loss: 5.01 | perplexity: 150.23 | lr: 7.54e-04 | norm: 0.5823 | dt: 11709.9011ms | tok/sec: 33.5798\n",
      "step  539 | train loss: 5.16 | val loss: 5.01 | perplexity: 149.30 | lr: 7.55e-04 | norm: 0.4566 | dt: 11727.0145ms | tok/sec: 33.5308\n",
      "step  540 | train loss: 5.08 | val loss: 5.00 | perplexity: 148.01 | lr: 7.57e-04 | norm: 0.4619 | dt: 11724.2913ms | tok/sec: 33.5386\n",
      "step  541 | train loss: 5.17 | val loss: 4.99 | perplexity: 146.52 | lr: 7.58e-04 | norm: 0.4723 | dt: 11731.8928ms | tok/sec: 33.5168\n",
      "step  542 | train loss: 5.15 | val loss: 4.98 | perplexity: 145.42 | lr: 7.59e-04 | norm: 0.5233 | dt: 11711.2520ms | tok/sec: 33.5759\n",
      "step  543 | train loss: 5.30 | val loss: 4.98 | perplexity: 145.70 | lr: 7.61e-04 | norm: 0.5224 | dt: 11699.6889ms | tok/sec: 33.6091\n",
      "step  544 | train loss: 5.27 | val loss: 5.01 | perplexity: 149.32 | lr: 7.62e-04 | norm: 0.4920 | dt: 11721.6640ms | tok/sec: 33.5461\n",
      "step  545 | train loss: 5.05 | val loss: 5.01 | perplexity: 149.61 | lr: 7.64e-04 | norm: 0.5536 | dt: 11736.2545ms | tok/sec: 33.5044\n",
      "step  546 | train loss: 4.90 | val loss: 5.00 | perplexity: 148.85 | lr: 7.65e-04 | norm: 0.5922 | dt: 11722.5366ms | tok/sec: 33.5436\n",
      "step  547 | train loss: 5.02 | val loss: 4.99 | perplexity: 146.83 | lr: 7.66e-04 | norm: 0.6270 | dt: 11698.0810ms | tok/sec: 33.6137\n",
      "step  548 | train loss: 5.09 | val loss: 4.98 | perplexity: 145.67 | lr: 7.68e-04 | norm: 0.6398 | dt: 11700.8140ms | tok/sec: 33.6059\n",
      "step  549 | train loss: 5.40 | val loss: 4.98 | perplexity: 145.71 | lr: 7.69e-04 | norm: 0.3944 | dt: 11717.5920ms | tok/sec: 33.5577\n",
      "step  550 | train loss: 5.21 | val loss: 4.97 | perplexity: 144.64 | lr: 7.71e-04 | norm: 0.5645 | dt: 11708.4234ms | tok/sec: 33.5840\n",
      "step  551 | train loss: 5.18 | val loss: 4.97 | perplexity: 143.56 | lr: 7.72e-04 | norm: 0.5795 | dt: 11725.8952ms | tok/sec: 33.5340\n",
      "step  552 | train loss: 5.36 | val loss: 4.99 | perplexity: 146.87 | lr: 7.73e-04 | norm: 0.4374 | dt: 11720.4225ms | tok/sec: 33.5496\n",
      "step  553 | train loss: 5.04 | val loss: 4.96 | perplexity: 142.64 | lr: 7.75e-04 | norm: 0.4885 | dt: 11721.0603ms | tok/sec: 33.5478\n",
      "step  554 | train loss: 5.46 | val loss: 4.96 | perplexity: 142.85 | lr: 7.76e-04 | norm: 0.5096 | dt: 11700.1557ms | tok/sec: 33.6078\n",
      "step  555 | train loss: 5.00 | val loss: 4.98 | perplexity: 145.02 | lr: 7.78e-04 | norm: 0.5272 | dt: 11714.9496ms | tok/sec: 33.5653\n",
      "step  556 | train loss: 5.15 | val loss: 4.97 | perplexity: 144.04 | lr: 7.79e-04 | norm: 0.4643 | dt: 11731.6847ms | tok/sec: 33.5174\n",
      "step  557 | train loss: 5.41 | val loss: 4.97 | perplexity: 144.63 | lr: 7.80e-04 | norm: 0.3861 | dt: 11673.2442ms | tok/sec: 33.6852\n",
      "step  558 | train loss: 5.06 | val loss: 4.97 | perplexity: 143.52 | lr: 7.82e-04 | norm: 0.4986 | dt: 11700.4437ms | tok/sec: 33.6069\n",
      "step  559 | train loss: 4.96 | val loss: 4.97 | perplexity: 144.65 | lr: 7.83e-04 | norm: 0.5318 | dt: 11717.9937ms | tok/sec: 33.5566\n",
      "step  560 | train loss: 5.12 | val loss: 4.98 | perplexity: 145.29 | lr: 7.85e-04 | norm: 0.4686 | dt: 11717.7460ms | tok/sec: 33.5573\n",
      "step  561 | train loss: 5.15 | val loss: 4.98 | perplexity: 145.27 | lr: 7.86e-04 | norm: 0.5742 | dt: 11732.3496ms | tok/sec: 33.5155\n",
      "step  562 | train loss: 5.12 | val loss: 4.98 | perplexity: 145.49 | lr: 7.87e-04 | norm: 0.4797 | dt: 11717.4878ms | tok/sec: 33.5580\n",
      "step  563 | train loss: 5.22 | val loss: 4.97 | perplexity: 144.64 | lr: 7.89e-04 | norm: 0.4647 | dt: 11733.6907ms | tok/sec: 33.5117\n",
      "step  564 | train loss: 5.11 | val loss: 4.96 | perplexity: 142.84 | lr: 7.90e-04 | norm: 0.6170 | dt: 11720.3660ms | tok/sec: 33.5498\n",
      "step  565 | train loss: 5.07 | val loss: 4.97 | perplexity: 144.72 | lr: 7.92e-04 | norm: 0.5713 | dt: 11716.1269ms | tok/sec: 33.5619\n",
      "step  566 | train loss: 5.29 | val loss: 4.99 | perplexity: 146.44 | lr: 7.93e-04 | norm: 0.6247 | dt: 11716.5599ms | tok/sec: 33.5607\n",
      "step  567 | train loss: 5.14 | val loss: 4.98 | perplexity: 145.07 | lr: 7.94e-04 | norm: 0.5282 | dt: 11727.5996ms | tok/sec: 33.5291\n",
      "step  568 | train loss: 4.96 | val loss: 4.94 | perplexity: 139.76 | lr: 7.96e-04 | norm: 0.6766 | dt: 11680.5418ms | tok/sec: 33.6642\n",
      "step  569 | train loss: 5.36 | val loss: 4.93 | perplexity: 138.91 | lr: 7.97e-04 | norm: 0.6728 | dt: 11692.9610ms | tok/sec: 33.6284\n",
      "step  570 | train loss: 4.94 | val loss: 4.93 | perplexity: 138.82 | lr: 7.99e-04 | norm: 0.6741 | dt: 11717.7987ms | tok/sec: 33.5572\n",
      "step  571 | train loss: 5.20 | val loss: 4.93 | perplexity: 138.57 | lr: 8.00e-04 | norm: 0.5496 | dt: 11713.1395ms | tok/sec: 33.5705\n",
      "step  572 | train loss: 5.15 | val loss: 4.93 | perplexity: 138.81 | lr: 8.01e-04 | norm: 0.5025 | dt: 11701.3078ms | tok/sec: 33.6044\n",
      "step  573 | train loss: 5.10 | val loss: 4.92 | perplexity: 137.58 | lr: 8.03e-04 | norm: 0.4922 | dt: 11730.8462ms | tok/sec: 33.5198\n",
      "step  574 | train loss: 5.07 | val loss: 4.92 | perplexity: 137.39 | lr: 8.04e-04 | norm: 0.6440 | dt: 11734.0894ms | tok/sec: 33.5106\n",
      "step  575 | train loss: 4.97 | val loss: 4.92 | perplexity: 137.13 | lr: 8.06e-04 | norm: 0.7003 | dt: 11711.1442ms | tok/sec: 33.5762\n",
      "step  576 | train loss: 4.97 | val loss: 4.92 | perplexity: 137.37 | lr: 8.07e-04 | norm: 0.4701 | dt: 11714.4351ms | tok/sec: 33.5668\n",
      "step  577 | train loss: 5.38 | val loss: 4.92 | perplexity: 136.91 | lr: 8.08e-04 | norm: 0.5058 | dt: 11716.3453ms | tok/sec: 33.5613\n",
      "step  578 | train loss: 5.03 | val loss: 4.92 | perplexity: 136.49 | lr: 8.10e-04 | norm: 0.4130 | dt: 11723.5765ms | tok/sec: 33.5406\n",
      "step  579 | train loss: 5.09 | val loss: 4.91 | perplexity: 135.83 | lr: 8.11e-04 | norm: 0.5397 | dt: 11706.3107ms | tok/sec: 33.5901\n",
      "step  580 | train loss: 5.00 | val loss: 4.91 | perplexity: 135.65 | lr: 8.13e-04 | norm: 0.4724 | dt: 11723.8607ms | tok/sec: 33.5398\n",
      "step  581 | train loss: 5.08 | val loss: 4.92 | perplexity: 137.26 | lr: 8.14e-04 | norm: 0.4923 | dt: 11727.0627ms | tok/sec: 33.5306\n",
      "step  582 | train loss: 4.94 | val loss: 4.93 | perplexity: 138.12 | lr: 8.15e-04 | norm: 0.5822 | dt: 11740.3777ms | tok/sec: 33.4926\n",
      "step  583 | train loss: 5.00 | val loss: 4.91 | perplexity: 136.20 | lr: 8.17e-04 | norm: 0.5096 | dt: 11738.3254ms | tok/sec: 33.4985\n",
      "step  584 | train loss: 5.07 | val loss: 4.91 | perplexity: 135.53 | lr: 8.18e-04 | norm: 0.4917 | dt: 11727.3235ms | tok/sec: 33.5299\n",
      "step  585 | train loss: 5.13 | val loss: 4.92 | perplexity: 137.20 | lr: 8.20e-04 | norm: 0.5809 | dt: 11734.9305ms | tok/sec: 33.5082\n",
      "step  586 | train loss: 5.04 | val loss: 4.95 | perplexity: 140.67 | lr: 8.21e-04 | norm: 0.5321 | dt: 11713.3310ms | tok/sec: 33.5700\n",
      "step  587 | train loss: 5.02 | val loss: 4.92 | perplexity: 136.95 | lr: 8.22e-04 | norm: 0.5952 | dt: 11756.9218ms | tok/sec: 33.4455\n",
      "step  588 | train loss: 5.31 | val loss: 4.92 | perplexity: 137.20 | lr: 8.24e-04 | norm: 0.4752 | dt: 11682.9166ms | tok/sec: 33.6573\n",
      "step  589 | train loss: 5.06 | val loss: 4.92 | perplexity: 137.05 | lr: 8.25e-04 | norm: 0.6051 | dt: 11732.1024ms | tok/sec: 33.5162\n",
      "step  590 | train loss: 4.82 | val loss: 4.91 | perplexity: 136.27 | lr: 8.27e-04 | norm: 0.8187 | dt: 11746.9490ms | tok/sec: 33.4739\n",
      "step  591 | train loss: 5.03 | val loss: 4.92 | perplexity: 137.15 | lr: 8.28e-04 | norm: 0.4755 | dt: 11720.4092ms | tok/sec: 33.5497\n",
      "step  592 | train loss: 5.17 | val loss: 4.93 | perplexity: 138.63 | lr: 8.29e-04 | norm: 0.4432 | dt: 11732.8026ms | tok/sec: 33.5142\n",
      "step  593 | train loss: 4.95 | val loss: 4.94 | perplexity: 139.54 | lr: 8.31e-04 | norm: 0.5116 | dt: 11715.8990ms | tok/sec: 33.5626\n",
      "step  594 | train loss: 4.83 | val loss: 4.95 | perplexity: 141.13 | lr: 8.32e-04 | norm: 0.5550 | dt: 11720.8796ms | tok/sec: 33.5483\n",
      "step  595 | train loss: 5.03 | val loss: 4.91 | perplexity: 136.14 | lr: 8.34e-04 | norm: 0.7231 | dt: 11674.6662ms | tok/sec: 33.6811\n",
      "step  596 | train loss: 4.81 | val loss: 4.93 | perplexity: 138.24 | lr: 8.35e-04 | norm: 0.5735 | dt: 11719.7790ms | tok/sec: 33.5515\n",
      "step  597 | train loss: 5.13 | val loss: 4.92 | perplexity: 136.71 | lr: 8.36e-04 | norm: 0.5922 | dt: 11745.4855ms | tok/sec: 33.4781\n",
      "step  598 | train loss: 5.30 | val loss: 4.94 | perplexity: 139.65 | lr: 8.38e-04 | norm: 0.4931 | dt: 11732.6696ms | tok/sec: 33.5146\n",
      "step  599 | train loss: 5.10 | val loss: 4.92 | perplexity: 137.27 | lr: 8.39e-04 | norm: 0.5469 | dt: 11747.5224ms | tok/sec: 33.4722\n",
      "step  600 | train loss: 5.40 | val loss: 4.93 | perplexity: 137.89 | lr: 8.41e-04 | norm: 0.6378 | dt: 11711.6871ms | tok/sec: 33.5747\n",
      "step  601 | train loss: 4.91 | val loss: 4.94 | perplexity: 139.25 | lr: 8.42e-04 | norm: 0.5561 | dt: 11731.3490ms | tok/sec: 33.5184\n",
      "step  602 | train loss: 5.25 | val loss: 4.93 | perplexity: 139.02 | lr: 8.43e-04 | norm: 0.5589 | dt: 11728.5936ms | tok/sec: 33.5263\n",
      "step  603 | train loss: 5.08 | val loss: 4.94 | perplexity: 139.18 | lr: 8.45e-04 | norm: 0.5526 | dt: 11744.3426ms | tok/sec: 33.4813\n",
      "step  604 | train loss: 5.28 | val loss: 4.97 | perplexity: 144.16 | lr: 8.46e-04 | norm: 0.5435 | dt: 11711.5932ms | tok/sec: 33.5749\n",
      "step  605 | train loss: 5.09 | val loss: 4.97 | perplexity: 144.12 | lr: 8.48e-04 | norm: 0.7855 | dt: 11730.3121ms | tok/sec: 33.5214\n",
      "step  606 | train loss: 5.22 | val loss: 4.95 | perplexity: 141.34 | lr: 8.49e-04 | norm: 0.5834 | dt: 11699.7318ms | tok/sec: 33.6090\n",
      "step  607 | train loss: 5.06 | val loss: 4.95 | perplexity: 141.21 | lr: 8.50e-04 | norm: 0.6078 | dt: 11728.7660ms | tok/sec: 33.5258\n",
      "step  608 | train loss: 5.21 | val loss: 4.93 | perplexity: 138.37 | lr: 8.52e-04 | norm: 0.5548 | dt: 11733.2299ms | tok/sec: 33.5130\n",
      "step  609 | train loss: 5.30 | val loss: 4.92 | perplexity: 137.11 | lr: 8.53e-04 | norm: 0.4575 | dt: 11696.9972ms | tok/sec: 33.6168\n",
      "step  610 | train loss: 5.23 | val loss: 4.92 | perplexity: 137.15 | lr: 8.55e-04 | norm: 0.4662 | dt: 11723.2835ms | tok/sec: 33.5415\n",
      "step  611 | train loss: 5.13 | val loss: 4.93 | perplexity: 138.09 | lr: 8.56e-04 | norm: 0.4992 | dt: 11711.2212ms | tok/sec: 33.5760\n",
      "step  612 | train loss: 5.17 | val loss: 4.95 | perplexity: 141.31 | lr: 8.57e-04 | norm: 0.4902 | dt: 11668.4635ms | tok/sec: 33.6990\n",
      "step  613 | train loss: 5.14 | val loss: 4.98 | perplexity: 146.16 | lr: 8.59e-04 | norm: 0.5958 | dt: 11712.9328ms | tok/sec: 33.5711\n",
      "step  614 | train loss: 5.00 | val loss: 4.98 | perplexity: 145.71 | lr: 8.60e-04 | norm: 0.5469 | dt: 11723.3953ms | tok/sec: 33.5411\n",
      "step  615 | train loss: 4.98 | val loss: 4.98 | perplexity: 145.31 | lr: 8.62e-04 | norm: 0.5409 | dt: 11720.5029ms | tok/sec: 33.5494\n",
      "step  616 | train loss: 5.41 | val loss: 4.98 | perplexity: 145.75 | lr: 8.63e-04 | norm: 0.7038 | dt: 11709.7962ms | tok/sec: 33.5801\n",
      "step  617 | train loss: 5.00 | val loss: 4.96 | perplexity: 142.18 | lr: 8.64e-04 | norm: 0.7114 | dt: 11728.1892ms | tok/sec: 33.5274\n",
      "step  618 | train loss: 5.05 | val loss: 4.97 | perplexity: 144.28 | lr: 8.66e-04 | norm: 0.6648 | dt: 11717.2513ms | tok/sec: 33.5587\n",
      "step  619 | train loss: 5.04 | val loss: 4.97 | perplexity: 143.82 | lr: 8.67e-04 | norm: 0.6868 | dt: 11715.3451ms | tok/sec: 33.5642\n",
      "step  620 | train loss: 5.12 | val loss: 4.97 | perplexity: 143.36 | lr: 8.69e-04 | norm: 0.7124 | dt: 11733.1190ms | tok/sec: 33.5133\n",
      "step  621 | train loss: 4.81 | val loss: 4.97 | perplexity: 144.16 | lr: 8.70e-04 | norm: 0.6755 | dt: 11697.0062ms | tok/sec: 33.6168\n",
      "step  622 | train loss: 4.92 | val loss: 4.94 | perplexity: 140.37 | lr: 8.71e-04 | norm: 0.7090 | dt: 11654.3982ms | tok/sec: 33.7397\n",
      "step  623 | train loss: 4.69 | val loss: 4.94 | perplexity: 139.27 | lr: 8.73e-04 | norm: 0.7138 | dt: 11679.9576ms | tok/sec: 33.6659\n",
      "step  624 | train loss: 4.86 | val loss: 4.94 | perplexity: 139.66 | lr: 8.74e-04 | norm: 0.6090 | dt: 11698.8776ms | tok/sec: 33.6114\n",
      "step  625 | train loss: 5.19 | val loss: 4.95 | perplexity: 140.65 | lr: 8.76e-04 | norm: 0.4977 | dt: 11704.3147ms | tok/sec: 33.5958\n",
      "step  626 | train loss: 5.12 | val loss: 4.94 | perplexity: 139.64 | lr: 8.77e-04 | norm: 0.5255 | dt: 11728.9591ms | tok/sec: 33.5252\n",
      "step  627 | train loss: 5.19 | val loss: 4.94 | perplexity: 140.36 | lr: 8.78e-04 | norm: 0.6214 | dt: 11719.6431ms | tok/sec: 33.5519\n",
      "step  628 | train loss: 5.40 | val loss: 4.95 | perplexity: 140.91 | lr: 8.80e-04 | norm: 0.7213 | dt: 11713.6579ms | tok/sec: 33.5690\n",
      "step  629 | train loss: 4.91 | val loss: 4.96 | perplexity: 142.29 | lr: 8.81e-04 | norm: 0.5068 | dt: 11719.2657ms | tok/sec: 33.5530\n",
      "step  630 | train loss: 4.86 | val loss: 4.92 | perplexity: 137.12 | lr: 8.83e-04 | norm: 0.6991 | dt: 11727.5789ms | tok/sec: 33.5292\n",
      "step  631 | train loss: 5.01 | val loss: 4.92 | perplexity: 136.84 | lr: 8.84e-04 | norm: 0.5184 | dt: 11712.1372ms | tok/sec: 33.5734\n",
      "step  632 | train loss: 5.13 | val loss: 4.90 | perplexity: 134.03 | lr: 8.85e-04 | norm: 0.6047 | dt: 11712.7237ms | tok/sec: 33.5717\n",
      "step  633 | train loss: 4.74 | val loss: 4.93 | perplexity: 138.97 | lr: 8.87e-04 | norm: 0.5395 | dt: 11728.7734ms | tok/sec: 33.5258\n",
      "step  634 | train loss: 5.15 | val loss: 4.96 | perplexity: 142.92 | lr: 8.88e-04 | norm: 0.5231 | dt: 11719.4233ms | tok/sec: 33.5525\n",
      "step  635 | train loss: 5.29 | val loss: 4.94 | perplexity: 139.18 | lr: 8.90e-04 | norm: 0.5975 | dt: 11690.8808ms | tok/sec: 33.6344\n",
      "step  636 | train loss: 5.03 | val loss: 4.92 | perplexity: 136.36 | lr: 8.91e-04 | norm: 0.6240 | dt: 11693.5787ms | tok/sec: 33.6267\n",
      "step  637 | train loss: 4.89 | val loss: 4.88 | perplexity: 132.15 | lr: 8.92e-04 | norm: 0.7514 | dt: 11726.9461ms | tok/sec: 33.5310\n",
      "step  638 | train loss: 5.11 | val loss: 4.89 | perplexity: 133.26 | lr: 8.94e-04 | norm: 0.5104 | dt: 11716.3377ms | tok/sec: 33.5613\n",
      "step  639 | train loss: 5.22 | val loss: 4.91 | perplexity: 135.40 | lr: 8.95e-04 | norm: 0.5948 | dt: 11714.7119ms | tok/sec: 33.5660\n",
      "step  640 | train loss: 4.99 | val loss: 4.92 | perplexity: 137.53 | lr: 8.97e-04 | norm: 0.5963 | dt: 11732.3222ms | tok/sec: 33.5156\n",
      "step  641 | train loss: 5.24 | val loss: 4.93 | perplexity: 137.90 | lr: 8.98e-04 | norm: 0.5484 | dt: 11698.9496ms | tok/sec: 33.6112\n",
      "step  642 | train loss: 5.19 | val loss: 4.90 | perplexity: 134.71 | lr: 8.99e-04 | norm: 0.6064 | dt: 11688.4205ms | tok/sec: 33.6415\n",
      "step  643 | train loss: 5.30 | val loss: 4.89 | perplexity: 133.20 | lr: 9.01e-04 | norm: 0.5922 | dt: 11711.3392ms | tok/sec: 33.5757\n",
      "step  644 | train loss: 5.15 | val loss: 4.89 | perplexity: 132.67 | lr: 9.02e-04 | norm: 0.5288 | dt: 11717.6712ms | tok/sec: 33.5575\n",
      "step  645 | train loss: 5.02 | val loss: 4.89 | perplexity: 133.08 | lr: 9.03e-04 | norm: 0.4791 | dt: 11733.7103ms | tok/sec: 33.5117\n",
      "step  646 | train loss: 5.08 | val loss: 4.88 | perplexity: 131.56 | lr: 9.05e-04 | norm: 0.4482 | dt: 11743.9842ms | tok/sec: 33.4823\n",
      "step  647 | train loss: 5.17 | val loss: 4.88 | perplexity: 131.89 | lr: 9.06e-04 | norm: 0.6068 | dt: 11698.7658ms | tok/sec: 33.6118\n",
      "step  648 | train loss: 5.01 | val loss: 4.89 | perplexity: 133.23 | lr: 9.08e-04 | norm: 0.5721 | dt: 11715.8287ms | tok/sec: 33.5628\n",
      "step  649 | train loss: 5.07 | val loss: 4.90 | perplexity: 134.35 | lr: 9.09e-04 | norm: 0.6146 | dt: 11718.0448ms | tok/sec: 33.5565\n",
      "step  650 | train loss: 5.00 | val loss: 4.90 | perplexity: 134.23 | lr: 9.10e-04 | norm: 0.4187 | dt: 11717.4897ms | tok/sec: 33.5580\n",
      "step  651 | train loss: 4.84 | val loss: 4.89 | perplexity: 132.62 | lr: 9.12e-04 | norm: 0.5452 | dt: 11719.7716ms | tok/sec: 33.5515\n",
      "step  652 | train loss: 4.91 | val loss: 4.86 | perplexity: 129.14 | lr: 9.13e-04 | norm: 0.4850 | dt: 11723.4952ms | tok/sec: 33.5409\n",
      "step  653 | train loss: 4.94 | val loss: 4.85 | perplexity: 127.98 | lr: 9.15e-04 | norm: 0.4720 | dt: 11709.6930ms | tok/sec: 33.5804\n",
      "step  654 | train loss: 4.99 | val loss: 4.85 | perplexity: 128.38 | lr: 9.16e-04 | norm: 0.7052 | dt: 11694.1068ms | tok/sec: 33.6251\n",
      "step  655 | train loss: 4.95 | val loss: 4.87 | perplexity: 129.70 | lr: 9.17e-04 | norm: 0.4860 | dt: 11708.6046ms | tok/sec: 33.5835\n",
      "step  656 | train loss: 5.04 | val loss: 4.86 | perplexity: 129.50 | lr: 9.19e-04 | norm: 0.5431 | dt: 11718.0924ms | tok/sec: 33.5563\n",
      "step  657 | train loss: 4.82 | val loss: 4.85 | perplexity: 127.76 | lr: 9.20e-04 | norm: 0.5074 | dt: 11720.0587ms | tok/sec: 33.5507\n",
      "step  658 | train loss: 5.14 | val loss: 4.86 | perplexity: 129.12 | lr: 9.22e-04 | norm: 0.5391 | dt: 11722.6684ms | tok/sec: 33.5432\n",
      "step  659 | train loss: 5.02 | val loss: 4.85 | perplexity: 127.45 | lr: 9.23e-04 | norm: 0.4873 | dt: 11726.1143ms | tok/sec: 33.5334\n",
      "step  660 | train loss: 4.98 | val loss: 4.84 | perplexity: 126.37 | lr: 9.24e-04 | norm: 0.5538 | dt: 11700.2311ms | tok/sec: 33.6075\n",
      "step  661 | train loss: 4.98 | val loss: 4.85 | perplexity: 127.53 | lr: 9.26e-04 | norm: 0.6068 | dt: 11722.2998ms | tok/sec: 33.5443\n",
      "step  662 | train loss: 4.91 | val loss: 4.85 | perplexity: 127.36 | lr: 9.27e-04 | norm: 0.5314 | dt: 11735.6477ms | tok/sec: 33.5061\n",
      "step  663 | train loss: 4.80 | val loss: 4.83 | perplexity: 125.27 | lr: 9.29e-04 | norm: 0.4839 | dt: 11721.7407ms | tok/sec: 33.5459\n",
      "step  664 | train loss: 5.04 | val loss: 4.82 | perplexity: 124.29 | lr: 9.30e-04 | norm: 0.4650 | dt: 11712.5838ms | tok/sec: 33.5721\n",
      "step  665 | train loss: 5.33 | val loss: 4.83 | perplexity: 125.36 | lr: 9.31e-04 | norm: 0.5412 | dt: 11713.9263ms | tok/sec: 33.5682\n",
      "step  666 | train loss: 5.81 | val loss: 4.89 | perplexity: 133.40 | lr: 9.33e-04 | norm: 0.8406 | dt: 11726.5499ms | tok/sec: 33.5321\n",
      "step  667 | train loss: 5.14 | val loss: 4.86 | perplexity: 128.99 | lr: 9.34e-04 | norm: 1.1339 | dt: 11721.7605ms | tok/sec: 33.5458\n",
      "step  668 | train loss: 4.99 | val loss: 4.86 | perplexity: 129.15 | lr: 9.36e-04 | norm: 0.7382 | dt: 11684.2744ms | tok/sec: 33.6534\n",
      "step  669 | train loss: 5.08 | val loss: 4.90 | perplexity: 134.00 | lr: 9.37e-04 | norm: 0.6950 | dt: 11720.5670ms | tok/sec: 33.5492\n",
      "step  670 | train loss: 5.14 | val loss: 4.87 | perplexity: 130.69 | lr: 9.38e-04 | norm: 0.7758 | dt: 11718.9982ms | tok/sec: 33.5537\n",
      "step  671 | train loss: 5.16 | val loss: 4.88 | perplexity: 131.75 | lr: 9.40e-04 | norm: 0.5869 | dt: 11720.3350ms | tok/sec: 33.5499\n",
      "step  672 | train loss: 4.90 | val loss: 4.87 | perplexity: 129.78 | lr: 9.41e-04 | norm: 0.5894 | dt: 11707.2794ms | tok/sec: 33.5873\n",
      "step  673 | train loss: 5.06 | val loss: 4.85 | perplexity: 128.13 | lr: 9.43e-04 | norm: 0.5240 | dt: 11713.2332ms | tok/sec: 33.5702\n",
      "step  674 | train loss: 4.79 | val loss: 4.86 | perplexity: 128.46 | lr: 9.44e-04 | norm: 0.6843 | dt: 11730.8781ms | tok/sec: 33.5197\n",
      "step  675 | train loss: 4.94 | val loss: 4.84 | perplexity: 126.98 | lr: 9.45e-04 | norm: 0.5979 | dt: 11719.4703ms | tok/sec: 33.5524\n",
      "step  676 | train loss: 5.07 | val loss: 4.84 | perplexity: 126.49 | lr: 9.47e-04 | norm: 0.4804 | dt: 11738.3180ms | tok/sec: 33.4985\n",
      "step  677 | train loss: 4.93 | val loss: 4.85 | perplexity: 127.76 | lr: 9.48e-04 | norm: 0.8005 | dt: 11726.0735ms | tok/sec: 33.5335\n",
      "step  678 | train loss: 5.14 | val loss: 4.85 | perplexity: 127.27 | lr: 9.50e-04 | norm: 0.5912 | dt: 11763.5601ms | tok/sec: 33.4266\n",
      "step  679 | train loss: 5.25 | val loss: 4.83 | perplexity: 125.24 | lr: 9.51e-04 | norm: 0.6238 | dt: 11749.5317ms | tok/sec: 33.4665\n",
      "step  680 | train loss: 4.68 | val loss: 4.82 | perplexity: 124.54 | lr: 9.52e-04 | norm: 0.6088 | dt: 11731.4086ms | tok/sec: 33.5182\n",
      "step  681 | train loss: 4.78 | val loss: 4.82 | perplexity: 123.43 | lr: 9.54e-04 | norm: 0.5789 | dt: 11691.2684ms | tok/sec: 33.6333\n",
      "step  682 | train loss: 4.85 | val loss: 4.83 | perplexity: 125.05 | lr: 9.55e-04 | norm: 0.4891 | dt: 11718.5502ms | tok/sec: 33.5550\n",
      "step  683 | train loss: 4.90 | val loss: 4.80 | perplexity: 121.98 | lr: 9.57e-04 | norm: 0.5741 | dt: 11715.3072ms | tok/sec: 33.5643\n",
      "step  684 | train loss: 4.87 | val loss: 4.79 | perplexity: 120.46 | lr: 9.58e-04 | norm: 0.5699 | dt: 11754.9264ms | tok/sec: 33.4512\n",
      "step  685 | train loss: 4.75 | val loss: 4.78 | perplexity: 118.95 | lr: 9.59e-04 | norm: 0.5801 | dt: 11720.0069ms | tok/sec: 33.5508\n",
      "step  686 | train loss: 4.68 | val loss: 4.78 | perplexity: 119.30 | lr: 9.61e-04 | norm: 0.7337 | dt: 11721.5962ms | tok/sec: 33.5463\n",
      "step  687 | train loss: 4.64 | val loss: 4.81 | perplexity: 123.09 | lr: 9.62e-04 | norm: 0.7872 | dt: 11708.7052ms | tok/sec: 33.5832\n",
      "step  688 | train loss: 4.76 | val loss: 4.77 | perplexity: 117.68 | lr: 9.64e-04 | norm: 0.8627 | dt: 11736.3799ms | tok/sec: 33.5040\n",
      "step  689 | train loss: 4.51 | val loss: 4.77 | perplexity: 117.96 | lr: 9.65e-04 | norm: 0.9739 | dt: 11724.1445ms | tok/sec: 33.5390\n",
      "step  690 | train loss: 5.67 | val loss: 4.81 | perplexity: 123.20 | lr: 9.66e-04 | norm: 1.2101 | dt: 11754.0214ms | tok/sec: 33.4537\n",
      "step  691 | train loss: 5.14 | val loss: 4.78 | perplexity: 119.65 | lr: 9.68e-04 | norm: 0.9239 | dt: 11719.9888ms | tok/sec: 33.5509\n",
      "step  692 | train loss: 4.54 | val loss: 4.77 | perplexity: 117.81 | lr: 9.69e-04 | norm: 0.9397 | dt: 11726.6908ms | tok/sec: 33.5317\n",
      "step  693 | train loss: 4.96 | val loss: 4.75 | perplexity: 115.51 | lr: 9.71e-04 | norm: 0.7558 | dt: 11702.3394ms | tok/sec: 33.6015\n",
      "step  694 | train loss: 4.93 | val loss: 4.75 | perplexity: 115.28 | lr: 9.72e-04 | norm: 0.7460 | dt: 11740.0672ms | tok/sec: 33.4935\n",
      "step  695 | train loss: 4.98 | val loss: 4.77 | perplexity: 117.99 | lr: 9.73e-04 | norm: 0.5855 | dt: 11725.0376ms | tok/sec: 33.5364\n",
      "step  696 | train loss: 4.68 | val loss: 4.79 | perplexity: 120.00 | lr: 9.75e-04 | norm: 0.5497 | dt: 11711.5109ms | tok/sec: 33.5752\n",
      "step  697 | train loss: 4.61 | val loss: 4.79 | perplexity: 120.47 | lr: 9.76e-04 | norm: 0.6787 | dt: 11723.9382ms | tok/sec: 33.5396\n",
      "step  698 | train loss: 4.76 | val loss: 4.79 | perplexity: 119.72 | lr: 9.78e-04 | norm: 0.8263 | dt: 11706.2042ms | tok/sec: 33.5904\n",
      "step  699 | train loss: 4.65 | val loss: 4.79 | perplexity: 119.89 | lr: 9.79e-04 | norm: 0.5950 | dt: 11694.4098ms | tok/sec: 33.6243\n",
      "step  700 | train loss: 4.67 | val loss: 4.79 | perplexity: 120.12 | lr: 9.80e-04 | norm: 0.5713 | dt: 11721.9479ms | tok/sec: 33.5453\n",
      "step  701 | train loss: 4.60 | val loss: 4.81 | perplexity: 122.86 | lr: 9.82e-04 | norm: 0.5491 | dt: 11740.5641ms | tok/sec: 33.4921\n",
      "step  702 | train loss: 4.79 | val loss: 4.83 | perplexity: 124.92 | lr: 9.83e-04 | norm: 0.5020 | dt: 11719.1298ms | tok/sec: 33.5533\n",
      "step  703 | train loss: 4.55 | val loss: 4.81 | perplexity: 122.49 | lr: 9.85e-04 | norm: 0.7698 | dt: 15492.8036ms | tok/sec: 25.3806\n",
      "step  704 | train loss: 4.96 | val loss: 4.81 | perplexity: 122.70 | lr: 9.86e-04 | norm: 0.6879 | dt: 11732.1727ms | tok/sec: 33.5160\n",
      "step  705 | train loss: 4.86 | val loss: 4.78 | perplexity: 119.25 | lr: 9.87e-04 | norm: 0.7481 | dt: 11728.1787ms | tok/sec: 33.5275\n",
      "step  706 | train loss: 4.65 | val loss: 4.78 | perplexity: 119.62 | lr: 9.89e-04 | norm: 0.6363 | dt: 11714.7994ms | tok/sec: 33.5657\n",
      "step  707 | train loss: 4.76 | val loss: 4.77 | perplexity: 118.15 | lr: 9.90e-04 | norm: 0.7028 | dt: 11747.4699ms | tok/sec: 33.4724\n",
      "step  708 | train loss: 4.84 | val loss: 4.79 | perplexity: 120.19 | lr: 9.92e-04 | norm: 0.7038 | dt: 11747.1302ms | tok/sec: 33.4734\n",
      "step  709 | train loss: 4.62 | val loss: 4.79 | perplexity: 120.05 | lr: 9.93e-04 | norm: 0.6533 | dt: 11723.4786ms | tok/sec: 33.5409\n",
      "step  710 | train loss: 4.48 | val loss: 4.80 | perplexity: 122.09 | lr: 9.94e-04 | norm: 1.0065 | dt: 11757.8158ms | tok/sec: 33.4429\n",
      "step  711 | train loss: 4.90 | val loss: 4.80 | perplexity: 121.11 | lr: 9.96e-04 | norm: 0.7158 | dt: 11739.6975ms | tok/sec: 33.4946\n",
      "step  712 | train loss: 5.16 | val loss: 4.80 | perplexity: 121.15 | lr: 9.97e-04 | norm: 0.5971 | dt: 11728.0383ms | tok/sec: 33.5279\n",
      "step  713 | train loss: 4.87 | val loss: 4.76 | perplexity: 117.28 | lr: 9.99e-04 | norm: 0.9153 | dt: 11743.0427ms | tok/sec: 33.4850\n",
      "step  714 | train loss: 4.64 | val loss: 4.79 | perplexity: 119.71 | lr: 1.00e-03 | norm: 0.6669 | dt: 11747.9243ms | tok/sec: 33.4711\n",
      "step  715 | train loss: 5.15 | val loss: 4.80 | perplexity: 121.37 | lr: 1.00e-03 | norm: 0.5682 | dt: 11720.7634ms | tok/sec: 33.5487\n",
      "step  716 | train loss: 4.65 | val loss: 4.79 | perplexity: 120.05 | lr: 1.00e-03 | norm: 1.3553 | dt: 11717.5410ms | tok/sec: 33.5579\n",
      "step  717 | train loss: 4.80 | val loss: 4.80 | perplexity: 121.62 | lr: 1.00e-03 | norm: 0.8761 | dt: 11722.3361ms | tok/sec: 33.5442\n",
      "step  718 | train loss: 4.52 | val loss: 4.78 | perplexity: 119.12 | lr: 1.00e-03 | norm: 1.0077 | dt: 11720.0782ms | tok/sec: 33.5506\n",
      "step  719 | train loss: 4.62 | val loss: 4.76 | perplexity: 116.34 | lr: 1.00e-03 | norm: 0.7708 | dt: 11764.6141ms | tok/sec: 33.4236\n",
      "step  720 | train loss: 4.62 | val loss: 4.76 | perplexity: 116.93 | lr: 9.99e-04 | norm: 0.6938 | dt: 11731.6210ms | tok/sec: 33.5176\n",
      "step  721 | train loss: 4.58 | val loss: 4.77 | perplexity: 117.58 | lr: 9.99e-04 | norm: 0.5404 | dt: 11734.2062ms | tok/sec: 33.5102\n",
      "step  722 | train loss: 4.75 | val loss: 4.76 | perplexity: 116.64 | lr: 9.99e-04 | norm: 0.5300 | dt: 11699.7237ms | tok/sec: 33.6090\n",
      "step  723 | train loss: 4.63 | val loss: 4.75 | perplexity: 115.99 | lr: 9.98e-04 | norm: 0.6779 | dt: 11729.7454ms | tok/sec: 33.5230\n",
      "step  724 | train loss: 4.71 | val loss: 4.77 | perplexity: 117.95 | lr: 9.98e-04 | norm: 0.5686 | dt: 11745.1489ms | tok/sec: 33.4790\n",
      "step  725 | train loss: 4.46 | val loss: 4.77 | perplexity: 118.03 | lr: 9.97e-04 | norm: 0.6487 | dt: 11723.1722ms | tok/sec: 33.5418\n",
      "step  726 | train loss: 4.56 | val loss: 4.77 | perplexity: 118.17 | lr: 9.97e-04 | norm: 0.5545 | dt: 11702.5361ms | tok/sec: 33.6009\n",
      "step  727 | train loss: 5.13 | val loss: 4.76 | perplexity: 117.09 | lr: 9.96e-04 | norm: 0.5997 | dt: 11718.5102ms | tok/sec: 33.5551\n",
      "step  728 | train loss: 4.69 | val loss: 4.75 | perplexity: 115.02 | lr: 9.95e-04 | norm: 0.7783 | dt: 11726.4831ms | tok/sec: 33.5323\n",
      "step  729 | train loss: 4.75 | val loss: 4.75 | perplexity: 115.47 | lr: 9.95e-04 | norm: 0.5943 | dt: 11737.1254ms | tok/sec: 33.5019\n",
      "step  730 | train loss: 4.95 | val loss: 4.74 | perplexity: 114.11 | lr: 9.94e-04 | norm: 0.5133 | dt: 11728.8198ms | tok/sec: 33.5256\n",
      "step  731 | train loss: 4.73 | val loss: 4.75 | perplexity: 115.36 | lr: 9.93e-04 | norm: 0.6270 | dt: 11737.7086ms | tok/sec: 33.5002\n",
      "step  732 | train loss: 5.12 | val loss: 4.77 | perplexity: 117.45 | lr: 9.92e-04 | norm: 0.6443 | dt: 11758.8842ms | tok/sec: 33.4399\n",
      "step  733 | train loss: 4.75 | val loss: 4.73 | perplexity: 113.20 | lr: 9.91e-04 | norm: 0.7061 | dt: 11722.9826ms | tok/sec: 33.5423\n",
      "step  734 | train loss: 4.64 | val loss: 4.70 | perplexity: 110.20 | lr: 9.90e-04 | norm: 0.7389 | dt: 11732.6207ms | tok/sec: 33.5148\n",
      "step  735 | train loss: 4.85 | val loss: 4.70 | perplexity: 110.16 | lr: 9.89e-04 | norm: 0.7031 | dt: 11731.6184ms | tok/sec: 33.5176\n",
      "step  736 | train loss: 4.76 | val loss: 4.72 | perplexity: 111.78 | lr: 9.88e-04 | norm: 0.9306 | dt: 11753.8123ms | tok/sec: 33.4543\n",
      "step  737 | train loss: 4.61 | val loss: 4.73 | perplexity: 113.61 | lr: 9.87e-04 | norm: 0.5819 | dt: 11763.1748ms | tok/sec: 33.4277\n",
      "step  738 | train loss: 4.67 | val loss: 4.72 | perplexity: 112.56 | lr: 9.86e-04 | norm: 0.6504 | dt: 11731.7412ms | tok/sec: 33.5173\n",
      "step  739 | train loss: 4.76 | val loss: 4.70 | perplexity: 109.58 | lr: 9.84e-04 | norm: 0.5927 | dt: 11729.5682ms | tok/sec: 33.5235\n",
      "step  740 | train loss: 4.97 | val loss: 4.70 | perplexity: 110.07 | lr: 9.83e-04 | norm: 0.5508 | dt: 11716.8524ms | tok/sec: 33.5599\n",
      "step  741 | train loss: 5.12 | val loss: 4.70 | perplexity: 109.61 | lr: 9.82e-04 | norm: 0.5449 | dt: 11733.7832ms | tok/sec: 33.5114\n",
      "step  742 | train loss: 5.14 | val loss: 4.69 | perplexity: 108.70 | lr: 9.80e-04 | norm: 0.6411 | dt: 11732.6522ms | tok/sec: 33.5147\n",
      "step  743 | train loss: 5.13 | val loss: 4.68 | perplexity: 107.83 | lr: 9.79e-04 | norm: 0.4956 | dt: 11731.8244ms | tok/sec: 33.5170\n",
      "step  744 | train loss: 5.45 | val loss: 4.69 | perplexity: 108.63 | lr: 9.77e-04 | norm: 0.7187 | dt: 11709.9211ms | tok/sec: 33.5797\n",
      "step  745 | train loss: 4.76 | val loss: 4.67 | perplexity: 106.79 | lr: 9.76e-04 | norm: 0.9348 | dt: 11731.7727ms | tok/sec: 33.5172\n",
      "step  746 | train loss: 4.57 | val loss: 4.68 | perplexity: 107.57 | lr: 9.74e-04 | norm: 0.6320 | dt: 11719.6491ms | tok/sec: 33.5519\n",
      "step  747 | train loss: 4.81 | val loss: 4.69 | perplexity: 108.66 | lr: 9.72e-04 | norm: 0.9954 | dt: 11730.7889ms | tok/sec: 33.5200\n",
      "step  748 | train loss: 4.47 | val loss: 4.68 | perplexity: 108.28 | lr: 9.71e-04 | norm: 0.6379 | dt: 11717.1111ms | tok/sec: 33.5591\n",
      "step  749 | train loss: 4.58 | val loss: 4.68 | perplexity: 107.99 | lr: 9.69e-04 | norm: 0.4545 | dt: 11748.2798ms | tok/sec: 33.4701\n",
      "step  750 | train loss: 4.82 | val loss: 4.67 | perplexity: 106.62 | lr: 9.67e-04 | norm: 0.5496 | dt: 11723.2473ms | tok/sec: 33.5416\n",
      "step  751 | train loss: 4.78 | val loss: 4.66 | perplexity: 105.86 | lr: 9.65e-04 | norm: 0.4534 | dt: 11712.8391ms | tok/sec: 33.5714\n",
      "step  752 | train loss: 4.57 | val loss: 4.67 | perplexity: 106.37 | lr: 9.63e-04 | norm: 0.5374 | dt: 11702.3280ms | tok/sec: 33.6015\n",
      "step  753 | train loss: 4.81 | val loss: 4.65 | perplexity: 104.23 | lr: 9.61e-04 | norm: 0.5050 | dt: 11707.2020ms | tok/sec: 33.5875\n",
      "step  754 | train loss: 5.01 | val loss: 4.63 | perplexity: 103.01 | lr: 9.59e-04 | norm: 0.4976 | dt: 11709.5656ms | tok/sec: 33.5808\n",
      "step  755 | train loss: 5.51 | val loss: 4.65 | perplexity: 104.19 | lr: 9.57e-04 | norm: 0.9229 | dt: 11748.3120ms | tok/sec: 33.4700\n",
      "step  756 | train loss: 4.81 | val loss: 4.65 | perplexity: 104.99 | lr: 9.55e-04 | norm: 0.5713 | dt: 11716.9449ms | tok/sec: 33.5596\n",
      "step  757 | train loss: 4.57 | val loss: 4.64 | perplexity: 103.49 | lr: 9.53e-04 | norm: 0.5681 | dt: 11694.5970ms | tok/sec: 33.6237\n",
      "step  758 | train loss: 4.85 | val loss: 4.72 | perplexity: 112.51 | lr: 9.50e-04 | norm: 1.0331 | dt: 11697.7847ms | tok/sec: 33.6146\n",
      "step  759 | train loss: 5.01 | val loss: 4.69 | perplexity: 108.93 | lr: 9.48e-04 | norm: 1.0626 | dt: 11714.3207ms | tok/sec: 33.5671\n",
      "step  760 | train loss: 4.75 | val loss: 4.68 | perplexity: 107.47 | lr: 9.46e-04 | norm: 0.6940 | dt: 11736.3558ms | tok/sec: 33.5041\n",
      "step  761 | train loss: 4.62 | val loss: 4.65 | perplexity: 105.05 | lr: 9.43e-04 | norm: 0.5097 | dt: 11726.5625ms | tok/sec: 33.5321\n",
      "step  762 | train loss: 4.73 | val loss: 4.64 | perplexity: 103.09 | lr: 9.41e-04 | norm: 0.6816 | dt: 11719.0254ms | tok/sec: 33.5536\n",
      "step  763 | train loss: 4.71 | val loss: 4.63 | perplexity: 102.53 | lr: 9.38e-04 | norm: 0.5973 | dt: 11735.1809ms | tok/sec: 33.5075\n",
      "step  764 | train loss: 4.76 | val loss: 4.63 | perplexity: 102.35 | lr: 9.36e-04 | norm: 0.5458 | dt: 11715.8594ms | tok/sec: 33.5627\n",
      "step  765 | train loss: 4.89 | val loss: 4.62 | perplexity: 101.96 | lr: 9.33e-04 | norm: 0.5645 | dt: 11724.1631ms | tok/sec: 33.5389\n",
      "step  766 | train loss: 4.50 | val loss: 4.61 | perplexity: 100.61 | lr: 9.31e-04 | norm: 0.5303 | dt: 11721.6790ms | tok/sec: 33.5460\n",
      "step  767 | train loss: 4.49 | val loss: 4.61 | perplexity: 100.43 | lr: 9.28e-04 | norm: 0.6140 | dt: 11707.2489ms | tok/sec: 33.5874\n",
      "step  768 | train loss: 4.69 | val loss: 4.61 | perplexity: 100.05 | lr: 9.25e-04 | norm: 0.6157 | dt: 11721.5164ms | tok/sec: 33.5465\n",
      "step  769 | train loss: 4.49 | val loss: 4.61 | perplexity: 100.19 | lr: 9.23e-04 | norm: 0.5382 | dt: 11727.1831ms | tok/sec: 33.5303\n",
      "step  770 | train loss: 4.78 | val loss: 4.61 | perplexity: 100.12 | lr: 9.20e-04 | norm: 0.5332 | dt: 11732.4038ms | tok/sec: 33.5154\n",
      "step  771 | train loss: 5.19 | val loss: 4.62 | perplexity: 101.25 | lr: 9.17e-04 | norm: 0.5786 | dt: 11714.3979ms | tok/sec: 33.5669\n",
      "step  772 | train loss: 4.93 | val loss: 4.59 | perplexity: 98.90 | lr: 9.14e-04 | norm: 0.6577 | dt: 11716.1314ms | tok/sec: 33.5619\n",
      "step  773 | train loss: 5.03 | val loss: 4.60 | perplexity: 99.72 | lr: 9.11e-04 | norm: 0.5576 | dt: 11694.9921ms | tok/sec: 33.6226\n",
      "step  774 | train loss: 4.63 | val loss: 4.59 | perplexity: 98.40 | lr: 9.08e-04 | norm: 0.5048 | dt: 11707.1295ms | tok/sec: 33.5877\n",
      "step  775 | train loss: 4.49 | val loss: 4.59 | perplexity: 98.69 | lr: 9.05e-04 | norm: 0.4527 | dt: 11716.7246ms | tok/sec: 33.5602\n",
      "step  776 | train loss: 5.13 | val loss: 4.59 | perplexity: 98.75 | lr: 9.02e-04 | norm: 0.6704 | dt: 11726.8693ms | tok/sec: 33.5312\n",
      "step  777 | train loss: 5.10 | val loss: 4.62 | perplexity: 101.14 | lr: 8.99e-04 | norm: 0.5606 | dt: 11722.5103ms | tok/sec: 33.5437\n",
      "step  778 | train loss: 4.53 | val loss: 4.59 | perplexity: 98.41 | lr: 8.96e-04 | norm: 0.7923 | dt: 11670.7363ms | tok/sec: 33.6925\n",
      "step  779 | train loss: 4.83 | val loss: 4.58 | perplexity: 97.78 | lr: 8.93e-04 | norm: 0.5051 | dt: 11732.2388ms | tok/sec: 33.5159\n",
      "step  780 | train loss: 4.73 | val loss: 4.59 | perplexity: 98.31 | lr: 8.89e-04 | norm: 0.6567 | dt: 11724.2362ms | tok/sec: 33.5387\n",
      "step  781 | train loss: 4.54 | val loss: 4.59 | perplexity: 98.06 | lr: 8.86e-04 | norm: 0.6424 | dt: 11707.5999ms | tok/sec: 33.5864\n",
      "step  782 | train loss: 4.47 | val loss: 4.60 | perplexity: 99.22 | lr: 8.83e-04 | norm: 0.5460 | dt: 11688.3957ms | tok/sec: 33.6416\n",
      "step  783 | train loss: 4.58 | val loss: 4.59 | perplexity: 98.36 | lr: 8.79e-04 | norm: 0.6211 | dt: 11712.5766ms | tok/sec: 33.5721\n",
      "step  784 | train loss: 4.59 | val loss: 4.57 | perplexity: 96.67 | lr: 8.76e-04 | norm: 0.5038 | dt: 11683.6970ms | tok/sec: 33.6551\n",
      "step  785 | train loss: 4.63 | val loss: 4.56 | perplexity: 95.92 | lr: 8.73e-04 | norm: 0.4289 | dt: 11719.3923ms | tok/sec: 33.5526\n",
      "step  786 | train loss: 4.91 | val loss: 4.56 | perplexity: 95.40 | lr: 8.69e-04 | norm: 0.4575 | dt: 11701.7925ms | tok/sec: 33.6031\n",
      "step  787 | train loss: 4.56 | val loss: 4.55 | perplexity: 94.49 | lr: 8.66e-04 | norm: 0.4524 | dt: 11705.7717ms | tok/sec: 33.5916\n",
      "step  788 | train loss: 4.64 | val loss: 4.56 | perplexity: 95.49 | lr: 8.62e-04 | norm: 0.4547 | dt: 11700.0241ms | tok/sec: 33.6081\n",
      "step  789 | train loss: 4.96 | val loss: 4.56 | perplexity: 95.30 | lr: 8.58e-04 | norm: 0.4003 | dt: 11711.3879ms | tok/sec: 33.5755\n",
      "step  790 | train loss: 4.68 | val loss: 4.54 | perplexity: 94.16 | lr: 8.55e-04 | norm: 0.4894 | dt: 11726.9323ms | tok/sec: 33.5310\n",
      "step  791 | train loss: 4.62 | val loss: 4.53 | perplexity: 92.89 | lr: 8.51e-04 | norm: 0.4157 | dt: 11705.3740ms | tok/sec: 33.5928\n",
      "step  792 | train loss: 4.71 | val loss: 4.53 | perplexity: 92.99 | lr: 8.47e-04 | norm: 0.4218 | dt: 11689.0063ms | tok/sec: 33.6398\n",
      "step  793 | train loss: 4.54 | val loss: 4.53 | perplexity: 92.95 | lr: 8.44e-04 | norm: 0.4394 | dt: 11706.9192ms | tok/sec: 33.5883\n",
      "step  794 | train loss: 4.65 | val loss: 4.54 | perplexity: 94.11 | lr: 8.40e-04 | norm: 0.4716 | dt: 11731.3924ms | tok/sec: 33.5183\n",
      "step  795 | train loss: 4.75 | val loss: 4.52 | perplexity: 92.04 | lr: 8.36e-04 | norm: 0.5907 | dt: 11715.4698ms | tok/sec: 33.5638\n",
      "step  796 | train loss: 4.63 | val loss: 4.51 | perplexity: 90.80 | lr: 8.32e-04 | norm: 0.4884 | dt: 11715.3382ms | tok/sec: 33.5642\n",
      "step  797 | train loss: 4.52 | val loss: 4.51 | perplexity: 90.48 | lr: 8.28e-04 | norm: 0.4054 | dt: 11720.5625ms | tok/sec: 33.5492\n",
      "step  798 | train loss: 4.36 | val loss: 4.51 | perplexity: 90.89 | lr: 8.24e-04 | norm: 0.4507 | dt: 11723.1126ms | tok/sec: 33.5419\n",
      "step  799 | train loss: 4.54 | val loss: 4.51 | perplexity: 90.49 | lr: 8.20e-04 | norm: 0.4463 | dt: 11736.6364ms | tok/sec: 33.5033\n",
      "step  800 | train loss: 4.64 | val loss: 4.51 | perplexity: 90.48 | lr: 8.17e-04 | norm: 0.4782 | dt: 11732.7287ms | tok/sec: 33.5145\n",
      "step  801 | train loss: 4.51 | val loss: 4.49 | perplexity: 89.06 | lr: 8.12e-04 | norm: 0.4999 | dt: 11720.6190ms | tok/sec: 33.5491\n",
      "step  802 | train loss: 4.73 | val loss: 4.49 | perplexity: 88.97 | lr: 8.08e-04 | norm: 0.4737 | dt: 11726.4748ms | tok/sec: 33.5323\n",
      "step  803 | train loss: 4.64 | val loss: 4.50 | perplexity: 90.46 | lr: 8.04e-04 | norm: 0.4304 | dt: 11726.0792ms | tok/sec: 33.5335\n",
      "step  804 | train loss: 4.61 | val loss: 4.51 | perplexity: 90.77 | lr: 8.00e-04 | norm: 0.5525 | dt: 11743.3078ms | tok/sec: 33.4843\n",
      "step  805 | train loss: 4.54 | val loss: 4.51 | perplexity: 91.00 | lr: 7.96e-04 | norm: 0.4911 | dt: 11716.1734ms | tok/sec: 33.5618\n",
      "step  806 | train loss: 4.33 | val loss: 4.49 | perplexity: 88.70 | lr: 7.92e-04 | norm: 0.7258 | dt: 11702.2028ms | tok/sec: 33.6019\n",
      "step  807 | train loss: 4.32 | val loss: 4.48 | perplexity: 88.24 | lr: 7.88e-04 | norm: 0.6548 | dt: 11764.3504ms | tok/sec: 33.4244\n",
      "step  808 | train loss: 4.61 | val loss: 4.49 | perplexity: 88.83 | lr: 7.84e-04 | norm: 0.4569 | dt: 11721.0867ms | tok/sec: 33.5477\n",
      "step  809 | train loss: 4.56 | val loss: 4.51 | perplexity: 90.54 | lr: 7.79e-04 | norm: 0.4844 | dt: 11708.5652ms | tok/sec: 33.5836\n",
      "step  810 | train loss: 4.66 | val loss: 4.51 | perplexity: 91.36 | lr: 7.75e-04 | norm: 0.5281 | dt: 11725.8835ms | tok/sec: 33.5340\n",
      "step  811 | train loss: 4.84 | val loss: 4.51 | perplexity: 90.68 | lr: 7.71e-04 | norm: 0.5290 | dt: 11706.7051ms | tok/sec: 33.5890\n",
      "step  812 | train loss: 4.67 | val loss: 4.51 | perplexity: 90.82 | lr: 7.66e-04 | norm: 0.4948 | dt: 11714.9229ms | tok/sec: 33.5654\n",
      "step  813 | train loss: 5.22 | val loss: 4.51 | perplexity: 90.82 | lr: 7.62e-04 | norm: 0.4314 | dt: 11705.7226ms | tok/sec: 33.5918\n",
      "step  814 | train loss: 4.57 | val loss: 4.50 | perplexity: 90.29 | lr: 7.58e-04 | norm: 0.4633 | dt: 11715.9362ms | tok/sec: 33.5625\n",
      "step  815 | train loss: 4.55 | val loss: 4.51 | perplexity: 90.94 | lr: 7.53e-04 | norm: 0.3882 | dt: 11703.3477ms | tok/sec: 33.5986\n",
      "step  816 | train loss: 4.68 | val loss: 4.52 | perplexity: 91.66 | lr: 7.49e-04 | norm: 0.5376 | dt: 11724.6497ms | tok/sec: 33.5375\n",
      "step  817 | train loss: 4.68 | val loss: 4.50 | perplexity: 89.76 | lr: 7.44e-04 | norm: 0.5857 | dt: 11731.7266ms | tok/sec: 33.5173\n",
      "step  818 | train loss: 4.58 | val loss: 4.49 | perplexity: 89.29 | lr: 7.40e-04 | norm: 0.4409 | dt: 11748.0497ms | tok/sec: 33.4707\n",
      "step  819 | train loss: 4.81 | val loss: 4.50 | perplexity: 89.67 | lr: 7.35e-04 | norm: 0.4170 | dt: 11728.8170ms | tok/sec: 33.5256\n",
      "step  820 | train loss: 4.79 | val loss: 4.49 | perplexity: 88.85 | lr: 7.31e-04 | norm: 0.4554 | dt: 11739.5842ms | tok/sec: 33.4949\n",
      "step  821 | train loss: 4.53 | val loss: 4.48 | perplexity: 87.86 | lr: 7.26e-04 | norm: 0.4917 | dt: 11720.4316ms | tok/sec: 33.5496\n",
      "step  822 | train loss: 4.63 | val loss: 4.47 | perplexity: 87.45 | lr: 7.22e-04 | norm: 0.4885 | dt: 11732.5397ms | tok/sec: 33.5150\n",
      "step  823 | train loss: 4.50 | val loss: 4.47 | perplexity: 87.51 | lr: 7.17e-04 | norm: 0.6588 | dt: 11721.3397ms | tok/sec: 33.5470\n",
      "step  824 | train loss: 4.80 | val loss: 4.48 | perplexity: 88.59 | lr: 7.12e-04 | norm: 0.4945 | dt: 11707.3851ms | tok/sec: 33.5870\n",
      "step  825 | train loss: 4.82 | val loss: 4.49 | perplexity: 89.13 | lr: 7.08e-04 | norm: 0.4022 | dt: 11723.3663ms | tok/sec: 33.5412\n",
      "step  826 | train loss: 4.88 | val loss: 4.47 | perplexity: 87.16 | lr: 7.03e-04 | norm: 0.5036 | dt: 11717.2921ms | tok/sec: 33.5586\n",
      "step  827 | train loss: 4.72 | val loss: 4.46 | perplexity: 86.42 | lr: 6.98e-04 | norm: 0.4065 | dt: 11726.0051ms | tok/sec: 33.5337\n",
      "step  828 | train loss: 4.55 | val loss: 4.45 | perplexity: 85.57 | lr: 6.94e-04 | norm: 0.4550 | dt: 11715.6050ms | tok/sec: 33.5634\n",
      "step  829 | train loss: 4.58 | val loss: 4.44 | perplexity: 84.99 | lr: 6.89e-04 | norm: 0.3993 | dt: 11739.0721ms | tok/sec: 33.4963\n",
      "step  830 | train loss: 4.52 | val loss: 4.43 | perplexity: 83.77 | lr: 6.84e-04 | norm: 0.4375 | dt: 11737.6962ms | tok/sec: 33.5003\n",
      "step  831 | train loss: 4.72 | val loss: 4.43 | perplexity: 83.65 | lr: 6.80e-04 | norm: 0.4293 | dt: 11716.6722ms | tok/sec: 33.5604\n",
      "step  832 | train loss: 4.73 | val loss: 4.43 | perplexity: 83.53 | lr: 6.75e-04 | norm: 0.4738 | dt: 11727.1652ms | tok/sec: 33.5304\n",
      "step  833 | train loss: 4.47 | val loss: 4.42 | perplexity: 82.98 | lr: 6.70e-04 | norm: 0.3725 | dt: 11772.9759ms | tok/sec: 33.3999\n",
      "step  834 | train loss: 4.59 | val loss: 4.42 | perplexity: 82.94 | lr: 6.65e-04 | norm: 0.4031 | dt: 11753.2234ms | tok/sec: 33.4560\n",
      "step  835 | train loss: 5.03 | val loss: 4.43 | perplexity: 83.52 | lr: 6.60e-04 | norm: 0.4710 | dt: 11744.6742ms | tok/sec: 33.4804\n",
      "step  836 | train loss: 4.82 | val loss: 4.44 | perplexity: 84.49 | lr: 6.56e-04 | norm: 0.5556 | dt: 11748.1458ms | tok/sec: 33.4705\n",
      "step  837 | train loss: 4.78 | val loss: 4.43 | perplexity: 84.09 | lr: 6.51e-04 | norm: 0.4734 | dt: 11699.3358ms | tok/sec: 33.6101\n",
      "step  838 | train loss: 4.74 | val loss: 4.42 | perplexity: 83.14 | lr: 6.46e-04 | norm: 0.4990 | dt: 11758.2088ms | tok/sec: 33.4418\n",
      "step  839 | train loss: 4.08 | val loss: 4.42 | perplexity: 82.98 | lr: 6.41e-04 | norm: 0.4501 | dt: 11716.7509ms | tok/sec: 33.5602\n",
      "step  840 | train loss: 4.90 | val loss: 4.42 | perplexity: 82.72 | lr: 6.36e-04 | norm: 0.4585 | dt: 11723.8791ms | tok/sec: 33.5398\n",
      "step  841 | train loss: 4.87 | val loss: 4.42 | perplexity: 82.74 | lr: 6.31e-04 | norm: 0.4424 | dt: 11725.1709ms | tok/sec: 33.5361\n",
      "step  842 | train loss: 4.74 | val loss: 4.42 | perplexity: 83.23 | lr: 6.27e-04 | norm: 0.4231 | dt: 11726.8813ms | tok/sec: 33.5312\n",
      "step  843 | train loss: 5.11 | val loss: 4.43 | perplexity: 83.81 | lr: 6.22e-04 | norm: 0.8903 | dt: 11751.3604ms | tok/sec: 33.4613\n",
      "step  844 | train loss: 4.53 | val loss: 4.43 | perplexity: 83.66 | lr: 6.17e-04 | norm: 0.5685 | dt: 11737.8244ms | tok/sec: 33.4999\n",
      "step  845 | train loss: 4.85 | val loss: 4.43 | perplexity: 84.24 | lr: 6.12e-04 | norm: 0.5623 | dt: 11736.3222ms | tok/sec: 33.5042\n",
      "step  846 | train loss: 5.09 | val loss: 4.44 | perplexity: 84.65 | lr: 6.07e-04 | norm: 0.5108 | dt: 11727.9797ms | tok/sec: 33.5280\n",
      "step  847 | train loss: 5.01 | val loss: 4.43 | perplexity: 84.26 | lr: 6.02e-04 | norm: 0.6057 | dt: 11738.8721ms | tok/sec: 33.4969\n",
      "step  848 | train loss: 4.72 | val loss: 4.43 | perplexity: 83.88 | lr: 5.97e-04 | norm: 0.4619 | dt: 11747.8089ms | tok/sec: 33.4714\n",
      "step  849 | train loss: 4.53 | val loss: 4.42 | perplexity: 83.11 | lr: 5.92e-04 | norm: 0.6332 | dt: 11738.6296ms | tok/sec: 33.4976\n",
      "step  850 | train loss: 4.75 | val loss: 4.42 | perplexity: 83.26 | lr: 5.87e-04 | norm: 0.4160 | dt: 11729.6948ms | tok/sec: 33.5231\n",
      "step  851 | train loss: 4.65 | val loss: 4.41 | perplexity: 82.47 | lr: 5.82e-04 | norm: 0.4511 | dt: 11723.7568ms | tok/sec: 33.5401\n",
      "step  852 | train loss: 4.63 | val loss: 4.40 | perplexity: 81.79 | lr: 5.77e-04 | norm: 0.3697 | dt: 11761.5087ms | tok/sec: 33.4324\n",
      "step  853 | train loss: 4.73 | val loss: 4.39 | perplexity: 80.85 | lr: 5.72e-04 | norm: 0.4523 | dt: 11737.8480ms | tok/sec: 33.4998\n",
      "step  854 | train loss: 5.55 | val loss: 4.40 | perplexity: 81.34 | lr: 5.67e-04 | norm: 0.6515 | dt: 11744.0803ms | tok/sec: 33.4821\n",
      "step  855 | train loss: 4.63 | val loss: 4.41 | perplexity: 81.98 | lr: 5.62e-04 | norm: 0.5589 | dt: 11704.4570ms | tok/sec: 33.5954\n",
      "step  856 | train loss: 4.65 | val loss: 4.40 | perplexity: 81.76 | lr: 5.57e-04 | norm: 0.4455 | dt: 11748.1086ms | tok/sec: 33.4706\n",
      "step  857 | train loss: 4.88 | val loss: 4.40 | perplexity: 81.49 | lr: 5.52e-04 | norm: 0.4272 | dt: 11741.4227ms | tok/sec: 33.4896\n",
      "step  858 | train loss: 4.58 | val loss: 4.39 | perplexity: 80.50 | lr: 5.48e-04 | norm: 0.4885 | dt: 11705.7400ms | tok/sec: 33.5917\n",
      "step  859 | train loss: 4.66 | val loss: 4.39 | perplexity: 80.57 | lr: 5.43e-04 | norm: 0.3677 | dt: 11720.3653ms | tok/sec: 33.5498\n",
      "step  860 | train loss: 4.40 | val loss: 4.40 | perplexity: 81.78 | lr: 5.38e-04 | norm: 0.4016 | dt: 11756.7997ms | tok/sec: 33.4458\n",
      "step  861 | train loss: 4.98 | val loss: 4.40 | perplexity: 81.42 | lr: 5.33e-04 | norm: 0.9473 | dt: 11723.1228ms | tok/sec: 33.5419\n",
      "step  862 | train loss: 4.33 | val loss: 4.40 | perplexity: 81.32 | lr: 5.28e-04 | norm: 0.5346 | dt: 11720.8004ms | tok/sec: 33.5486\n",
      "step  863 | train loss: 4.41 | val loss: 4.39 | perplexity: 80.80 | lr: 5.23e-04 | norm: 0.5818 | dt: 11713.0589ms | tok/sec: 33.5707\n",
      "step  864 | train loss: 4.32 | val loss: 4.39 | perplexity: 80.96 | lr: 5.18e-04 | norm: 0.4084 | dt: 11712.8825ms | tok/sec: 33.5712\n",
      "step  865 | train loss: 4.41 | val loss: 4.39 | perplexity: 80.85 | lr: 5.13e-04 | norm: 0.3589 | dt: 11727.5839ms | tok/sec: 33.5292\n",
      "step  866 | train loss: 4.86 | val loss: 4.39 | perplexity: 80.41 | lr: 5.08e-04 | norm: 0.6104 | dt: 11720.5014ms | tok/sec: 33.5494\n",
      "step  867 | train loss: 4.73 | val loss: 4.39 | perplexity: 80.87 | lr: 5.03e-04 | norm: 0.4457 | dt: 11729.6519ms | tok/sec: 33.5232\n",
      "step  868 | train loss: 4.56 | val loss: 4.39 | perplexity: 80.36 | lr: 4.98e-04 | norm: 0.5427 | dt: 11753.4013ms | tok/sec: 33.4555\n",
      "step  869 | train loss: 4.55 | val loss: 4.39 | perplexity: 80.89 | lr: 4.93e-04 | norm: 0.4302 | dt: 11725.4386ms | tok/sec: 33.5353\n",
      "step  870 | train loss: 4.50 | val loss: 4.40 | perplexity: 81.46 | lr: 4.88e-04 | norm: 0.4964 | dt: 11700.0921ms | tok/sec: 33.6079\n",
      "step  871 | train loss: 4.43 | val loss: 4.40 | perplexity: 81.11 | lr: 4.83e-04 | norm: 0.6171 | dt: 11741.0409ms | tok/sec: 33.4907\n",
      "step  872 | train loss: 4.78 | val loss: 4.40 | perplexity: 81.23 | lr: 4.78e-04 | norm: 0.4823 | dt: 11701.0231ms | tok/sec: 33.6053\n",
      "step  873 | train loss: 4.82 | val loss: 4.40 | perplexity: 81.38 | lr: 4.73e-04 | norm: 0.5596 | dt: 11737.0389ms | tok/sec: 33.5021\n",
      "step  874 | train loss: 4.90 | val loss: 4.39 | perplexity: 80.82 | lr: 4.69e-04 | norm: 0.9949 | dt: 11700.3565ms | tok/sec: 33.6072\n",
      "step  875 | train loss: 4.29 | val loss: 4.38 | perplexity: 79.44 | lr: 4.64e-04 | norm: 0.4758 | dt: 11750.5736ms | tok/sec: 33.4636\n",
      "step  876 | train loss: 4.68 | val loss: 4.37 | perplexity: 79.04 | lr: 4.59e-04 | norm: 0.4114 | dt: 11693.5906ms | tok/sec: 33.6266\n",
      "step  877 | train loss: 4.49 | val loss: 4.37 | perplexity: 78.92 | lr: 4.54e-04 | norm: 0.4748 | dt: 11709.1882ms | tok/sec: 33.5818\n",
      "step  878 | train loss: 4.42 | val loss: 4.37 | perplexity: 78.77 | lr: 4.49e-04 | norm: 0.4205 | dt: 11726.9440ms | tok/sec: 33.5310\n",
      "step  879 | train loss: 4.79 | val loss: 4.37 | perplexity: 78.72 | lr: 4.44e-04 | norm: 0.4376 | dt: 11722.4352ms | tok/sec: 33.5439\n",
      "step  880 | train loss: 4.67 | val loss: 4.37 | perplexity: 78.85 | lr: 4.40e-04 | norm: 0.4328 | dt: 11705.5666ms | tok/sec: 33.5922\n",
      "step  881 | train loss: 4.59 | val loss: 4.38 | perplexity: 79.58 | lr: 4.35e-04 | norm: 0.4101 | dt: 11726.1636ms | tok/sec: 33.5332\n",
      "step  882 | train loss: 4.68 | val loss: 4.37 | perplexity: 79.23 | lr: 4.30e-04 | norm: 0.4201 | dt: 11729.5997ms | tok/sec: 33.5234\n",
      "step  883 | train loss: 4.74 | val loss: 4.35 | perplexity: 77.60 | lr: 4.25e-04 | norm: 0.5290 | dt: 11725.3816ms | tok/sec: 33.5355\n",
      "step  884 | train loss: 4.47 | val loss: 4.33 | perplexity: 75.90 | lr: 4.20e-04 | norm: 0.4680 | dt: 11710.3736ms | tok/sec: 33.5784\n",
      "step  885 | train loss: 4.79 | val loss: 4.32 | perplexity: 74.92 | lr: 4.16e-04 | norm: 0.5078 | dt: 11730.0718ms | tok/sec: 33.5220\n",
      "step  886 | train loss: 4.42 | val loss: 4.31 | perplexity: 74.28 | lr: 4.11e-04 | norm: 0.3890 | dt: 11722.7917ms | tok/sec: 33.5429\n",
      "step  887 | train loss: 4.85 | val loss: 4.31 | perplexity: 74.44 | lr: 4.06e-04 | norm: 0.5475 | dt: 11718.7066ms | tok/sec: 33.5546\n",
      "step  888 | train loss: 4.32 | val loss: 4.32 | perplexity: 75.33 | lr: 4.02e-04 | norm: 0.4628 | dt: 11719.3434ms | tok/sec: 33.5527\n",
      "step  889 | train loss: 4.64 | val loss: 4.32 | perplexity: 74.93 | lr: 3.97e-04 | norm: 0.4778 | dt: 11734.8657ms | tok/sec: 33.5084\n",
      "step  890 | train loss: 4.57 | val loss: 4.30 | perplexity: 74.05 | lr: 3.92e-04 | norm: 0.4608 | dt: 11721.8747ms | tok/sec: 33.5455\n",
      "step  891 | train loss: 4.83 | val loss: 4.30 | perplexity: 73.78 | lr: 3.88e-04 | norm: 0.3894 | dt: 11732.2927ms | tok/sec: 33.5157\n",
      "step  892 | train loss: 4.83 | val loss: 4.30 | perplexity: 73.62 | lr: 3.83e-04 | norm: 0.4991 | dt: 11705.1177ms | tok/sec: 33.5935\n",
      "step  893 | train loss: 4.49 | val loss: 4.30 | perplexity: 73.76 | lr: 3.78e-04 | norm: 0.3810 | dt: 11700.7349ms | tok/sec: 33.6061\n",
      "step  894 | train loss: 4.22 | val loss: 4.30 | perplexity: 73.58 | lr: 3.74e-04 | norm: 0.5265 | dt: 11693.4381ms | tok/sec: 33.6271\n",
      "step  895 | train loss: 4.70 | val loss: 4.30 | perplexity: 73.50 | lr: 3.69e-04 | norm: 0.4403 | dt: 11713.2208ms | tok/sec: 33.5703\n",
      "step  896 | train loss: 4.38 | val loss: 4.30 | perplexity: 73.44 | lr: 3.65e-04 | norm: 0.4337 | dt: 11707.5896ms | tok/sec: 33.5864\n",
      "step  897 | train loss: 4.56 | val loss: 4.30 | perplexity: 73.35 | lr: 3.60e-04 | norm: 0.3825 | dt: 11689.6813ms | tok/sec: 33.6379\n",
      "step  898 | train loss: 4.60 | val loss: 4.29 | perplexity: 73.08 | lr: 3.56e-04 | norm: 0.3766 | dt: 11686.2016ms | tok/sec: 33.6479\n",
      "step  899 | train loss: 4.54 | val loss: 4.28 | perplexity: 72.45 | lr: 3.51e-04 | norm: 0.3582 | dt: 11722.2130ms | tok/sec: 33.5445\n",
      "step  900 | train loss: 4.71 | val loss: 4.28 | perplexity: 72.19 | lr: 3.47e-04 | norm: 0.6372 | dt: 11715.4396ms | tok/sec: 33.5639\n",
      "step  901 | train loss: 4.48 | val loss: 4.28 | perplexity: 72.14 | lr: 3.42e-04 | norm: 0.3824 | dt: 11705.3010ms | tok/sec: 33.5930\n",
      "step  902 | train loss: 4.55 | val loss: 4.28 | perplexity: 72.17 | lr: 3.38e-04 | norm: 0.4289 | dt: 11688.2353ms | tok/sec: 33.6420\n",
      "step  903 | train loss: 4.56 | val loss: 4.28 | perplexity: 72.42 | lr: 3.34e-04 | norm: 0.3493 | dt: 11689.7388ms | tok/sec: 33.6377\n",
      "step  904 | train loss: 4.52 | val loss: 4.28 | perplexity: 72.42 | lr: 3.29e-04 | norm: 0.3628 | dt: 11728.7681ms | tok/sec: 33.5258\n",
      "step  905 | train loss: 4.42 | val loss: 4.28 | perplexity: 72.42 | lr: 3.25e-04 | norm: 0.3572 | dt: 11703.4812ms | tok/sec: 33.5982\n",
      "step  906 | train loss: 5.77 | val loss: 4.29 | perplexity: 72.80 | lr: 3.21e-04 | norm: 1.9371 | dt: 11718.1172ms | tok/sec: 33.5562\n",
      "step  907 | train loss: 4.59 | val loss: 4.29 | perplexity: 72.71 | lr: 3.16e-04 | norm: 0.4066 | dt: 11718.7395ms | tok/sec: 33.5545\n",
      "step  908 | train loss: 4.50 | val loss: 4.28 | perplexity: 72.32 | lr: 3.12e-04 | norm: 0.4603 | dt: 11739.8841ms | tok/sec: 33.4940\n",
      "step  909 | train loss: 4.58 | val loss: 4.27 | perplexity: 71.71 | lr: 3.08e-04 | norm: 0.4835 | dt: 11709.2943ms | tok/sec: 33.5815\n",
      "step  910 | train loss: 4.33 | val loss: 4.27 | perplexity: 71.51 | lr: 3.04e-04 | norm: 0.4429 | dt: 11719.8563ms | tok/sec: 33.5513\n",
      "step  911 | train loss: 4.65 | val loss: 4.27 | perplexity: 71.49 | lr: 3.00e-04 | norm: 0.3497 | dt: 11720.3143ms | tok/sec: 33.5500\n",
      "step  912 | train loss: 4.81 | val loss: 4.27 | perplexity: 71.56 | lr: 2.96e-04 | norm: 0.3469 | dt: 11673.0778ms | tok/sec: 33.6857\n",
      "step  913 | train loss: 4.42 | val loss: 4.27 | perplexity: 71.59 | lr: 2.92e-04 | norm: 0.4023 | dt: 11708.0889ms | tok/sec: 33.5850\n",
      "step  914 | train loss: 4.75 | val loss: 4.27 | perplexity: 71.48 | lr: 2.88e-04 | norm: 0.4595 | dt: 11710.4628ms | tok/sec: 33.5782\n",
      "step  915 | train loss: 4.33 | val loss: 4.27 | perplexity: 71.45 | lr: 2.83e-04 | norm: 0.3530 | dt: 11720.5625ms | tok/sec: 33.5492\n",
      "step  916 | train loss: 4.59 | val loss: 4.27 | perplexity: 71.44 | lr: 2.80e-04 | norm: 0.4859 | dt: 11727.1194ms | tok/sec: 33.5305\n",
      "step  917 | train loss: 4.79 | val loss: 4.27 | perplexity: 71.32 | lr: 2.76e-04 | norm: 0.4247 | dt: 11715.4889ms | tok/sec: 33.5638\n",
      "step  918 | train loss: 4.60 | val loss: 4.27 | perplexity: 71.43 | lr: 2.72e-04 | norm: 0.4556 | dt: 11692.9507ms | tok/sec: 33.6285\n",
      "step  919 | train loss: 4.84 | val loss: 4.27 | perplexity: 71.55 | lr: 2.68e-04 | norm: 0.4199 | dt: 11697.4988ms | tok/sec: 33.6154\n",
      "step  920 | train loss: 4.57 | val loss: 4.27 | perplexity: 71.35 | lr: 2.64e-04 | norm: 0.4246 | dt: 11727.4506ms | tok/sec: 33.5295\n",
      "step  921 | train loss: 4.63 | val loss: 4.27 | perplexity: 71.53 | lr: 2.60e-04 | norm: 0.4321 | dt: 11712.3883ms | tok/sec: 33.5727\n",
      "step  922 | train loss: 4.61 | val loss: 4.27 | perplexity: 71.78 | lr: 2.56e-04 | norm: 0.4085 | dt: 11699.9021ms | tok/sec: 33.6085\n",
      "step  923 | train loss: 4.49 | val loss: 4.26 | perplexity: 71.11 | lr: 2.53e-04 | norm: 0.4797 | dt: 11703.6653ms | tok/sec: 33.5977\n",
      "step  924 | train loss: 4.47 | val loss: 4.26 | perplexity: 70.67 | lr: 2.49e-04 | norm: 0.3171 | dt: 11704.7939ms | tok/sec: 33.5944\n",
      "step  925 | train loss: 4.93 | val loss: 4.26 | perplexity: 70.62 | lr: 2.45e-04 | norm: 0.3852 | dt: 11703.8364ms | tok/sec: 33.5972\n",
      "step  926 | train loss: 4.82 | val loss: 4.26 | perplexity: 70.78 | lr: 2.42e-04 | norm: 0.4187 | dt: 11725.2760ms | tok/sec: 33.5358\n",
      "step  927 | train loss: 4.46 | val loss: 4.26 | perplexity: 70.60 | lr: 2.38e-04 | norm: 0.3744 | dt: 11741.3943ms | tok/sec: 33.4897\n",
      "step  928 | train loss: 4.26 | val loss: 4.25 | perplexity: 70.45 | lr: 2.34e-04 | norm: 0.3626 | dt: 11714.3335ms | tok/sec: 33.5671\n",
      "step  929 | train loss: 4.81 | val loss: 4.25 | perplexity: 70.42 | lr: 2.31e-04 | norm: 0.3137 | dt: 11715.0898ms | tok/sec: 33.5649\n",
      "step  930 | train loss: 4.52 | val loss: 4.25 | perplexity: 70.39 | lr: 2.27e-04 | norm: 0.3419 | dt: 11726.2387ms | tok/sec: 33.5330\n",
      "step  931 | train loss: 4.59 | val loss: 4.25 | perplexity: 70.30 | lr: 2.24e-04 | norm: 0.3185 | dt: 11735.4443ms | tok/sec: 33.5067\n",
      "step  932 | train loss: 4.56 | val loss: 4.25 | perplexity: 70.20 | lr: 2.21e-04 | norm: 0.2897 | dt: 11750.6073ms | tok/sec: 33.4635\n",
      "step  933 | train loss: 4.76 | val loss: 4.25 | perplexity: 69.96 | lr: 2.17e-04 | norm: 0.3530 | dt: 11700.9552ms | tok/sec: 33.6055\n",
      "step  934 | train loss: 4.50 | val loss: 4.25 | perplexity: 69.94 | lr: 2.14e-04 | norm: 0.3305 | dt: 11714.3459ms | tok/sec: 33.5670\n",
      "step  935 | train loss: 4.50 | val loss: 4.25 | perplexity: 70.03 | lr: 2.11e-04 | norm: 0.2907 | dt: 11686.6083ms | tok/sec: 33.6467\n",
      "step  936 | train loss: 4.50 | val loss: 4.25 | perplexity: 70.02 | lr: 2.07e-04 | norm: 0.4096 | dt: 11716.9106ms | tok/sec: 33.5597\n",
      "step  937 | train loss: 4.89 | val loss: 4.25 | perplexity: 70.07 | lr: 2.04e-04 | norm: 0.4286 | dt: 11714.6595ms | tok/sec: 33.5661\n",
      "step  938 | train loss: 4.03 | val loss: 4.25 | perplexity: 70.21 | lr: 2.01e-04 | norm: 0.3283 | dt: 11728.0796ms | tok/sec: 33.5277\n",
      "step  939 | train loss: 4.32 | val loss: 4.25 | perplexity: 70.25 | lr: 1.98e-04 | norm: 0.2927 | dt: 11730.0923ms | tok/sec: 33.5220\n",
      "step  940 | train loss: 4.63 | val loss: 4.25 | perplexity: 70.05 | lr: 1.95e-04 | norm: 0.4400 | dt: 11721.4460ms | tok/sec: 33.5467\n",
      "step  941 | train loss: 4.36 | val loss: 4.25 | perplexity: 69.97 | lr: 1.92e-04 | norm: 0.2917 | dt: 11715.4357ms | tok/sec: 33.5639\n",
      "step  942 | train loss: 4.52 | val loss: 4.25 | perplexity: 69.93 | lr: 1.89e-04 | norm: 0.3339 | dt: 11721.7290ms | tok/sec: 33.5459\n",
      "step  943 | train loss: 4.71 | val loss: 4.25 | perplexity: 69.85 | lr: 1.86e-04 | norm: 0.3543 | dt: 11705.4129ms | tok/sec: 33.5927\n",
      "step  944 | train loss: 4.48 | val loss: 4.24 | perplexity: 69.66 | lr: 1.83e-04 | norm: 0.3603 | dt: 11718.9472ms | tok/sec: 33.5539\n",
      "step  945 | train loss: 4.44 | val loss: 4.24 | perplexity: 69.61 | lr: 1.80e-04 | norm: 0.3510 | dt: 11705.1730ms | tok/sec: 33.5934\n",
      "step  946 | train loss: 4.25 | val loss: 4.24 | perplexity: 69.62 | lr: 1.77e-04 | norm: 0.2900 | dt: 11723.3388ms | tok/sec: 33.5413\n",
      "step  947 | train loss: 4.28 | val loss: 4.24 | perplexity: 69.54 | lr: 1.75e-04 | norm: 0.3379 | dt: 11722.7571ms | tok/sec: 33.5430\n",
      "step  948 | train loss: 4.32 | val loss: 4.24 | perplexity: 69.49 | lr: 1.72e-04 | norm: 0.3600 | dt: 11715.6291ms | tok/sec: 33.5634\n",
      "step  949 | train loss: 4.59 | val loss: 4.24 | perplexity: 69.54 | lr: 1.69e-04 | norm: 0.2967 | dt: 11719.6918ms | tok/sec: 33.5517\n",
      "step  950 | train loss: 4.62 | val loss: 4.24 | perplexity: 69.63 | lr: 1.67e-04 | norm: 0.3574 | dt: 11750.8287ms | tok/sec: 33.4628\n",
      "step  951 | train loss: 4.22 | val loss: 4.24 | perplexity: 69.55 | lr: 1.64e-04 | norm: 0.4007 | dt: 11727.8326ms | tok/sec: 33.5284\n",
      "step  952 | train loss: 4.75 | val loss: 4.24 | perplexity: 69.44 | lr: 1.62e-04 | norm: 0.4561 | dt: 11720.6874ms | tok/sec: 33.5489\n",
      "step  953 | train loss: 4.64 | val loss: 4.24 | perplexity: 69.44 | lr: 1.59e-04 | norm: 0.2739 | dt: 11718.0686ms | tok/sec: 33.5564\n",
      "step  954 | train loss: 4.47 | val loss: 4.24 | perplexity: 69.47 | lr: 1.57e-04 | norm: 0.2933 | dt: 11706.0091ms | tok/sec: 33.5910\n",
      "step  955 | train loss: 4.73 | val loss: 4.24 | perplexity: 69.48 | lr: 1.54e-04 | norm: 0.4016 | dt: 11695.5070ms | tok/sec: 33.6211\n",
      "step  956 | train loss: 4.39 | val loss: 4.24 | perplexity: 69.42 | lr: 1.52e-04 | norm: 0.3333 | dt: 11720.0432ms | tok/sec: 33.5507\n",
      "step  957 | train loss: 4.68 | val loss: 4.24 | perplexity: 69.29 | lr: 1.50e-04 | norm: 0.3625 | dt: 11723.3179ms | tok/sec: 33.5414\n",
      "step  958 | train loss: 4.60 | val loss: 4.24 | perplexity: 69.19 | lr: 1.47e-04 | norm: 0.3031 | dt: 11707.0084ms | tok/sec: 33.5881\n",
      "step  959 | train loss: 4.28 | val loss: 4.24 | perplexity: 69.15 | lr: 1.45e-04 | norm: 0.4154 | dt: 11702.9021ms | tok/sec: 33.5999\n",
      "step  960 | train loss: 4.37 | val loss: 4.23 | perplexity: 69.04 | lr: 1.43e-04 | norm: 0.3323 | dt: 11731.2729ms | tok/sec: 33.5186\n",
      "step  961 | train loss: 4.29 | val loss: 4.23 | perplexity: 69.00 | lr: 1.41e-04 | norm: 0.3460 | dt: 11668.0613ms | tok/sec: 33.7002\n",
      "step  962 | train loss: 4.32 | val loss: 4.24 | perplexity: 69.09 | lr: 1.39e-04 | norm: 0.3130 | dt: 11710.3016ms | tok/sec: 33.5786\n",
      "step  963 | train loss: 4.63 | val loss: 4.24 | perplexity: 69.19 | lr: 1.37e-04 | norm: 0.3152 | dt: 11708.5726ms | tok/sec: 33.5836\n",
      "step  964 | train loss: 4.51 | val loss: 4.24 | perplexity: 69.21 | lr: 1.35e-04 | norm: 0.2906 | dt: 11731.2777ms | tok/sec: 33.5186\n",
      "step  965 | train loss: 4.55 | val loss: 4.24 | perplexity: 69.12 | lr: 1.33e-04 | norm: 0.3648 | dt: 11728.2832ms | tok/sec: 33.5272\n",
      "step  966 | train loss: 4.48 | val loss: 4.24 | perplexity: 69.06 | lr: 1.31e-04 | norm: 0.2580 | dt: 11722.7073ms | tok/sec: 33.5431\n",
      "step  967 | train loss: 4.78 | val loss: 4.23 | perplexity: 69.01 | lr: 1.29e-04 | norm: 0.5590 | dt: 11728.2386ms | tok/sec: 33.5273\n",
      "step  968 | train loss: 4.46 | val loss: 4.23 | perplexity: 68.85 | lr: 1.28e-04 | norm: 0.2947 | dt: 11726.0888ms | tok/sec: 33.5334\n",
      "step  969 | train loss: 4.11 | val loss: 4.23 | perplexity: 68.75 | lr: 1.26e-04 | norm: 0.4329 | dt: 11751.9703ms | tok/sec: 33.4596\n",
      "step  970 | train loss: 4.55 | val loss: 4.23 | perplexity: 68.67 | lr: 1.24e-04 | norm: 0.2955 | dt: 11723.2842ms | tok/sec: 33.5415\n",
      "step  971 | train loss: 5.16 | val loss: 4.23 | perplexity: 68.68 | lr: 1.23e-04 | norm: 0.6655 | dt: 11762.7792ms | tok/sec: 33.4288\n",
      "step  972 | train loss: 4.48 | val loss: 4.23 | perplexity: 68.65 | lr: 1.21e-04 | norm: 0.3586 | dt: 11738.2128ms | tok/sec: 33.4988\n",
      "step  973 | train loss: 4.32 | val loss: 4.23 | perplexity: 68.59 | lr: 1.20e-04 | norm: 0.3375 | dt: 11744.9682ms | tok/sec: 33.4795\n",
      "step  974 | train loss: 4.18 | val loss: 4.23 | perplexity: 68.59 | lr: 1.18e-04 | norm: 0.3070 | dt: 11786.4087ms | tok/sec: 33.3618\n",
      "step  975 | train loss: 4.32 | val loss: 4.23 | perplexity: 68.66 | lr: 1.17e-04 | norm: 0.3453 | dt: 11714.4752ms | tok/sec: 33.5667\n",
      "step  976 | train loss: 4.14 | val loss: 4.23 | perplexity: 68.63 | lr: 1.16e-04 | norm: 0.4844 | dt: 11727.0503ms | tok/sec: 33.5307\n",
      "step  977 | train loss: 4.53 | val loss: 4.23 | perplexity: 68.61 | lr: 1.14e-04 | norm: 0.3080 | dt: 11720.4473ms | tok/sec: 33.5496\n",
      "step  978 | train loss: 4.58 | val loss: 4.23 | perplexity: 68.69 | lr: 1.13e-04 | norm: 0.3082 | dt: 11734.9386ms | tok/sec: 33.5081\n",
      "step  979 | train loss: 4.83 | val loss: 4.23 | perplexity: 68.74 | lr: 1.12e-04 | norm: 0.3903 | dt: 11732.7151ms | tok/sec: 33.5145\n",
      "step  980 | train loss: 4.35 | val loss: 4.23 | perplexity: 68.70 | lr: 1.11e-04 | norm: 0.2908 | dt: 11678.8650ms | tok/sec: 33.6690\n",
      "step  981 | train loss: 4.33 | val loss: 4.23 | perplexity: 68.62 | lr: 1.10e-04 | norm: 0.3329 | dt: 11728.0116ms | tok/sec: 33.5279\n",
      "step  982 | train loss: 4.41 | val loss: 4.22 | perplexity: 68.37 | lr: 1.09e-04 | norm: 0.3271 | dt: 11732.7909ms | tok/sec: 33.5143\n",
      "step  983 | train loss: 4.77 | val loss: 4.22 | perplexity: 68.18 | lr: 1.08e-04 | norm: 0.2878 | dt: 11729.7506ms | tok/sec: 33.5230\n",
      "step  984 | train loss: 4.55 | val loss: 4.22 | perplexity: 68.04 | lr: 1.07e-04 | norm: 0.3063 | dt: 11707.9301ms | tok/sec: 33.5854\n",
      "step  985 | train loss: 4.52 | val loss: 4.22 | perplexity: 67.97 | lr: 1.06e-04 | norm: 0.2894 | dt: 11673.5604ms | tok/sec: 33.6843\n",
      "step  986 | train loss: 4.53 | val loss: 4.22 | perplexity: 67.94 | lr: 1.05e-04 | norm: 0.2585 | dt: 11723.5739ms | tok/sec: 33.5406\n",
      "step  987 | train loss: 4.34 | val loss: 4.22 | perplexity: 67.89 | lr: 1.05e-04 | norm: 0.2619 | dt: 11701.1817ms | tok/sec: 33.6048\n",
      "step  988 | train loss: 4.43 | val loss: 4.22 | perplexity: 67.86 | lr: 1.04e-04 | norm: 0.2817 | dt: 11729.6574ms | tok/sec: 33.5232\n",
      "step  989 | train loss: 4.60 | val loss: 4.22 | perplexity: 67.87 | lr: 1.03e-04 | norm: 0.4201 | dt: 11718.2555ms | tok/sec: 33.5558\n",
      "step  990 | train loss: 4.35 | val loss: 4.22 | perplexity: 67.82 | lr: 1.03e-04 | norm: 0.2598 | dt: 11733.4876ms | tok/sec: 33.5123\n",
      "step  991 | train loss: 4.28 | val loss: 4.22 | perplexity: 67.73 | lr: 1.02e-04 | norm: 0.2579 | dt: 11729.5842ms | tok/sec: 33.5234\n",
      "step  992 | train loss: 4.44 | val loss: 4.21 | perplexity: 67.63 | lr: 1.02e-04 | norm: 0.2875 | dt: 11715.1253ms | tok/sec: 33.5648\n",
      "step  993 | train loss: 4.24 | val loss: 4.21 | perplexity: 67.49 | lr: 1.01e-04 | norm: 0.2994 | dt: 11731.8947ms | tok/sec: 33.5168\n",
      "step  994 | train loss: 4.29 | val loss: 4.21 | perplexity: 67.43 | lr: 1.01e-04 | norm: 0.3770 | dt: 11717.7341ms | tok/sec: 33.5573\n",
      "step  995 | train loss: 4.61 | val loss: 4.21 | perplexity: 67.38 | lr: 1.01e-04 | norm: 0.3108 | dt: 11713.7713ms | tok/sec: 33.5687\n",
      "step  996 | train loss: 4.24 | val loss: 4.21 | perplexity: 67.38 | lr: 1.00e-04 | norm: 0.2769 | dt: 11732.5346ms | tok/sec: 33.5150\n",
      "step  997 | train loss: 4.57 | val loss: 4.21 | perplexity: 67.36 | lr: 1.00e-04 | norm: 0.3002 | dt: 11751.6735ms | tok/sec: 33.4604\n",
      "step  998 | train loss: 4.21 | val loss: 4.21 | perplexity: 67.37 | lr: 1.00e-04 | norm: 0.3089 | dt: 11727.8802ms | tok/sec: 33.5283\n",
      "step  999 | train loss: 4.50 | val loss: 4.21 | perplexity: 67.42 | lr: 1.00e-04 | norm: 0.3312 | dt: 11709.0905ms | tok/sec: 33.5821\n",
      "[Text Eval] samples=60 BLEU=0.00 ROUGE-L=0.0735 SELF-BLEU=11.65 REP=0.8256 D1=0.1159 D2=0.1480\n",
      "Total training time: 3.8909hr\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, token_encoder, EVAL_FREQ, grad_accum_steps, device,master_process, logpath)\n",
    "history,evaluation = trainer.train(MAX_STEPS, WARMUP_STEPS, MAX_LR, MIN_LR)\n",
    "dt = (time.time() - start_time) / (60*60)\n",
    "print(f\"Total training time: {dt:.4f}hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b89f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4965cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(evaluation, f, indent=4)\n",
    "\n",
    "with open('training_history.json','w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
