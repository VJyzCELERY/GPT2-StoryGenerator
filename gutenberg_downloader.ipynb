{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65f45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b340a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"story_download\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "CSV_PATH = \"./dataset/gutenberg.csv\"\n",
    "API_URL = \"https://gutendex.com/books/?languages=en\"\n",
    "\n",
    "START_REGEX = r\"\\*+\\s*START\\s+OF\\s+(?:THE\\s+)?PROJECT\\s+GUTENBERG\\s+EBOOK\"\n",
    "END_REGEX   = r\"\\*+\\s*END\\s+OF\\s+(?:THE\\s+)?PROJECT\\s+GUTENBERG\\s+EBOOK\"\n",
    "FICTION_TAGS = [\n",
    "    \"fiction\", \"novel\", \"story\", \"stories\", \"horror\",\n",
    "    \"fantasy\", \"gothic\", \"adventure\", \"mystery\",\n",
    "    \"detective\", \"romance\", \"science fiction\", \"sci-fi\"\n",
    "]\n",
    "\n",
    "NONFICTION_EXCLUDE = [\n",
    "    \"poetry\", \"poem\", \"drama\", \"play\", \"philosophy\",\n",
    "    \"religion\", \"theology\", \"sermon\", \"essay\", \"essays\",\n",
    "    \"history\", \"biography\", \"autobiography\",\n",
    "    \"reference\", \"dictionary\", \"textbook\", \"manual\"\n",
    "]\n",
    "def is_fiction(book):\n",
    "    \"\"\"\n",
    "    Checks Gutenberg metadata to filter for storybooks only.\n",
    "    \"\"\"\n",
    "    if book[\"media_type\"].lower() == \"sound\":\n",
    "        return False\n",
    "\n",
    "    subs = [s.lower() for s in book.get(\"subjects\", [])]\n",
    "    \n",
    "    shelves = [s.lower() for s in book.get(\"bookshelves\", [])]\n",
    "\n",
    "    if not any(tag in \" \".join(subs + shelves) for tag in FICTION_TAGS):\n",
    "        return False\n",
    "\n",
    "    if any(ex in \" \".join(subs + shelves) for ex in NONFICTION_EXCLUDE):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract_gutenberg_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    header = soup.find(id=\"pg-header\")\n",
    "    footer = soup.find(id=\"pg-footer\")\n",
    "\n",
    "    if not header or not footer:\n",
    "        print(\"[WARN] Header or footer not found\")\n",
    "        return None\n",
    "\n",
    "    content_nodes = []\n",
    "    for elem in header.next_siblings:\n",
    "        if elem == footer:\n",
    "            break\n",
    "        content_nodes.append(elem)\n",
    "\n",
    "    paragraphs = []\n",
    "    for node in content_nodes:\n",
    "        parsed = BeautifulSoup(str(node), \"html.parser\")\n",
    "\n",
    "        for p in parsed.find_all(\"p\"):\n",
    "            if p.attrs:\n",
    "                continue\n",
    "\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "\n",
    "    final_text = \"\\n\\n\".join(paragraphs).strip()\n",
    "\n",
    "    return final_text if len(final_text) > 200 else None\n",
    "\n",
    "\n",
    "def normalize_gutenberg_text(text):\n",
    "\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "\n",
    "    merged = []\n",
    "    for line in lines:\n",
    "        if line == \"\":\n",
    "            merged.append(\"\") \n",
    "        else:\n",
    "            merged.append(line)\n",
    "    paragraphs = []\n",
    "    current = []\n",
    "\n",
    "    for line in merged:\n",
    "        if line == \"\":\n",
    "            if current:\n",
    "                paragraphs.append(\" \".join(current))\n",
    "                current = []\n",
    "        else:\n",
    "            current.append(line)\n",
    "\n",
    "    if current:\n",
    "        paragraphs.append(\" \".join(current))\n",
    "\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def clean_book_text(text: str):\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = unicodedata.normalize(\"NFKC\",text)\n",
    "    return text.strip()\n",
    "def convert_gutenberg_html_url(api_url, book_id):\n",
    "    if api_url.endswith(\".html.images\"):\n",
    "        return f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}-images.html\"\n",
    "\n",
    "    if api_url.endswith(\".html.noimages\"):\n",
    "        return f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.html\"\n",
    "\n",
    "    return api_url\n",
    "\n",
    "def download_clean_text(book):\n",
    "    book_id = book[\"id\"]\n",
    "    if not is_fiction(book):\n",
    "        print(f\"[SKIP] {book_id}: not fiction\")\n",
    "        return None\n",
    "    if book[\"media_type\"].lower() == \"sound\":\n",
    "        print(f\"[SKIP] {book_id}: audiobook\")\n",
    "        return None\n",
    "\n",
    "    html_url = None\n",
    "\n",
    "    for fmt, url in book[\"formats\"].items():\n",
    "\n",
    "        if \"text/html\" in fmt:\n",
    "\n",
    "            if url.endswith(\".html\"):\n",
    "                html_url = url\n",
    "                break\n",
    "\n",
    "            elif url.endswith(\".html.images\"):\n",
    "                html_url = f\"https://www.gutenberg.org/cache/epub/{book['id']}/pg{book['id']}-images.html\"\n",
    "                break\n",
    "\n",
    "    if not html_url:\n",
    "        print(f\"[SKIP] {book_id}: no HTML\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        r = requests.get(html_url, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[ERR] {book_id}: failed HTML download\")\n",
    "            return None\n",
    "\n",
    "        extracted = extract_gutenberg_html(r.text)\n",
    "        if not extracted:\n",
    "            print(f\"[SKIP] {book_id}: no valid HTML text found\")\n",
    "            return None\n",
    "\n",
    "        cleaned = clean_book_text(extracted)\n",
    "\n",
    "        if len(cleaned) < 1000:\n",
    "            print(f\"[SKIP] {book_id}: too short after cleaning\")\n",
    "            return None\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] {book_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"[INFO] Fetching English book list via Gutendex...\")\n",
    "    MAX_BOOK = 3000\n",
    "    collected = 0\n",
    "    url = API_URL\n",
    "\n",
    "    os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
    "\n",
    "    with open(CSV_PATH, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"text\"])\n",
    "\n",
    "        while collected < MAX_BOOK and url:\n",
    "            r = requests.get(url).json()\n",
    "            books = r[\"results\"]\n",
    "            url = r[\"next\"]\n",
    "\n",
    "            print(f\"[INFO] Loaded {len(books)} books from page...\")\n",
    "\n",
    "            for book in books:\n",
    "                if collected >= MAX_BOOK:\n",
    "                    break\n",
    "\n",
    "                book_id = book[\"id\"]\n",
    "                print(f\"[DL] {book_id}...\")\n",
    "\n",
    "                cleaned = download_clean_text(book)\n",
    "                if not cleaned:\n",
    "                    continue\n",
    "\n",
    "                txt_path = os.path.join(SAVE_DIR, f\"{book_id}.txt\")\n",
    "                with open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                    out.write(cleaned)\n",
    "\n",
    "                writer.writerow([cleaned])\n",
    "                collected += 1\n",
    "                print(f\"[OK] Saved book {book_id} (total clean: {collected})\")\n",
    "\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    print(\"\\n[DONE] Dataset ready!\")\n",
    "    print(f\"TXT dir: {SAVE_DIR}/\")\n",
    "    print(f\"CSV: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b10e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching English book list via Gutendex...\n",
      "[INFO] Loaded 32 books from page...\n",
      "[DL] 84...\n",
      "[OK] Saved book 84 (total clean: 1)\n",
      "[DL] 2701...\n",
      "[OK] Saved book 2701 (total clean: 2)\n",
      "[DL] 1342...\n",
      "[OK] Saved book 1342 (total clean: 3)\n",
      "[DL] 1513...\n",
      "[SKIP] 1513: not fiction\n",
      "[DL] 43...\n",
      "[OK] Saved book 43 (total clean: 4)\n",
      "[DL] 11...\n",
      "[OK] Saved book 11 (total clean: 5)\n",
      "[DL] 100...\n",
      "[SKIP] 100: not fiction\n",
      "[DL] 8492...\n",
      "[OK] Saved book 8492 (total clean: 6)\n",
      "[DL] 2641...\n",
      "[OK] Saved book 2641 (total clean: 7)\n",
      "[DL] 145...\n",
      "[OK] Saved book 145 (total clean: 8)\n",
      "[DL] 25344...\n",
      "[SKIP] 25344: not fiction\n",
      "[DL] 37106...\n",
      "[OK] Saved book 37106 (total clean: 9)\n",
      "[DL] 345...\n",
      "[OK] Saved book 345 (total clean: 10)\n",
      "[DL] 16328...\n",
      "[SKIP] 16328: not fiction\n",
      "[DL] 67979...\n",
      "[SKIP] 67979: not fiction\n",
      "[DL] 16389...\n",
      "[OK] Saved book 16389 (total clean: 11)\n",
      "[DL] 2554...\n",
      "[OK] Saved book 2554 (total clean: 12)\n",
      "[DL] 1260...\n",
      "[OK] Saved book 1260 (total clean: 13)\n",
      "[DL] 6761...\n",
      "[OK] Saved book 6761 (total clean: 14)\n",
      "[DL] 5197...\n",
      "[SKIP] 5197: not fiction\n",
      "[DL] 394...\n",
      "[OK] Saved book 394 (total clean: 15)\n",
      "[DL] 6593...\n",
      "[OK] Saved book 6593 (total clean: 16)\n",
      "[DL] 2160...\n",
      "[SKIP] 2160: not fiction\n",
      "[DL] 174...\n",
      "[SKIP] 174: not fiction\n",
      "[DL] 1259...\n",
      "[SKIP] 1259: not fiction\n",
      "[DL] 4085...\n",
      "[OK] Saved book 4085 (total clean: 17)\n",
      "[DL] 2542...\n",
      "[SKIP] 2542: not fiction\n",
      "[DL] 46...\n",
      "[OK] Saved book 46 (total clean: 18)\n",
      "[DL] 844...\n",
      "[SKIP] 844: not fiction\n",
      "[DL] 64317...\n",
      "[OK] Saved book 64317 (total clean: 19)\n",
      "[DL] 1080...\n",
      "[SKIP] 1080: not fiction\n",
      "[DL] 98...\n",
      "[SKIP] 98: not fiction\n",
      "[INFO] Loaded 32 books from page...\n",
      "[DL] 768...\n",
      "[OK] Saved book 768 (total clean: 20)\n",
      "[DL] 1661...\n",
      "[OK] Saved book 1661 (total clean: 21)\n",
      "[DL] 76...\n",
      "[OK] Saved book 76 (total clean: 22)\n",
      "[DL] 1184...\n",
      "[SKIP] 1184: not fiction\n",
      "[DL] 3207...\n",
      "[SKIP] 3207: not fiction\n",
      "[DL] 5200...\n",
      "[OK] Saved book 5200 (total clean: 23)\n",
      "[DL] 41445...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    195\u001b[39m book_id = book[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DL] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m cleaned = \u001b[43mdownload_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cleaned:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mdownload_clean_text\u001b[39m\u001b[34m(book)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERR] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: failed HTML download\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m extracted = \u001b[43mextract_gutenberg_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m extracted:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SKIP] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: no valid HTML text found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mextract_gutenberg_html\u001b[39m\u001b[34m(html)\u001b[39m\n\u001b[32m     56\u001b[39m paragraphs = []\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m content_nodes:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     parsed = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhtml.parser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parsed.find_all(\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.attrs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\bs4\\__init__.py:476\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.initialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\bs4\\__init__.py:661\u001b[39m, in \u001b[36mBeautifulSoup._feed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.reset()\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[38;5;28mself\u001b[39m.endData()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:467\u001b[39m, in \u001b[36mHTMLParserTreeBuilder.feed\u001b[39m\u001b[34m(self, markup)\u001b[39m\n\u001b[32m    464\u001b[39m parser = BeautifulSoupHTMLParser(\u001b[38;5;28mself\u001b[39m.soup, *args, **kwargs)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     parser.close()\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\html\\parser.py:142\u001b[39m, in \u001b[36mHTMLParser.feed\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.rawdata = \u001b[38;5;28mself\u001b[39m.rawdata + data\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\html\\parser.py:222\u001b[39m, in \u001b[36mHTMLParser.goahead\u001b[39m\u001b[34m(self, end)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m, i):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen.match(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m         k = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m, i):\n\u001b[32m    224\u001b[39m         k = \u001b[38;5;28mself\u001b[39m.parse_endtag(i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\html\\parser.py:402\u001b[39m, in \u001b[36mHTMLParser.parse_starttag\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_starttag\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# See the HTML5 specs section \"13.2.5.8 Tag name state\"\u001b[39;00m\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# https://html.spec.whatwg.org/multipage/parsing.html#tag-name-state\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28mself\u001b[39m.__starttag_text = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     endpos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_for_whole_start_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m endpos < \u001b[32m0\u001b[39m:\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m endpos\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\.conda\\envs\\deep_learning_cuda\\Lib\\html\\parser.py:448\u001b[39m, in \u001b[36mHTMLParser.check_for_whole_start_tag\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_for_whole_start_tag\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[32m    447\u001b[39m     rawdata = \u001b[38;5;28mself\u001b[39m.rawdata\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     match = \u001b[43mlocatetagend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m match\n\u001b[32m    450\u001b[39m     j = match.end()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
